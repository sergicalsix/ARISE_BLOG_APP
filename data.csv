url,title,article
https://www.ariseanalytics.com/activities/report/20200925-2/,ARISE analytics流ギルド活動,初めまして、Advanced Tech Divisionに所属している村瀬と申します。Advanced Tech Divisionでは、多くの会社と協創しながら分析ソリューション案件を推進しております。今回は、多くの案件を推進していく中で得られた暗黙知を形式知にしていくための活動をご紹介いたします。ARISE analyticsが携わっている分析ソリューション案件は多岐にわたり、インプットデータだけでも顧客情報、位置情報、画像データ、IoTデータなど様々な種類を取り扱っております。またPoCフェーズ、開発フェーズ、運用フェーズなど様々なフェーズでの提案やPJ管理を行なっております。従来、提案やPJ管理は案件担当者の経験や社内ファイルサーバーにある資料を参考に行なっておりました。しかし、担当者ごとのクオリティの差が発生したり、案件が多くなるにつれ過去資料を探すことが難しくなってきました。また、エンジニアが他案件で利用している最新技術やアーキテクチャを知りたいと思っても、他案件の状況が分からず、技術力が人に依存してしまうという課題もありました。上記のような状況を打破するため、案件のノウハウを形式知化し、誰でも自発的に情報を収集できるような仕組みを構築しております。これらを通じて、安定的なPJ品質の担保と効率化に加え、エンジニアの育成を行なっております。（さらに集めた知見をこのTechBlogで発信し、ARISE analyticsのPRも狙っています笑）私たちはこれら一連の活動内容をギルド活動と称し、各テーマに沿って知見の収集・発信を行なっております。ギルドという名前はSpotifyが組織的に行なっている全社的なアジャイル活動※1からお借りしました。少し前に「SpotifyはSpotifyモデル（ギルドやスクワッドなど）を使っていない」※2という記事が公開されました。しかしながら、この活動は情報共有を目的とし、当社コーポレート部門とも協力し教育マテリアルの充実化につなげて部門に閉じず情報共有を加速させるなど、Spotifyの教訓を踏まえつつ日々改良をしていきながら進めている所です。今回は代表的なギルドの取り組み内容を5つご紹介いたします。次回以降は各ギルドの取り組み内容を紹介する予定です。業務で得た知識を横展開する際の参考になると幸いです。※1：https://lean-trenches.com/scaling-agile-at-spotify-ja/※2：https://jp.quora.com/q/agile/Spotifyは-Spotifyモデル-を使っていない
https://www.ariseanalytics.com/activities/report/20201008/,ARISE Kaggle部活動記,ARISE analyticsの長谷井です。ARISE Kaggle部の活動記へようこそ！我々は世界的に知名度の高いKaggleを中心に、国内外の分析コンペティションへ参加し、上位ランクインを目指して頑張っています。今後Kaggle部の活動記を連載していきますので、どうぞよろしくお願いします。 Kaggle部は2020年9月現在、参加人数は約30名。業務に支障のない範囲で楽しく活動中です。M5 Forecasting – Accuracy @ Kaggle (398/5598 銅ランク) M5 Forecasting – Accuracyで取り組んだ分析内容と苦労や工夫した点について説明します！オンライン飲み会の際に、Kaggleで実績をつくっていきたい！と熱く語ったARISE社員が有志で5人集まり、コンペに参加することにしました。今回のメンバーは普段データ分析の仕事をしていますが、業務の繁忙期なども踏まえ、まずは今回のコンペでメダルを受賞するという目標を立てました。役割分担・サブミットの管理・MTG等のコミュニケーション方法に工夫を加え、無事にコンペを最後まで諦めずにやりきりました。今回の結果は5598人中398位で銅ランク（メダル）となり、今後のKaggle参戦に役立つヒントをたくさん得られました。今回のミッションは世界最大小売Walmartにおける売上個数予測でした。M5コンペの概要は以下の通り。※上記に掲載されているロゴは、商標または登録商標です。以下の図では月別の売上データです。青い部分が与えられている過去の売上データで、オレンジの部分が予測をする断面です。州別の売上データです。カリフォルニアが突出して高くなっています。まずは与えられたデータの構造を理解するため、ディスカッションボードにある情報をひたすら読み込み、データの情報を整理するというところからはじめました。以下はその一部の抜粋です。頻繁に発生する欠損値継続的に売れている商品でも、在庫切れなどが原因で売り上げが0になっている商品があり、欠損値の状態でモデルに組み込んだ場合、予測に無意味な周期性などが生まれてしまい、問題となっていました。欠損値補完のために4つの補完手法を比較した結果、テストデータのスコアが一番改善される指数平滑法を採用しました。今回の評価指標はWRMSSEというかなり特殊なもので、モデリングによる最適化が困難でした。どのようなモデルを使えば、アウトプットの最適化をうまく行えるか、決めるために試行錯誤が必要でした。モデル選定については、各時系列の探索は、スペックや時間に制限があり困難なため、初期変数のみを用いたベースモデルの作成で当てはまりの良いLightGBMを採用しました。Kaggle Notebookは無料でGPU環境が使えますが、Kaggleで戦うには心もとなく、モデルが回せないときもしばしばあり、int64をint8型に変換する等の工夫でなんとか処理コストの短縮を図りました。その結果、今回のコンペはKaggle NotebookやGCPの無料枠(2020/6時点)を活用し、モデリングを行うことができました。以下はそれぞれの環境の比較表となります。反省としてはやはり、高スペック環境の準備がコンペで戦うために必要だと気づかされました。最終的にサブミットに利用したモデルはLight GBMとなりました。メンバーは部門横断で集まっているため、お互い別々のプロジェクトを担当しており、繁忙期などにはあまり貢献できない人もいました。そのため、毎回進捗を共有する人を決めて週に1度のリモート会議を実施しました。お互いの分析内容を共有し、チームで効率的に進められるよう工夫しました。参加メンバーはそれぞれ違ったレベルのモチベーションや時間的余裕をもって分析コンペに取り組んでいるので、目的意識の統一と情報共有がとても重要です。今回は初めての本格的なKaggle参加ということで、まずはメダル受賞を目標としていました。コミュニケーションとしては、リモート会議の他、Slackで逐一情報を共有し、コードやサブミットファイルのバージョン管理と分析知見の認識合わせをしていました。今回は銅メダル受賞となりましたが、これからもっと効率的に・スピーディにKaggleに取り組めるヒントをたくさん得られました。今後はもっと強力な分析基盤をKaggle部として用意し、様々な分析コンペに参加予定です。分析はチームワークです。1人でもできるかもしれませんが、役割を決め、お互いの強みを持ち合う事でより効率に分析課題に取り組めます！我々と一緒にKaggle部を盛り上げてくださる方はこちらのページからご連絡お待ちしております！
https://www.ariseanalytics.com/activities/report/20201016/,Multi-Object Trackingの精度評価指標,"Multi-Object Tracking（MOT）は、動画の中で移動していく複数の人や物をそれぞれ区別して継続的に追跡する画像処理技術です。技術的には画像処理の基本的な技術の一つである物体認識の応用技術になりますが、静止画の物体認識とは異なり動画中の物体は常に外観が変化し続けることから、非常に難しいタスクとされています。MOTは研究が盛んな分野であり多くのアルゴリズムが開発されています。広く使われているTracking by Detectionと呼ばれる手法では、まず動画の各フレームから追跡対象の物体を検知し、次にフレーム間で同じ物体に対して同一のIDを割り振っていきます。MOTにおいて動画中の物体の位置は一つの矩形で表され、Bounding Boxと呼ばれます。MOTのモデルは、動画の各フレームで検出された同じ物体のBounding Boxに対して一意なIDを割り振ることによって、物体を「追跡」します。では、物体を追跡するアルゴリズムの性能はどのように評価されるのでしょうか。MOTのアルゴリズムの性能は正解データのBounding BoxとIDをモデルの出力と比較することによって評価されるのですが、以下この記事ではMOTでよく使われる評価指標を紹介していきます。機械学習における評価指標といえば、Accuracy，Precision，Recallといった指標がありますが、Multi-Object Trackingにおいても似た名前の評価指標が使われます[1]。まず、MOTでモデルの出力が正しいかどうかをどのようにして判断すべきか、基本的な考え方をおさえておきます。下図のようにモデルの出力が正解データと同じように物体を追跡できた場合、正しい結果とみなします。MOTでは、下図のような一続きの物体の追跡結果をトラックと呼んでいます。 一方で正しく物体を追跡できない場合として、以下のようなパターンが考えられます。下図のように、モデルから出力されたトラックがどの正解データにもマッチしない場合をFalse Positiveと呼びます。一方で、正解データに存在するトラックをモデルが予測できなかった場合をMissesと呼びます。最後に、下図のように正解データの2つのトラックが一時交錯する場合には、モデルが追跡対象の物体を誤ってしまうことがあります。MOTでは、この場合をID Switch（Fragmentation）と呼んでいます。前節で紹介したFalse Positive，Misses，ID Switchに基づいて、MOTでは次式でモデルの精度（Multi-Object Tracking Accuracy; MOTA）を定義しています。MOTAはMOTA<=1の範囲で値をとり、1に近いほど良いと判断されます。分母は各フレームにおける正解データの総和、分子はフレームごとにFalse Positive，Misses，ID Switchを足し合わせたものです。直感的には、正解データのBounding Boxと予測モデルのBounding Boxが近いほど、また予測モデルのトラックが正解データと一致しているほどMOTAが高くなります。MOTでは、動画の各フレームから物体認識でBounding Boxを検出し、フレーム間で時系列に沿ってトラックIDを割り振るTracking-by-Detectionの手法が広く使われてきたため、MOTAはそれらのモデルを正しく評価できるように設計されています。MOTAはモデルの出力結果と正解データのトラックの一致度合を数値化したものですが、それではMulti-Object Trackingの性能をうまく表現できない場合もあります。下図では、ある正解データに対するモデルAとモデルBの出力をそれぞれ示しています。MOTとしては、同じ正解データをより長い期間追跡できている方が性能が良いと考えられるので、パターンBの方が性能の良いモデルとしたいところです。ところが、図中のAとBのモデルをMOTAで評価した場合、どちらのモデルでもID Switchが1回発生しているだけなので、同じ精度（MOTA=0.9）として評価されてしまいます。つまり、MOTAはBounding Boxレベルでの一致度合の評価には適しているのですが、物体をどれくらい長く追跡できているかという点での評価には適していないのです。そこで、最近では物体をどれくらい長い間追跡できているかという性能を評価するため、トラックのIDを基準にしたIdentification Precision（IDP），Identification Recall（IDR），Identification F1（IDF1）という指標が合わせて使用されるようになってきました[2]。MOTAがBounding Boxレベルでの一致度合を計算するのに対して、IDF，IDR，IDF1はトラックのIDレベルでの一致度合に基づいて精度を算出します。まず正解トラック（τ∈AT）とモデルの出力のトラック（γ∈AC）をFPとFNの総和が最小になるようにマッチングし、正解トラックτにマッチした出力トラックをγm(τ)、出力トラックγにマッチした正解トラックをτm(γ)とあらわします。Tτ，Tγをそれぞれ正解トラックとモデルの出力トラックに含まれるフレーム番号の集合とし、m(x, y, t)は2つのトラックをt番目のフレームで比較した場合に2つのBounding BoxのIoUが閾値以下であれば1を、そうでなければ0を返す関数とします。また、トラックτ，トラックγの長さをそれぞれlen(τ)，len(γ)とあらわします。このとき、IDFN（ID False Negative），IDFP（ID False Positive），IDTP（ID True Positive）を以下のように定義します。あとは、通常のPrecisionやRecallと同様にIDP，IDR，IDF1を算出すれば、Bounding Boxの一致度合ではなくトラックの長さで性能を評価する指標が下記のように定義されます。ちなみに、上図のモデルAではIDF1=0.6，モデルBではIDF1=0.8となり、こちらの評価指標では期待した通りモデルBの方が性能が高いと評価されます。この記事では、MOTで広く使われている精度指標MOTAと最近注目されているトラックIDを基準にした精度指標IDP，IDR，IDF1を紹介しました。直感的には、MOTAがトラックのIDをあまり考慮せずにBounding Box間での一致度合を重視しているのに対して、トラックIDを基準にした指標では、まず正解トラックとモデルの出力トラックを一致するものと一致しないものに分類し、それぞれの分類の中でBounding Boxの重なり具合を比較しているという点が異なっています。Multi-Object Trackingでは、従来Tracking-by-Detectionタイプのモデルが多かったのですが、最近ではDetectionをベースにしないモデルや、逆にDetectionとTrackingを一体化したようなモデルも出てきています[3][4]。モデルが多様化するにしたがって、評価指標も新しいものが必要になってきているようです。一方で、実務レベルでは処理速度やシステムのアーキテクチャなどの問題でTracking-by-Detectionモデルの方が適している場合もあり、一概に新しいものが良いとも言えません。MOTには，ここで紹介した以外にも様々な評価指標が存在するため、モデルのどの側面を評価したいかによって適切な指標を選択することが重要です。関連文献Bernardin, Keni, and Rainer Stiefelhagen. “Evaluating multiple object tracking performance: the CLEAR MOT metrics.” EURASIP Journal on Image and Video Processing 2008.1 (2008): 1-10.Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking. E. Ristani, F. Solera, R. S. Zou, R. Cucchiara and C. Tomasi. ECCV 2016 Workshop on Benchmarking Multi-Target Tracking.Bergmann, Philipp, Tim Meinhardt, and Laura Leal-Taixe. “Tracking without bells and whistles.” Proceedings of the IEEE international conference on computer vision. 2019.Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. “Tracking Objects as Points.” arXiv preprint arXiv:2004.01177 (2020)."
https://www.ariseanalytics.com/activities/report/20201023/,コロナ禍における経済分析と位置情報の活用,"こんにちは、Social Innovation Divisionで位置情報分析を担当している高良と申します。今回は、コロナ禍における経済活動の分析と、それに対して位置情報がどのように活用されているかをご紹介します。2020年初めから世界的に感染拡大が生じている新型コロナウィルス感染症は、各国の経済活動にとても大きな影響を与えてきました。IMF（International Monetary Fund）によると、2012-2019年にかけて+3~4％程度で推移していた世界経済の成長率は、2020年には-4.4％の落ち込みが見込まれています。日本も同様に、2012-2019年にかけて+0~2％程度で成長していたのに対し、2020年には-5.3％の落ち込みが見込まれています。こうした状況で、新型コロナ感染症の経済活動への影響や、各国の政策対応の経済活動への影響などについての研究が、理論面、実証面の両方で急速に進んできました。例えば、ヨーロッパの経済・政策分析機関であるCenter for Economic Policy Researchは、新型コロナ感染症に関する研究をリアルタイムに公表、蓄積していくことを目的として、2020年3月末にCovid Economicsという雑誌を刊行しました。この雑誌は、これまで月に5回以上のペースでの発行を続けており、2020年9月末時点で第50号まで発行されています。位置情報は新型コロナ感染症が拡大する2019年以前まで、商圏分析やマーティング、交通・都市計画、防災など様々な分野に活用されてきました。それに加え2020年以降は、感染拡大の防止を目的とするモニタリング指標として、全国の観光地・主要駅の混雑状況や、県をまたぐ移動人口などの推計に位置情報が活用されています（https://corona.go.jp/dashboard/）。民間企業による分析例としては、KDDI株式会社が公表している位置情報を利用した人口変動分析のレポートなどがあります。これらの分析のなかで、当社も、ゴールデンウィーク中の観光地の人口変化や、県をまたぐ移動人口についてのレポートなどで分析協力を行っています。分析を行う際には、人流と経済活動との関係については重要な分析観点として挙がったものの、そのために必要なプロセス（経済指標を公的統計調査などから収集し、位置情報分析基盤に乗せること）に時間がかかってしまうため、速報性などを考慮して断念した分析もありました。研究者や研究機関による分析例は数多くありますが、以下では、コロナ禍における経済活動の分析例として、①企業の倒産確率と企業業績・行動制限政策・モビリティとの関係、②業態別の感染リスクと経済的重要性のトレードオフ、という2つの分析と、その中でどのように位置情報が活用されているかをご紹介します。①企業の倒産確率と企業業績・行動制限政策・モビリティとの関係分析名：宮川大介 (2020) 「コロナ危機後の行動制限政策と企業業績・倒産――マイクロデータの活用による実態把握」『コロナ危機の経済学：提言と分析』 第14章, pp.239-255.この分析は、日本における感染拡大前後の企業の業績・倒産傾向の変化や、それらと行動制限政策との関係を明らかにすることを目的としています。そのために、企業の倒産履歴・業績データと属性データ、都道府県・業態別のモビリティデータを利用して、感染症拡大前後の企業の倒産確率の要因分析を行っています。具体的には、感染拡大前（2019年2~4月）と拡大後（2020年2～4月）の企業の倒産確率が、業種や売上高、利益、社齢、資本金、小売業のモビリティ指数（訪問人数）とのどのような関係にあるかを推計しています。推計により、感染拡大前には、売上高の伸び率が低い、社齢が若いという特徴を持つ企業の倒産確率が高く、感染拡大後も同様の傾向があることがわかりました。さらに、感染拡大後、小売業のモビリティ指数が低下している都道府県では、企業の倒産確率が上昇していることも確認されています。このモビリティ指数は、スマートフォンの地図サービス利用者より得られた位置情報により推計されたもので、コロナ禍における企業の倒産に対する人流減少の影響という点で、位置情報の活用例となっています。②業態別の感染リスクと経済的重要性のトレードオフ分析名：Benzell, Seth G., Avinash Collis, and Christos Nicolaides, 2020, “Rationing Social Contact During the COVID-19 Pandemic: Transmission Risk and Social Benefits of US Locations,” Proceedings of the National Academy of Sciences Jun 2020, 117(26), pp.14642-14644.この分析は、米国の業態別の感染リスクと経済的重要性のトレードオフを、様々なデータを用いて測定することを目的としています。そのために、感染リスクの計測、経済的重要性の計測、両者の比較、という3つのプロセスで分析を行っています。まず、公園、カフェ、レストラン、大学などの業態に該当する各スポットの訪問ユーザ動向を比較することで、30程度ある各業態の感染リスクを相対的に評価します。具体的には、各業態に属する個別のスポットへの訪問ユーザ数、訪問回数、滞在時間などを、スマートフォンから得られる位置情報から推計し、感染リスクとして指標化します。次に、事業所統計調査や消費者への選好サーベイのデータを用いて、各業態に属する事業所の従業者数や、従業者に支払っている給与、消費者の各業態への選好などから、各業態の経済的重要性を相対評価します。上記の感染リスク、経済的重要性の2つを比較することでそれらのトレードオフを考慮します。図1は、各業態の感染リスク（横軸）と経済的重要性（縦軸）の関係を示したものです。図1を見ると、感染リスクと経済的重要性は概ね正の関係にあり、感染リスクが高い業態は経済的重要性も高いこと、つまり両者にトレードオフがあることがわかります。ただし、一部そうではない業態があります。図の左上に位置する、感染リスクが低い、かつ経済的重要性が高い、金融機関・大学などや、右下に位置する感染リスクが高い、かつ経済的重要性が低い、ジム・カフェ/パーラー・スポーツ用品店などです。このように、感染リスクと経済的重要性のトレードオフを考慮することで、各業態に対する規制政策としての優先順位付けが可能となります。本稿でご紹介した2つの分析は、コロナ禍において急速に進展した経済分析のうち、位置情報を活用したものの一部ですが、ご参考になれば幸いです。KDDI株式会社, 位置情報を利用した人口変動分析のレポート（https://www.au.com/information/covid-19/#accordionOpen2）"
https://www.ariseanalytics.com/activities/report/20201030-2/,sparkパラメータ最適化チューニング,こんにちは。Customer Analytics Division所属データサイエンティスト兼データエンジニアの渡邉です。ARISE analyticsでは数百人のデータサイエンティストが活躍しています。一般的な分析環境は、データサイエンティストがそれぞれEMRを立て、その上のsparkで分析を走らせています。ただ、その分日々の分析費用も大きいものとなっています。そこで、sparkパラメータ最適化にトライしました。こちらのAWSの記事に従って計算しました。一部のパラメータについて、絵で説明したいと思います。sparkはExecutorという単位があり、これが処理を実行する単位です。spark.executor.coresは各ExecutorがCPUのcoreをいくつ使用するか決めるパラメータです。絵で直感的にわかる通り、余っているcoreがもったいないので、できるだけ全部使うようにします。AWSプラクティスでは、最適値は5とされています。少し複雑ですので3stepに分けて説明します。先ほど説明した通り、Executorに割り当てるcoreの数を決めると、インスタンス（EMRを構成する一つのマシン）ごとのExecutorの数が決まります。注意点としては、インスタンスのcoreのうちの1つはHadoop daemonsが使うので、Executorに割り当てることができません。Executorごとに割り当てられるメモリは、インスタンスのメモリをExecutorの数で割ったものです。ただし、注意点が2つあり、インスタンスのメモリとはyarn.nodemanager.resource.memory-mbに設定された数値です。（こちらにインスタンスタイプの一覧があります）つまり計算式は次のようになります。ここはAWSベストプラクティスにしたがって、spark.executor.memoryに9割、spark.executor.memoryOverheadに1割を割り当ててください。r4.4xlargeのクラスタを起動することを考えてみます。r4.4xlargeは16コア、yarn.nodemanager.resource.memory-mb=116GBです。となりました。最適なパラメータを計算できたので、sparkを起動していきましょう。EMRでは起動時にsparkのパラメータを渡すことができます。https://docs.aws.amazon.com/ja_jp/emr/latest/ReleaseGuide/emr-spark-configure.html起動が出来たらWebUI(port:8088)からパラメータが正しくセットできたか確認してみてください。sparkは奥が深く、全貌を理解することは大変ですが、その分チューニングの余地もあるのではないかと思っています。この記事がお役に立てれば幸いです。
https://www.ariseanalytics.com/activities/report/20201111/,Scala×SparkのUDFとウィンドウ関数を使って緯度経度データから2点間の距離を求める,"こんにちは、Marketing Solution Divisionの鴨居です。私は位置情報を扱う分析ソリューションの開発チームに1年ほど在籍したのちに現在のチームに移籍しました。前チームではScala×Sparkで位置情報データの分析・ロジック開発を行っていました。この記事では、Scala×Sparkを使った位置情報データ分析の例をご紹介したいと思います。これまではマーケティング分析において位置情報データが活用されることはほとんどありませんでしたが、最近では位置情報データを使ったマーケティング活動が注目されてきており、コロナ禍を経てさらに脚光を浴びています。参考までに、KDDIとARISE analyticsが共同で出しているコロナ分析レポートのリンクを貼っておきます。（こちら：主要観光地におけるGW期間中の詳細人流分析レポート）位置情報データ活用の波を受けて、私のチームでも現在位置情報を活用したマーケティング分析を検討しております。そこで、個人としては1年ぶりになるScala言語を使った位置情報関連の実装をご紹介したいと思います。今回はスマートフォン端末で計測されるGPS位置情報データから2点間の距離を求める処理をScala sparkで実装してみました。Sparkに馴染みのない方に向けて、Sparkでよく使われるウィンドウ関数というものを意図的に使った実装を紹介していきたいと思います。この記事で紹介するソースコードはすべてScalaで書かれていますが、Pyspark(Python×Spark)で書きたい方もほとんど同じ実装で書くことができるため参考にしてみてください。Sparkではウィンドウ関数(window functions)という機能を使うことができます。これはSQLなどにも実装されている機能で、対象レコード以外のレコードを参照することができるものです。今回は、以下のような時間情報を持った位置情報データがあるときに、対象レコードの前時刻の緯度経度と対象レコードの緯度経度の距離を求めます。locDtaには各時刻の緯度経度データしかないため、ここに前時刻の緯度経度のカラムを追加することで、レコードごとに前時刻との距離を求めることができます。まずウィンドウ関数を次のように定義します。今回は1人のユーザ分のデータを使うためtimestamp順にデータを並べるだけですが、複数ユーザがいる場合は以下のようにパーティションを切ってウィンドウを定義します。上記で定義したウィンドウ関数を使って、前レコードの緯度経度カラムを次のように追加します。これでデータの下準備は終わりです。このデータにUDFを適用することで2点間の距離を求めていきますSparkにはScala spark, PysparkともにUDF(User Defined Functions)という概念があります。Spark UDFとは、Spark上でユーザが自由に定義することができる関数を表しています。ウィンドウ関数のlagはあらかじめ定義されていた関数であるのに対して、UDFでは”lat”, “lon”, “prev_lat”, “prev_lon”の4カラムを入力として2点間の距離を出力するような複雑な関数を実装することができます。まずは、2点の緯度経度を入力として距離を求めるScalaメソッドを次のように定義します。ここでは、haversine formulaという式を使って2点間の距離を求めます。(参考:https://en.wikipedia.org/wiki/Haversine_formula )今回は分かりやすくするために、簡易な実装で書いています。次に、このメソッドをUDFとして定義します。このステップを行うことで、定義した関数を自由に呼び出すことができます。最後に各レコードに対して前時刻のレコードとの距離を求めた結果のカラムを追加することで、求めたかった2点間の距離を求めることができます。Scala SparkのUDFとウィンドウ関数を使って緯度経度データから2点間の距離を求めるコードを実装しました。今回は位置情報データを扱う際の実装をイメージしていただくために簡単なお題をご紹介しました。実際に位置情報データを扱う際には大量のデータを扱うことになるため、パフォーマンスを考慮した実装を意識する必要があるというところに難しさと同時に面白さがあります。今回は位置情報データを分析するまでの前準備が主なトピックとなってしまいましたが、位置情報データをこういう風に扱ったほうがいいといったアイデアがあればぜひご意見を頂ければと思います。※本記事で使われている位置情報データは全てサンプルデータです。"
https://www.ariseanalytics.com/activities/report/20201120/,レコメンド#1 　～レコメンドって何？～,はじめまして、Marketing Solution Division所属の岩永と申します。Marketing Solution Divisionでは、主にKDDIグループ会社に対し、データ分析観点でのコンサルティング、ソリューションの提供などを行っています。ARISEでは現在、新規ソリューションとしてレコメンドエンジンの開発を進めています。レコメンドエンジンと聞くとあまり馴染みのない方もいらっしゃるのではないかと思い、今回を含め全3回で、レコメンド自体の概要から、どのような仕組みを用いているのか、具体的にどのように大規模データを扱っているのかなどを紹介する予定です。初回はレコメンドとは何なのか、一般的にどのような仕組みで動いているのか、などを紹介していきます。レコメンドエンジンは、推薦システムと呼ばれることもありますが、その名の通りユーザに何かをおすすめするしくみです。レコメンドエンジンやレコメンドという言葉に馴染みがなくても、普段ネットに触れていると、自然と触れている事が多いと思います。身近なものだとネット通販や動画配信サービスの「この商品(動画)を見た人はこちらも見ています」「あなたへのおすすめです」や、ユーザの属性や興味関心を元に表示されるターゲティング広告などが、まさにレコメンドです。例えば、マウスを探しているときに「こういうマウスもありますよ」「ご一緒にマウスパッドもいかがですか？」と教えてもらったり、ドラマや映画、アニメなどを1シリーズ見終わった後に「続編やスピンオフもありますよ」と提案してもらったり、ヒントとなるようなものを自動でおすすめしてくれるため、重宝している方も多いかもれません。ユーザが「マウスを探していたので、次はマウスパッドを探します」「1シリーズ見終わったので、関連作品を見ます」などと、明示しているわけではないのに、どうして行動を先読みできるのでしょうか。それは、以前に同じことをしている人が多いからです。通販や動画配信サイトでは、毎日大量のユーザから様々なアクセスが行われます。この人はこれを見た、その後にこれを見た、これは候補として表示されたけど、選ばれなかったなど、大量のログが蓄積されていきます。このログを用いることで、「今マウスを見た人がいるが、今までこのマウスを見た人はこのマウスパッドに興味を持つことが多かったので、きっとこの人もマウスパッドに興味を持つだろう」と、先読みして推薦できるのです。このような仕組みはと呼ばれ、レコメンドエンジンでは頻繁に用いられています。長くなるのと、わかりやすい説明がすでに多数なされているため、協調フィルタリングの説明はここでは割愛させていただきます。協調フィルタリング以外のアルゴリズムとしては、この商品を買った人には必ずこれを出すといったや、商品自体の情報を使うがあります。コンテンツベースについては後ほど説明します。協調フィルタリングを行う際のデータの大きさは、商品数*ユーザ数となります。商品数・ユーザ数が増えていくと、データの大きさが数億数兆のオーダーになることもあり、そのまま処理するには、相応のスペックのマシンと、時間が必要になります。現在我々が開発しているレコメンドエンジンの導入先では、商品数が数千万、ユーザ数が数百万であり、素直に協調フィルタリングを行うことができませんでした。そこで、を導入し、各ユーザのログをベクトル化することで、巨大なデータに対しても擬似的に協調フィルタリングを行うことができました。word2vecは、一般的には文書中の単語の出現傾向から単語にベクトルを付与する手法として、機械翻訳や文書要約など、自然言語処理の分野で用いられることが多い手法ですが、レコメンドで用いることもできます。word2vec自体の詳しい説明も、長くなるため割愛させていただきます。ユーザごとに閲覧した商品IDを時系列で並べword2vecを用いると、各商品にベクトルを付与できます。近くで出現することが多いID、すなわち近いタイミングで閲覧された商品は、ベクトルの値が近くなるように紐付けがされます。このベクトルが近い商品同士をレコメンドすることで、「擬似的に」協調フィルタリングを実現できました。「擬似的に」と書いたのは、厳密には処理の結果レコメンドされるものが協調フィルタリングとは異なるからです。word2vecでは単語1個ずつに注目し、その単語から一定範囲の前後にどういった単語が出現しているのか、出現頻度で紐付けを行います。上の例では、文中のeatという単語に注目しています。eatの1単語前にはto、1単語後にはmanyがあり、紐付けする範囲Wを1単語とした場合、この文ではeatにtoとmanyが紐付けされます。Wを2にした場合、wantとsushiも紐付けされるようになります。この紐付けする範囲Wをウィンドウサイズと呼びます。ウィンドウサイズを調整することで、一定範囲内の単語のみを紐付けします。用いるモデルによって異なりますが、ウィンドウサイズには5や10が推奨されています。注目する単語をずらしながら、大量の文章で学習を行います。学習が完了すると、ある単語が別のどんな単語と類似しているか(どんな単語と近くで使われやすいか)、数値で出力できるようになります。例えばeatとは、上の例のsushiや、ramenといった食べ物を表す単語や、restaurantやdishといった、食事を表す単語と類似度が高い、と出力されるでしょう。一方、computerなど、並べて使われることの少ない単語は類似度が低い、と出力されることが予想できます。レコメンドでは、商品IDを単語とみなし各ユーザが閲覧した商品のIDを時系列に並べることで、ある商品が別のどんな商品と近くで閲覧されているのかをもとに、商品間の類似度を出力できます。商品AからGまで順番に閲覧したユーザがいたとすると、商品Dの後に見た商品Eや、Dの直前に見た商品CはDと類似度が高そうだ、と考えることができます。一方で、商品Aと商品Gは閲覧したタイミングが遠く、あまり類似度が高くなさそうだと予想できます。この類似度を用いることで、閲覧ログからレコメンドを出力しています。ウィンドウサイズを無制限にすると、1ユーザの行動内すべてで紐付けが行われますが、一定数に設定することで、直近で閲覧された商品のほうがより類似度が高くなるようになります。1日のうち朝と夜に見たペアよりも、直前直後に見たペアのほうが類似度が高いと考え、直近に閲覧した商品に絞って紐付けされるようにしています。協調フィルタリングでは、「この人はAを見て、Bも見て、Cは見なくて、Dも見なかった」「別の人はAとBは見なかったけど、CとDは見た」といったログを用いて、「Aを見たならBも見るだろう」とレコメンドしています。一方、サイトを開設したばかりでユーザがほとんどおらず商品間の関連性がわからない、ということや、新商品を追加したためまだ誰も見ておらず、どういった商品と紐付ければよいのかがわからない、ということもあります。この問題はと呼ばれ、協調フィルタリングだけでレコメンドを行うのであれば、協調フィルタリングが使える程度のログが貯まるまで待つしかありません。これも協調フィルタリングの弱点です。しかしそれでは、他のレコメンドがしっかり表示されているサイトにユーザが流れてしまったり、いつまで経っても商品が見られず、ログが十分に蓄積されなかったりする可能性があり、別の対応をする必要がありそうです。そういったときに使えるのが、によるレコメンドです。コンテンツベースではユーザの行動履歴を参照するのではなく、コンテンツ(商品や動画)自体の情報を用いてレコメンドを行います。まだ誰も見ていない商品でも、その商品と似ているものがあれば出してしまおう、という発想です。例えば新しいマウスが登録されたとします。マウスはカテゴリとしては「PC周辺機器」に分類できますし、値段やメーカも、マウスという商品に付随した情報となります。説明文からは、無線なのか有線なのか、充電式なのか乾電池式なのか、なども読み取ることができます。こうした情報を使い、すでに登録されている商品から似たようなマウスを探し出して、レコメンドを出力できます。我々のレコメンドエンジンでは、word2vecを使って商品の説明文をベクトル化することで、コンテンツベースによるレコメンドを実現しています。先程説明した商品IDを並べる手法ではなく、説明文をベクトル化し近い商品をレコメンドする、という一般的なやり方です。(余談ですが、2種類の使い方があるため、文脈なしに「w2vのモデルが～」という話をすると、「どっちの？」となります。笑)しかし、コンテンツベースにも弱点があります。それは、似た商品しかレコメンドできないという点です。マウスを選んでいるときであればまだしも、すでにマウスを買った人に「今買ったマウスとこのマウス似てますよ！どうですか！」とおすすめしても、誰も買いません。また、「見た後」と「買った後」では出すべき商品が違ってくるので、どの画面に表示するかにより手法を使い分ける必要がありますし、ここまで紹介した2つの手法もバランス良く使っていく必要があります。今回は、レコメンドの一般的な手法についてお話しました。レコメンドは通販や動画配信サイトを始め様々なところで用いられており、よりユーザの興味関心に合致したものがおすすめされるよう、様々な手法が取り入れられています。過去には、単独企業による賞金付きのアルゴリズムコンテストもありました。次回は、大量のデータを並列処理するためにSpark上に機械学習モデルを載せ、分散推論させる話について掲載する予定です。お楽しみに！
https://www.ariseanalytics.com/activities/report/20201207/,データ分析におけるコーディング規約とフォーマッターの役割,こんにちは。Customer Analytics Divisionでデータサイエンティストをしている高田です。Customer Analytics Divisionでは、KDDIやauに関連するサービスのデータ分析や、それに伴うコンサルティングを行っています。データ分析ではpandasやPySparkを使うことが多く、私含むデータサイエンティストの多くの方が毎日Pythonを書いています。今回は、データ分析組織の業務効率向上を達成する過程でのコーディング規約の必要性、それに伴う取り組みであるフォーマッターの導入例をご紹介します。どのような事業にせよ、1人で1人分の仕事を達成することは容易ですが、5人でその5倍の大きな仕事を達成することは非常に困難です。組織の人数が増加すると、人と人が意思疎通する際のコストが発生するからです。プログラムにおいてもそれは同じです。誰かがその場で思うがままに書いたコードを別の人が読むのは困難です。そのコードを保守することも、再利用することも難しくなります。可能な限り保守性を高め、属人化を防ぎ、組織全体で効率的に業務を遂行するためには、ある程度ルールに則ったコーディングをすることが必要になります。その定められたルールこそ「コーディング規約」です。Pythonの一般的なコーディング規約としては、PEP8 [1] や Google Python Style Guide [2] が有名です。上記観点から、規模拡大を続けるARISEのデータ分析組織においてもコーディング規約の必要性が叫ばれるようになりました。コーディング規約を遵守することで発生するメリットやデメリットをざっくりと整理してみました。メリットは大きいのですが、どうしてもデメリットは生じます。特に、デメリット1の影響は非常に大きく、この確認や修正の時間が必要になるために、コーディング規約が結局守られないケースも生じてしまいます。特にデータ分析業務は、分析結果や分析速度は評価されやすい一方、分析過程や後続発展性は評価されにくいため、コードそのものを確認・修正する行為が疎かになりがちです。このデメリット1を大きく手助けするのがリンターやフォーマッターです。リンター：コードがコーディング規約に沿っているかを確認フォーマッター：コードがコーディング規約に沿っている状態へ自動整形現在ARISEの分析チームではBlack[3]というフォーマッターの導入を進めています。BlackはPEP8のコーディングスタイルに沿って自動でコードを整形します。フォーマッターを用いることで、デメリット1の時間を最小限に抑え、見た目に関するコーディング規約の多くを自動で解決することが可能です。具体的には、コンマの後にスペースを入れる、インデントはスペース4つ、引数が複数になる場合は改行する、などが該当します。Blackは、”The Uncompromising Code Formatter”という標語に表される通り、調整を許さない厳しいPythonフォーマッターです。初期設定からほとんど調整することができません[4]。https://github.com/psf/black#used-byDjangoの開発指針、Django Enhancement Proposals の DEP8 でも、Blackを使用しコードをフォーマットすることが採択されています。https://github.com/django/deps/blob/master/accepted/0008-black.rstBlackのインストールに加え、.ipynbファイルに記載された分析コードをJupyter Notebook上でフォーマットするため、以下の Jupyter Notebook Extension もインストールします。セルごとにコードをフォーマットすることが出来るようになります。https://github.com/drillan/jupyter-blackJupyter Lab を利用している方はこちらなどが利用できます。https://github.com/ryantam626/jupyterlab_code_formatterそれではインストールしていきましょう。これで準備は完了です。Blackを実行した際のコード変化例をお見せします。ボタンを押すだけで、一瞬で自動整形されるのがわかるかと思います。どのようにコードの見た目を整えるか、どのようにコードの見た目が整えられているのかについて、もう悩む必要はないのです。最小限の時間で、最低限の見た目を整えることが可能です。今回は、データ分析組織の業務効率向上を達成する過程でのコーディング規約の必要性、それに伴う取り組みであるフォーマッターの導入例をご紹介しました。フォーマッターが解決できる要素はコーディング規約が定める項目のごく一部でしかありませんが、その時間対効果は非常に大きいです。コーディング規約だけに留まらず、強いデータ分析組織であるために必要な取り組みをこれからも続けていきたいと思います。[1] PEP8 (https://www.python.org/dev/peps/pep-0008/ )[2] Google Python Style Guide (https://google.github.io/styleguide/pyguide.html)[3] Black(https://github.com/psf/black – Connect to preview)[4] とはいってもいくらか設定することがあります。例えば、line-lengthで1行の文字数上限指定、skip-string-normalizationで文字列をシングルクォートをダブルクォートにするのを無効化など。
https://www.ariseanalytics.com/activities/report/20201220-2/,レコメンド#2 Sparkで機械学習モデルを高速分散推論させる,"レコメンドエンジン連載の第2回目です。前回の「レコメンドって何？」はこちらを御覧ください。Analytics Delivery Division Initiative Center機械学習担当の下野です。Initiative Centerでは最新技術を用いて、分析の手法や環境を業務適用可能な品質で実現する活動しています。SparkクラスタをKubernetes上に構築し、分析者はボタン1つで高速に自分専用の分析環境を立ち上げられるソリューションを開発しています。KubernetesはEKSを使用しており、TerraformによりInfrastructure as Codeを実現しています。インフラから機械学習までフルスタックな開発業務を行っています。今回はC++でビルドされたネイティブコードを使用したモデル（ライブラリ）を用いてPySparkで分散推論する方法についてお話します。レコメンドで大量のデータを扱う際のお話しになるため、第1回からより技術的な内容になっています。背景と課題PySparkの分散処理における落とし穴PySparkによる分散推論方法分散推論を行うため調整すべきSparkの設定値ECサイトのお客様に「レコメンドエンジン導入」支援させていただいた時の話です。大きく2つの課題を抱えており、既存導入パッケージのレコメンドエンジンでは対応が難しくなっていました。数千万から数億件の大規模な商品数商品の入れ替わりが激しく、大半の商品がコールドスタート状態これらの課題に対処するため過去のARISEアセットを高度化する形でカスタムレコメンドエンジンを適用することになりました。コールドスタート（※第1回参照）については「商品タイトルと説明文」を用いて自然言語処理を行い、商品を数百次元のベクトル空間に埋め込むことで対処しました。埋め込まれたベクトル（分散表現）のCosine距離が近い商品をレコメンドすることで、似ている商品をレコメンドする手法です。ログデータを使わず、商品自体を解釈して類似商品を推薦するコンテンツベースレコメンドという手法です。自然言語処理にはFacebookが開発したfastTextを採用しました。第1回で紹介したWord2Vec系のモデルです。fastTextですが大元のコードはC++で書かれています。SWIGによりPythonのインタフェースを生成し、PythonからC++でビルドされたネイティブコードを呼び出しています。推論対象の商品が数億件に上るため、PySparkのUDFを用いて分散推論させることにしました。ここで落とし穴にハマります。SparkではDriverで実行コードをシリアライズしてExecutorへ配布します。Executorはコードをデシリアライズして実際に分散処理を実行します。この際PySparkではpickleやcloudpickleを使用してシリアライズを行います。ソースコードを確認するといくつかシリアライザーが用意されていることを確認できます。PySparkを使用した分散推論はだいたいこのようになるのではないでしょうか。上記のコードはsc.broadcast(model)でエラーが発生します。broadcastせずUDF内で直接modelを使用しても、上記のエラーメッセージのbroadcastがobjectに変わっただけの同じエラーが発生します。fastTextはことが原因です。C++でコア部分が書かれ、Pythonインタフェースが用意されているライブラリは機械学習では少なくないと思います。そこで、このようなライブラリを用いてPySparkで分散推論する方法をお話します。ライブラリ側でsaveメソッドが用意され、独自ファイル形式で保存されるモデルを使用して分散推論するケースでお役に立てると思います。前節ではfastTextがpickleでシリアライズできないため、PySparkのUDFで使用できないことを述べました。従ってPySpark上で分散推論を行うためには必要があります。配布方法は主に2通りあります。オブジェクトストレージ (S3, GCS等) を用いるSparkのメソッドSparkContext.addFileを用いる分散推論方法を簡単にまとめると次の通りです。fastTextのsave_modelメソッドを使用してモデルファイルを保存するオブジェクトストレージかSparkContextにモデルファイルをアップロードするUDF内でオブジェクトストレージからモデルをダウンロード&読み込みして推論する重要なことはモデルをファイルとして保存しておき、点です。fastTextに限らずpickleでシリアライズできないモデルオブジェクトを用いて分散推論が可能になります。fastTextのモデルをs3へ配置して分散推論するケースを例に、具体的な実装を見ていきましょう。分散推論のため3つの機能を作成します。s3ダウンロード関数シングルトンクラスfastText推論クラスシングルトンクラスについては後述します。s3ダウンロード関数を作成します。シングルトンクラスを作成します。推論用クラスを作成します。main実行部分を作成します。main実行部分のpredict_udfが実行されます。ここにC++ネイティブコードは含まれていないのでpickleエラーは発生しません。ドライバでシリアライズされ配布されたpredict_udfの内容を、各Executorはデシリアライズし実行しますタスク内でFastTextSentenceEstimator.predictメソッドが実行され、FastTextSentenceEstimatorをインスタンス化しますFastTextSentenceEstimatorがインスタンス化される際に__init__メソッド内でs3からモデルファイルをダウンロードし、モデルを読み込んでFastTextSentenceEstimator.modelにセットしますfastTextモデルのget_sentence_vectorメソッドにより、文章の分散表現を得ます同じタスク内の2レコード目以降の処理はシングルトンパターンにより、2で生成されたFastTextSentenceEstimatorが使用されます（モデルダウンロードと読み込みは発生しません）タスク数分2-5を繰り返しますこのようにC++ネイティブコードをpickle化する工程を避けることと、シングルトンパターンにより高速にPySpark上で分散推論を実現します。これまで触れられなかったシングルトンクラスの役割について少しお話します。シングルトンとはデザインパターンの1つで、ものを指します。クラスをインスタンス化すると都度つどメモリ上の別アドレスに新しいインスタンスが生成されます。しかしシングルトンパターンを用いると、。SparkのUDFで分散推論するにあたりシングルトンは、という大きなメリットがあります。通常UDFはDataFrameのレコード単位で処理を行うため、シングルトンを使用しない場合レコード数分モデル読み込みを行います。シングルトンを使用すれば、Sparkのタスク数分だけのモデルを読み込みすれば良くなるので、処理時間の大幅短縮が見込めます。Pandas UDFにおいてもシングルトンは有効です。Pandas UDFでは、spark.sql.execution.arrow.maxRecordsPerBatchで指定されたレコード数（デフォルトは10000）ごとにバッチ処理を行います。つまり1つのタスクで処理する行数がこの値を超える場合、1タスク内で複数回モデルダウンロードと読み込みをします。シングルトンを使用すれば、Pandas UDFにおいても1タスク内で一度のモデル読み込みで済ませることができます。シングルトンによるノーマルUDF実行時間の違いを、以下の実験条件で比較しました。モデルの読み込み時に5秒かかる処理を想定しています。画像のDurationカラムに注目してください。通常はレコード毎にモデル読み込みが行われるため、1タスク（10レコード）終えるのに50秒（5秒x10レコード）以上かかっています。シングルトンパターン適用時は、タスクの始めに一度読み込むだけなので1タスク5秒程度で処理が完了しています。ここまでオブジェクトストレージ（s3）を用いた実装を紹介しました。fastTextのようにモデルがシングルファイルで保存される場合にはSparkContext.addFileでモデルファイルをアップロードする方法が簡単です。ExecutorからはSparkFiles.getでファイル取得します。こちらの方法も簡単にサンプルコードを記載します。重要な点はaddFileをする必要があることです。推論クラスの方はSparkFiles.getでaddFileされたファイルを使用すれば良いです。オブジェクトストレージとaddFileの使い分けですが、個人的には基本的にaddFileを使用が簡単で良いと考えています。モデルファイルがディレクトリ構造で保存されるケース等、addFileで対応できないケースの際にオブジェクトストレージからダウンロードする方法を使用する使い分けに私はしています。ここまで紹介してきた3つのExecutorへのモデル配布手法を簡単な概念図にまとめると以下のようになります。broadcastでシリアライズして配布する方法 (今回のケースでは例外が発生)S3 (オブジェクトストレージ) 経由で配布する方法addFileで配布する方法 (今回のケースでは例外が発生)1はPythonオブジェクトのシリアライズを行うのに対し、2, 3の手法はモデルファイルを直接Executorへ配布しています。今回のようにSpark外のライブラリを使用して分散推論する際、UDFを安定して稼働させるためにはSparkの設定値を調整する必要があります。です。ベストプラクティス等に従ってExecutorのCPUやメモリ割り当てた状態では、モデルが使用するリソースを考慮していないため、タスクの並列度が高すぎてリソース不足により失敗することがあります。Sparkの設定値を適切に与えて分散推論UDF実行時に、やの発生を抑えることができます。分散推論手法を考えていた当時は、OOM Killerのようにメモリ不足によりUDF実行が失敗していたことが多々あり、原因特定に苦労しました。リソース不足によるUDFの失敗を避けるため行うことです。Executorインスタンス数とCPU割り当てを減らしタスク並列度を下げるExecutorインスタンス数調整のため、メモリ設定値を調整するモデルが使用するメモリ確保のためmemoryOverheadを多く確保する上記を行うため必要なSpark設定値です。公式説明はこちらを御覧ください。上記の値に決まった正解はないのですが、適切な値の探り方を紹介します。spark.executor.coresを1に設定してタスク並列度を下げる1 Workerノードで1 Executorしか起動しないようspark.executor.memoryとspark.executor.memoryOverheadを極振りするspark.executor.memoryとspark.executor.memoryOverheadを半分にして1 Workerノードで2 Executor起動するようにする3を繰り返し1 Workerノードで起動できるExecutor数を最大化する（繰り返すうちメモリエラーでUDFが失敗する）spark.executor.coresを増やし、spark.executor.memoryとspark.executor.memoryOverheadを減らし実行速度が最適となるタスク並列度を求めるメモリを大きく確保してExecutorのインスタンス数を抑えつつ少しずつ調整します。長くなりましたが今回の簡単なまとめです。ネイティブコードはPythonのpickleでシリアライズできないため、通常の方法だとUDFで使用できないオブジェクトストレージやSparkContext.addFileを使用することで、pickleする過程を避けSparkで分散推論が可能になる分散推論のUDFを安定させるにはSparkの設定値spark.executor.{cores|memory|memoryOverhead}を調整すると良いTensorFlow Kerasでも一部pickleによるシリアライズが推奨されておらず、今回の手法が有効です本家fastTextにもSparkで今回の問題が解決策となるissueが上がってましたSparkをバックエンドに使用できるApache BeamのGCPマネージドサービスDataflowのexampleにも、TensorFlowとシングルトンパターンで分散推論するものがありました"
https://www.ariseanalytics.com/activities/report/20201225/,GitHub Actionsは商用システムのCI/CDツールとして使えるか？！,"Advanced Tech Divisionでデータエンジニアをしている對馬（つしま）です。前回は分析モデルを商用化する際に考慮すべき5つのこと について書かせて頂きましたが、今回は少し毛色を変えてCI/CDについて書かせて頂きます。CI/CDはGitHub Actions、CircleCI、Travis CI、AWS CodePipelineなど、様々なサービスが存在しますが、今回はGitHub Actionsの魅力や商用サービスのCI/CDサービスとして実用に耐えうるか、について簡単に書かせて頂きます。GitHub Actionsは2019年11月に一般公開されたGitHub公式のCI/CDサービスです。サービスの詳細については公式サイトやQiita等に多くの記事が公開されているのでそちらを参照頂ければと思いますが、このサービスの魅力は何と言ってもパブリックリポジトリであれば完全無料で利用できることです。プライベートリポジトリについてはプラン毎に月当たりの無料枠がありますが、Freeプランでも2,000分/月なので1回のデプロイに5分かかった場合、400回/月デプロイできる計算になります。十分すぎます。参考：https://github.co.jp/pricing何となく安そうだけど商用システムで使いものになるの？ってところが気になるところだと思いますので次にまとめてみました。まず、私が商用システムで利用するCI/CDサービスに求めることは下記の通りです。低コストであることワークフローの定義ができること設定の管理がシンプルであることTerraform等のInfrastracture as codeのデプロイが容易にできることシェルスクリプトの記述ができること環境毎のデプロイが容易にできること環境毎のデプロイが容易にできること冒頭に記載した通り、GitHub Actionsはパブリックリポジトリであれば完全無料です。プライベートリポジトリであってもプラン毎に月当たりの無料枠が設定されておりますので無料枠の中で収めれば、CI/CDにかかるコストは０です。GitHub ActionsではYAML形式でワークフローファイルを作成できます。以下はシンプルな実装例です。上記の通り、YAML形式で1つのファイルで設定を管理するので扱いやすく、ワークフローは構造化するので可読性が高いです。AWSのCloudFormationやhashicorpのterraformなどで公開されているActionsが利用可能です。以下はterraformをAWSにデプロイするシンプルな実装例です。欲を言えばCircleCIのワークフローの手動承認機能が欲しいです。事前確認→承認→実行の流れでデプロイできると安心感を持てます。CI/CDサービスで用意された機能だけでは実現できないことに対応できるように個人的にはシェルスクリプトを書けることが絶対条件と考えています。例えば、node.jsのDOTENVファイルや、AWSのElastic Container RegistryにDockerイメージをPUSHする際のタグなど、デプロイの前段のビルド時に動的に設定したいものがある場合はシェルスクリプトを使わないと実現できません。「2. ワークフローの定義ができること」の実装例にある通り、GitHub Actionsはシェルスクリプトの記述が可能です。1つのリポジトリから開発環境や商用環境にデプロイしたいところですが、GitHub Actionsのシークレット(暗号化された環境変数)は環境毎に設定ができないので工夫が必要です。理想的には複数のシークレットを束ねるコンテキストが存在し、コンテキストを指定してデプロイしたいところです。（例えば開発環境へのデプロイ時に「DEVELOPMENT」というコンテキストを指定すれば開発環境用のシークレットを使用してデプロイできる）シークレットの設定イメージ（理想）しかし、2020年11月時点のGitHubは複数のシークレット束ねる機能は存在しません。シークレットの設定イメージ（現実）では、どうやって環境毎にデプロイするかというと下記の３つの実現方法があると考えています。環境毎にYAMLファイルを用意し、それぞれで参照するシークレットを書き分ける1つのYAMLファイル内で環境毎に条件分岐させてデプロイする環境のシークレットを設定するようにするリポジトリを開発環境用と商用環境用に分ける（開発リポジトリを正として、デプロイするソースコードを商用リポジトリに投入する）1つ目は似たようなファイルを重複管理する手間がかかる。2つ目は分岐条件が必要なのでワークフローが複雑になる。3つ目はリポジトリの管理する手間がかかるものの、1つ目や2つ目の問題点を回避できます。複数の環境（開発、ステージング、商用）にデプロイするには少し工夫が必要ですが、商用システムのCI/CDサービスとして十分使えるのではないかと思いました。簡単に評価結果を下記にまとめました。GitHub Actionsは一般公開されたばかりなのでこれからどんどん進化していくのではないかと思います。日本語の技術記事は徐々に増えてきてますが、商用システムで実用に耐えうるかについての記事は見当たりませんでしたので今回書いてみました。少しでも参考になれば幸いです。"
https://www.ariseanalytics.com/activities/report/20210122/,PylanceとVisual Studio Codeでバックエンドサーバを爆速開発,こんにちは。Marketing Solution Division(MSD)でAnalytics Platform(APF)を開発している山中です。MSDはデータ分析による顧客のマーケティングコンサルティング、機械学習マーケティング施策支援ツールの開発・導入支援を行っています。今年の6月にリリースされたPylanceをAPF開発にも採用しており、日々その恩恵にあずかっています。今回はPylanceの素晴らしさを実際の業務で感じた点にフォーカスして皆さんに紹介したいと思います。PylanceはVisual Studio Code(VScode)の拡張機能であり、Pythonの言語サポートを行ってくれます。下記に主だった機能を提供します。コード補完自動import(import文の自動挿入・削除)型チェックdocstring参照シグネチャ参照クラス定義参照詳細については公式ページ(https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance)を参照してください。ではこれらがAPF開発にどのように寄与するのでしょうか？APFのバックエンドサーバーはDjango REST Framework(DRF)というPythonフレームワークで構築されています。フレームワークの活用により、迅速なREST APIの構築が可能となります。しかしフレームワークで開発業務が効率化されるからといってコーディング作業自体は無くなりません。むしろフレームワークに沿った定型的で退屈な作業が増加します。またフレームワークが敷いてくれたレール通りに開発を行う場合には問題になりませんが、フレームワークの挙動を一部変更する・新たに機能を追加するといった高度な取り組みが必要になったときにはソースコードを読み解いてクラスの設計やそれらがどのように処理されているかを理解する必要があります。そのような場合にPylanceの採用には下記のメリットがあります。自動補完・サジェスト・auto importによるコーディング作業の効率化サジェスト機能・定義参照機能による挙動理解の効率化それぞれについて次節で詳しく見ていきましょう。Pylanceはフレームワークが提供する独自の型を持つインスタンスについても自動補完サポートします。あらかじめインスタンスの呼び出し時にtype hintをしておくことで、そのインスタンスメソッドについてサジェストや自動補完を行ってくれます。これによってコーディングの効率化やタイプミスの防止に貢献してくれます。(VScodeのスニペット機能と組み合わせることでより効率化できるかもしれません)またimport文の記述を行わずにソースコードを記述してもauto import機能が自動で必要なモジュールのimport文の挿入します。逆に使用しなくなったモジュールのimport文については自動で削除してくれます。Pylanceを導入しているとimportされているクラスやtype hintされているインスタンス(とそのメソッド)をマウスホバーすると、docstringやシグネチャを参照することが出来ます。またcommandキー + クリックでクラスやメソッドの定義を素早く参照することも可能です。以上の機能は、フレームワークが提供するインスタンスメソッドをオーバーライドして挙動を変更する際に、元々の挙動の確認やインスタンスがクラスがどのように呼び出されているか、ソースコードレベルで確認する際に役立ちます。(毎回ググってソースコードをgithubまで見に行く手間が省けます。)今回はPylanceの素晴らしさを、フレームワークを使った開発作業の効率化という点にフォーカスして紹介させていただきました。現在は他ツールと組み合わせてより開発の効率化が出来ないか模索中です。機会がありましたら今回ご紹介できなかった効率化の取り組みについても追加でご紹介できたらと思います。
https://www.ariseanalytics.com/activities/report/20210205/,Django REST Frameworkを使ってみる,"はじめまして。Marketing Solution Divisionに所属している新人データサイエンティストの山嵜です。ARISE analyticsでは、データ分析を通して様々な企業のマーケティングのサポートを実施しております。その一環として、自社開発プロダクトのAnalytics Platform(APF)を導入して頂くことにより、機械学習を用いたマーケティングの内製化支援もしています。前回の記事ではフレームワークを使った開発作業の効率化という点に着目し、Pylanceの良さをご紹介させて頂きました。今回の記事では、実際のフレームワークそのものに着目し、APFの開発にも用いられているDjango REST Frameworkについてご紹介します。Django REST Frameworkとは、一言で表すと「RESTful API開発に特化したDjangoのフレームワーク」です。Djangoは元々PythonでWebアプリを作成するためのフレームワークですが、TwitterやGithubなどのWebサービスにも用いられているRESTful API(※)を開発する上では、HTTP requestごとに機能を実装するためのアーキテクチャがないため、コードが煩雑かつ複雑になってしまいます。また、画面などの構成は含まずにデータだけをやりとりしたい場合には、専用の処理を実装する必要があります。そこで、そのようなニーズに答えるためにDjango REST Framework（DRF）が登場しました。(※) RESTful APIについて
「アドレス可能性」・「ステートレス性」・「接続性」・「統一インターフェース」の4つのRESTの原則に基づいて設計されたAPI。スケーラビリティの高さ、統一インターフェースなどのメリットがあり、昨今のWebサービスでは広く用いられています。より詳細な説明は他ページ参照（https://qiita.com/TakahiRoyte/items/949f4e88caecb02119aa）。PythonでRESTful APIを作成するフレームワークとしてはDRF以外にもFlaskやFastAPIなどが挙げられます。ここで、以下3つの基準にしたがってこれらのフレームワークを比較してみます。表現力（豊富なモジュールを持ち、少ないコード量で複雑なロジックを実装することができるか？リファレンスの充実度（参考文献の量・質の高さは十分か？）学習コスト（新規参画者でも数日〜数週間でキャッチアップ可能か？）簡単な設定をするだけでRESTful APIバックエンド構築の手間が省けることです。たとえば次のような機能がデフォルトで含まれています。ユーザー認証ユーザー・グループ毎のアクセス権の制御クエリ⽂字列によるページリソースの検索ページ毎のJSONレスポンス（ページネーション）アクセス回数などの制限（スロットリング）以上から、表現力の高さ・バックエンド構築の手軽さなどを考慮し、APFの開発ではDRFを用いてAPIを開発しております。ここまでDRFについて説明してきましたが、「そうは言うけど実際DRFってどうなの？」と思っている方もいらっしゃるでしょう。そこで以降では、DRFを使って実際にRESTful APIを実装してみます。今回はDRFの表現力の高さに着目し、3つの書き方を紹介します。

まず、HTTP requestの「POST（登録）」とPOSTのタイミングで別の処理（ここではhogeメソッド）が実行されるスクリプトを以下2つの書き方で実装していきます。a.rest_framework.views.APIViewを使用b.さらに、POSTと「GET（取得（一覧））」を同時に実装する例として、c.rest_framework.mixins.CreateModelMixinを使用する書き方を紹介してます。また、ページの都合上実装はViewのみとさせていただき、Model・Serializerについてはすでに実装済みとします（コード中ではそれぞれ、本のモデル<Book>、本モデルのシリアライザ<BookSerializer>と表現）。DRFについてのより詳しい説明は公式および、Djangoの公式を参照してください。（DRF公式、Django公式）対応するHTTP requestの名前でメソッドを記述してあげるだけで実装することができます。aの場合よりもさらに記述量が少なく、HTTP requestに対応するメソッド（ここではcreate）をoverrideすることで実装することができます。rest_framework.mixinsbの場合とほぼ同じ書き方ですが、それはbのコード中のgenerics.CreateAPIViewはもともとmixins.CreateModelMixin継承したものであるためです。mixins.ListModelMixinを継承し、querysetを追加するだけでGETも実装されます。今回はRESTful API実装フレームワークDRFについての紹介と実装例をご紹介させていただきました。
ARISE analyticsではAPF以外にもさまざまなプロダクトを開発しております。機会がありましたら今回ご紹介できなかった他のフレームワークについてもご紹介できたらと思います。"
https://www.ariseanalytics.com/activities/report/20210304/,レコメンド#3 GPUで近似近傍探索を行うことで大規模データの計算時間を、12時間から50分へ約1/12に削減したお話,"この記事はレコメンドエンジン連載の第3回目になります。前回までの記事はこちらを御覧ください。レコメンド#1 ～レコメンドって何？～レコメンド#2 Sparkで機械学習モデルを高速分散推論させるはじめまして、Marketing Solution Division所属の野尻と申します。19年度にARISEに新卒入社してから約1年間レコメンドエンジンの開発を担当しています。今回は商品間の類似距離を計算する際に近似近傍探索×PySparkを用いることで、大量の商品に対しての計算時間を当初の12時間から50分まで、大幅に削減したお話をします。背景と課題最近傍探索について近似近傍探索について転置インデックスについて直積量子化についてFaissの利用法Index作成学習探索その他の注意点FaissとGPUを使用するまでの経緯まとめ第2回とほぼ同じ内容ですが、簡単にご説明します。本シリーズはECサイトのお客様に「レコメンドエンジン導入」の支援をしたときの話です。レコメンドエンジンを導入するにあたり、大きく2つの課題を抱えておりました。数千万から数億件と、商品数が多いこと商品の入れ替わりが激しい為、大半の商品がコールドスタート状態であることコールドスタートについては「商品タイトルと商品説明文」を用いて自然言語処理を行い（※第1回参照）、fastTextを用いて商品を数百次元のベクトル空間に埋め込むことで対処しました（※第2回参照）。しかし、数百次元のベクトルを持つ数千万件の商品の類似度組み合わせ計算を行う際に、全ての組み合わせ計算を行うとメモリに乗り切らず、処理速度も現実的な時間に収まらないという課題を抱えていました。本題である近似近傍探索についてお話する前に、最近傍探索についてご説明させていただきます。最近傍探索とは、とあるベクトルに対しもともと保持していたN個のベクトルから1番距離が近いベクトルを探索する問題のことです。例としては、「アイドリッシュセブン」「アイドルマスターシャイニーカラーズ」「鬼滅の刃」といったコンテンツのタイトルがベクトル空間に埋め込まれていた場合に、「アイドリッシュセブン」に対して近いタイトルのコンテンツを探したい！という場合に用いられます。その際に使用される距離は、2点間の直線距離であるユークリッド距離や、ベクトル同士がどれぐらい同じ角度を向いているかを計算するCosine類似度が用いられることが多いです。最近傍、つまり自身と1番近いベクトルを探索する際にはN個のベクトルとの距離を逐一計算する必要がありますが、その際にネックとなるのがベクトルの個数の増加による計算時間の増加です。ベクトルが2つだった場合は2×2、3つに増えた場合には3×3…といったように2乗オーダーで増加して行くため、先程の課題で挙げたように1000万商品に対しては1000万×1000万で100兆、1億商品に対しては、1億×1億で1京と爆発的に増えていくため、現実的でない計算時間が必要となってしまいます。そのため、本プロジェクトでは次項でご説明する近似近傍探索を用いることで、現実的な計算時間に収めることに成功しました。近似近傍探索とは、最近傍探索に比べ厳密な近傍点を求めず、近傍点との距離を近似計算することで、計算時間を抑えながら近傍探索を行う手法です。本プロジェクトでは近似近傍探索を高速化するための方法として3つ採用しています。転置インデックス直積量子化GPUの使用また、これらの方法を組み込むに当たり、Faissというライブラリを使用しています。Faissとは、Facebook社が開発を行っている近似近傍探索のOSSで、転置インデックスと直積量子化も実装されています。コア部分がC++とCudaで実装されていますが、SWIGによりPythonインターフェースが用意されており、Python上でも動かすことが可能となっています。公式URL:facebookresearch/faiss転置インデックスとは、全ベクトルを予め決められた数のクラスタに分割し、各クラスタ内で重心点を求めそれにクラスタのIDを紐付けたものを保持しておき、またそれに紐付いたクラスタ内の全ベクトルを別で保持しておく方法です。この方法のメリットとして、検索速度の向上が挙げられます。とあるベクトルに対して全ベクトルとの距離を計算するのではなく、ひとまず各クラスタの重心点に対して距離を計算し一番近い重心点を持つクラスタを見つけ、そのクラスタ内でまた距離を計算するという処理により検索を効率よく行うことができます。Faissにおける動作としては、元のベクトルデータをそのまま用いてk-meansを行っており、train部分で行われる1つ目の処理になります。k-meansに関してはネット上に文献が多くあるため、ここでは割愛します。また、実際のコードサンプルは後述します。注意点として、trainの時点ではまだベクトルが登録されておらず、実際に登録されるのはその後のaddという処理の部分になります。こちらも詳細は後述したいと思います。直積量子化とは、分割したベクトルごとにベクトル量子化を行う方法のことです。まずベクトル量子化とは、観測対象となるベクトルの集合を有限個の代表ベクトル（コードブック）で表現することです。具体的な手法はk-meansで求めた各クラスタの重心点（コードブック）を保持しておき、とあるベクトルがどのクラスタの重心点に近いかを求め、一番近いクラスタの重心点のIDをベクトルとして扱う手法です。これを行うことで、観測ベクトルの表現に必要なデータサイズが、N次元 × 観測ベクトル数から1次元 × 代表点の個数まで圧縮できます。k-meansのクラスタ数（K）は大きければ大きいほど保持する重心点が増え、その分とあるベクトルに対してより近い重心点を持つクラスタを提示することができ精度が上がりますが、大きくした分各重心点との近さを求める回数が増え量子化が遅くなってしまうので注意してください。また、量子化後のベクトルは基本的に8bitのuchar（0~255の整数）で扱われます。Faissではnbitsというパラメータに該当しますが、現状だとGPUの場合だとnbits=8しか指定できないため注意してください（将来的に4, 5, 6, 8が利用できるように開発中のようです）さらに元々のN次元ベクトルをM分割し、N/M次元に分割されたサブベクトルそれぞれに対してベクトル量子化する手法を直積量子化と言います。直積量子化を行うことで、データ量の圧縮を行うことができ、その後の探索時間の大幅な短縮が可能となります。何分割するかは、分割した後のサブベクトルの次元を指定することで決まります。元の次元数÷分割後のサブベクトルの次元数=分割数となり、この分割後のサブベクトルの次元数が大きいほど、データサイズは小さくなるが近似の精度が下がります。逆に次元数が小さいほど、近似の精度は上がりますがデータサイズは大きくなります。例として、128次元の32bitの浮動小数点のベクトルだとデータサイズは4096bit（128×32）必要になりますが、直積量子化を行い128次元を4次元のサブベクトルに32分割し圧縮を行うと32次元×8bitで256bitとなり、1/16にデータサイズを圧縮することが可能です。これらのパラメータは精度と処理時間に直結するため、許容できるデータサイズや精度、実行時間等とご相談しながら決定してください。実際にFaissで利用するときのサンプルコードや注意点を説明していきます。trainを行うときのサンプルコードは以下になります。インデックス作成部のパラメータについて説明していきます。resはGPUを使用するときのリソースの設定です。一時的に割り当てるメモリのサイズ等を設定する必要がありますが、FaissではStandardGpuResources()というものが事前に用意されており、GPUのメモリサイズに応じて一時的に割り当てるメモリサイズを変更してくれる処理が実装されています。よって基本的にはStandartGpuResources()を指定しておけば良いと思います。もし割り当てるメモリサイズを下げたい等のチューニングが必要な場合はご自身で設定する必要があります。dimはindexに学習しようとしているベクトルの次元数を指定します。そのまま次元数を入力するだけで大丈夫ですが、ベクトルの次元数における注意点として、後述する分割後の次元数で割り切れる次元数でないとともそも量子化を行えない点に注意してください。計算時間を抑えたかったから分割後の次元数を32や64にしたかったのに、割り切れないから泣く泣く小さい次元数にして、計算時間が長くなってしまった…といったことにならないように要注意です！nlistは前述した転置インデックスのk-meansを行った後の分割後のクラスタ数です。nlistは大きければ大きいほど1クラスタ当たりのベクトル数が減り、その分探索時に計算対象となるベクトルの数が減るため速度が早くなります。しかし、その分本当はクエリのベクトルに近かったのに検索対象とならないクラスタにいたために、探索結果にそのベクトルが出現しない確率が上がってしまいます。よってnlistの値は速度と精度のトレードオフになります。検索対象とするクラスタ数は、後述のnprobeというパラメータで設定出来るため、そことの兼ね合いもあることにも注意してください。nlistについては、公式のissueで通常これで良いという計算式が提示されているため、それを用いて算出すると良いと思います。参考issue:How to select a suitable ‘nlist’? · Issue #112 · facebookresearch/faissMは分割後のサブベクトルの次元数です。作成するindexがGpuIndexIVFPQの場合は[1, 2, 3, 4, 8, 12, 16, 20, 24, 28, 32, 48, 56, 64, 96]の中から指定する必要があります。直積量子化の項でお話しましたが、分割後のサブベクトルの次元数が大きいほど、データサイズは小さくなるが近似の精度が下がります。逆に次元数が小さいほど、近似の精度は上がりますがデータサイズは大きくなります。トレードオフなので、処理時間と相談の上決めると良いと思います。またdim / Mが割り切れる値でないと動作しないため、注意してください。nbitsは分割後のサブベクトルのbit数です。現状GPUで動かす場合は8bitのみとなっています。よって基本的にnbits = 8と指定してください。（後述するivfpq_configでivfpq_config.interleavedLayout = Trueと設定すると4, 5, 6をnbitsに設定することが出来るようですが、開発中のためTrueにしないでくださいとの記述がありました。）metricは距離計算時のアルゴリズムです。選択出来る主なアルゴリズムは内積やL2ノルムです。この2つはほとんどのアルゴリズムでサポートしていると書かれているので、この2つから選択するのが一般的だと思います。内積: metric = faiss.METRIC_INNER_PRODUCTL2ノルム: metric = faiss.METRIC_L2ivfpq_configはindexに対するその他の設定です。設定は4つあります。これは計算した残差の距離のテーブルに対して16bit浮動小数点を用いるかどうかの設定です。Mを大きくした結果、GPUのメモリにサブベクトルが乗り切らなかった場合にTrueに設定するようです。これは事前計算されたテーブルを使用するかどうかという設定です。事前計算されたテーブル=コードブックのことだと思われます。デフォルトではFalseに設定されていますが、コードブックは使用するためivfpq_config.usePrecomputedTables = Trueと設定する必要がありそうです。ただし、公式wikiにはIf you see cudaMalloc errors, disable precomputed tables.との記載があるため、GPUのメモリが足りずエラーが出てしまった場合にはFalseと設定する必要がありそうです。こちらについては、GPUでは「開発中なので使用しないでください。」と記載されています。よってGPUを用いる場合はデフォルトのFalseのままで良いと思われます。CPUの方で確認すると、Whether or not our index uses an interleaved by 32 layoutと記載されていました。interleavingとは、メモリなどの記録媒体にデータを記録する際に予めデータを一定量で分割しておき、各媒体に同時に送ることで書き込み速度を向上させる手法のことです。おそらく予めベクトルを32個つづで分割しておき、各メモリに同時に書き込むことでメモリへの書き込み速度を向上させるオプションなのではないかと考えられます。この変数を用いたエラー文の実装のみされているようなので、今後の開発に期待ですね。これはコードブックを使用せずに、強制的に距離計算を行われる設定で、This is for debugging purprosesと書いてあったためデバッグ用の設定のようです。実際にTestGpuIndexIVFPQ.cppのコード内で使用されていました。以上がindex作成時のパラメータの説明になります。次にtrain部の説明と注意点です。index.train(vector_array)trainメソッドの内部で実際のベクトルを用いて2つの処理を行っています。1つ目の処理は、先述した転置インデックス用のk-meansです。ベクトルがnlistで指定した分のクラスタに分割され、各ベクトルで重心点のベクトルを保持します。2つ目の処理は、ベクトルを登録するときの直積量子化に用いるコードブックの作成です。こちらは上記の処理で作成した転置インデックスの各クラスタの重心点とクエリベクトルの残差を直積量子化しています。残差を用いることで、近似精度を高める工夫を行っています。注意点として、学習に用いるベクトルは32bit浮動小数点の二次元配列にのみ対応しているという点があります。たとえベクトルの数が1つの場合でも、二次元配列にする必要があるため注意してください。また、float64やfloat16でベクトルの配列を作成していた場合は、trainの手前でfloat32に変換する必要があります。次にベクトルの登録です。ベクトルの登録は、index.add()という関数で行います。train時に作成したコードブックを用いて、クエリのベクトルの直積量子化を行い、量子化後のベクトルを登録しています。また、登録する際には登録された順に0始まりで連番の整数IDが付与されます。こちらも上記と同じように32bitの浮動小数点にしか対応していないため注意してください。また、各ベクトルに対してユーザーが独自で定義したIDを紐付けたい場合はindex.add_with_ids()を使用します。ベクトルと同じ長さのIDの配列を用意し、ベクトルと一緒に登録します。こちらの注意点としては、64bitの整数のみ対応している点です。int32等だった場合は、add_with_idsの前にint64への変換が必要です。int64_index_array = index_array.astype(np.int64)次に探索部についての説明と注意点です。探索はindex.search()という関数で行えます。こちらがコードサンプルになります。では、パラメータについての説明です。kは探索する際に、クエリに近いベクトルをいくつ取得するか、というパラメータです。k = 100と指定した場合は、探索結果が100個得られます。kは大きければ大きいほど多くの結果を取得しますが、その分探索時間が長くなる上に、探索後のデータサイズも大きくなるため、取得したい数と処理時間のバランスで設定してください。また公式wikiにはk <= 2048との記載があるため、2048以下で設定してください。nprobeは、探索する際にクエリに対して一番近い重心点を持つクラスタを見つけた後、その周りに存在するクラスタに対して、何個のクラスタを探索しに行くか、というパラメータです。重心点が一番近いクラスタ外にもクエリに対して近いベクトルを登録しているクラスタは存在する可能性があるため、大きければ大きいほどよりクエリに対して近いベクトルを発見できる可能性が高まります。しかし精度が上がる分探索時間も長くなるため、処理時間とのバランスでこちらも設定してください。存在するクラスタ数以上は見に行くことが出来ないため、nprobe <= nlistとなっているのと、kと同じくnprobe <= 2048と公式wikiに記載されているため、そちらにも注意してください。次に実際の動作についてです。まずindexにクエリベクトルが与えられるとコードブックを用いて量子化を行い、量子化後のベクトルと各クラスタの重心点との距離を計算します。そして一番近い重心点を持つクラスタを特定し、その重心点とクエリとの残差を計算します。そしてそのクラスタ内に登録されているベクトルの残差との距離を計算し、より距離が近いものを取得します。返り値としては、距離とIDを近い順に格納した2つの配列が返却されます。探索を行う際の注意点として、kで指定した分のベクトルが取得できなかった場合にはIDの配列に-1の値が挿入される仕様のため、ID配列内の-1は除外するといった処理が必要になります。その他の注意点として、faissのGPU用のindexを一度ファイルとして保存したい場合は、CPU用のindexに変換する必要があります。その後保存したインデックスを読み込んで、再びGPUにインデックスを載せたい！となった場合は、GPU用のインデックスに変換する必要があります。一般的なユースケースとして、ベクトルの学習・登録をGPUで行い、その後のAPI等に用いる際にいはCPUで動作させるといったケースがあると思われますが、その際にはこちらの方法で変換・保存を行ってください。本プロジェクトではバッチで事前計算する必要があったため、複数のGPUマシンを用意し、PySparkを用いてFaissの探索を分散処理させています。こちらについては第2回の「Sparkで機械学習モデルを高速分散推論させる」という記事内で用いているfastTextをFaissに置き換えたような話になるため、そちらの記事をご参考ください。ここまでFaissとGPUを使用した近似近傍探索について解説してきましたが、そもそもどのような経緯でFaissとGPUを使用したかを簡単にご説明したいと思います。背景と課題で挙げたとおり、本プロジェクトでは数百次元のベクトルを持つ数千万件の商品の類似度組み合わせ計算を行う際に、全ての組み合わせ計算を行うとメモリに乗り切らず、処理速度も現実的な時間に収まらないという課題を抱えていました。一番最初のアプローチとしては、ダメ元で全ベクトル×全ベクトルの距離をPySparkのUDFを用いて計算を行っていました。しかし、Sparkがメモリエラーで落ち、想定通り計算できませんでした。対向システムの関係上、バッチで事前計算しておく必要があったため、どのようにしたらレコメンドエンジンのバッチ処理として現実的な時間で計算が行えるかを考えました。そこで出た案が以下の2つでした。事前にk-meansを行いある程度クラスタリングした上で、各クラスタ内でベクトル同士の距離計算を行うFaiss等のライブラリを用いて近似近傍探索を行う1については、同じく計算が終わることはなく途中で断念しました。そして次にFaissを利用しました。近似近傍探索を行うライブラリはいくつか存在しますが、GPUが利用できることベクトルIDのマッピング機能があることインデックスファイルがシングルファイルなため、PySparkのUDFで使用しやすいことバッチ検索が高速なため、PySparkのPandas UDFと相性が良かったこと以上4点の理由からFaissを選択しました。2のFaissのCPUによる近似近傍探索では処理を完了することはできたものの、約12時間の計算時間を要することが判明し、毎日レコメンドリストを連携する必要がある本プロジェクトにおいては実用的では無いと判断し、こちらも断念しました。さらなる高速化を目指した結果、同じFaissでもGPUを用いて実装することで、実用的な時間に計算時間を収めることにチャレンジしました。それまでMarketing Solution Division内ではGPUを使用した前例が存在せず、そもそも我々が利用できる基盤上にGPUを構築することができなかったため、基盤運用チームにGPUを利用できる環境を用意していただくところからのスタートとなりました。その後環境を用意していただきGPUを用いてFaissを実行したところ、CPUでは約12時間必要としていた処理を、に短縮することができました。皆様のプロジェクトで、大規模データで近似近傍探索を短い計算時間で行いたいとなった場合は、FaissとGPUを用いることを一考してみてはいかがでしょうか。今回は、GPUで近似近傍探索を行うことで大規模なデータでも処理時間を大幅に削減することができたというお話でした。近年は大規模データを扱う機会が増え、計算時間にお困りの方も多いのではないかと思います。しかし、クラウド上でGPUを使用できるサービスも登場し、気軽に使用できる機会も増えたのではないでしょうか。プロジェクトの予算にもよるとは思いますが、もし計算時間でお困りの方がいらっしゃいましたら、GPUを用いての計算時間短縮を試してみてください。大幅な時間短縮になれば、CPUとGPUのコスト差を上回り、計算時間短縮&コスト削減になる可能性があります。3回に渡ったレコメンドの記事は今回で最後となります。稚拙な文章ではありましたが、記事を読んでくださった皆様ありがとうございました。"
https://www.ariseanalytics.com/activities/report/20210312-2/,オフライン環境でのVSCodeを使ったSpark開発環境構築,皆さんこんにちは、Marketing Solution Divisionの伊藤です。私は2020年の4月に新卒として入社し、研修期間終了後に現在のチームに配属されました。現在は、KDDIの位置情報データを活用し、マーケティング対象の店舗に立ち寄りそうな顧客を予測するシステムを制作しております。このプロジェクトでは、位置情報や顧客の個人情報という秘匿性が高い情報を扱うため、セキュリティの観点からインターネットから隔離された分析環境でシステムの開発環境を構築する必要がありました。さらに、システムの処理や開発方針決定のための分析にApache Sparkを使用していたため、分析環境とおなじプライベートネットワーク内でAmazon EMRを使用する必要がありました。今回開発環境を構築するにあたって、エディタにはVisual Studio Code(VSCode)を採用することにしました。VSCodeは軽量にもかかわらず、強力な拡張機能群により快適なコーディングが可能です。SparkのPythonラッパーであるPysparkを用いたコーディングの際にも、Python関連の拡張機能により様々な便利機能(強力な補完、デバッグ・テスト機能、Jupyter Notebookの使用など)が使用できます。上記のようなセキュアなオフライン環境でも、VSCodeで快適にSparkを使用する環境を整えたので、実際に実施した設定方法を紹介したいと思います。(記事内の画像は別環境のWindowsのものなので様々な点が実際の環境と異なりますが、ご了承ください。)ホスト・EMRがともにオフラインな環境でVSCodeを使用してEMRに接続する方法Pysparkシェル以外からPySparkを実行する際に設定が必要な環境変数VSCodeでPySparkを使用したコードをデバッグ・テストする方法EMRとSSH接続する際の鍵の作成Python以外の言語でのSparkの設定RedHat系のLinux(セキュリティ上の理由で詳細は割愛します)Visual Studio Code 1.51.1Remote-SSH 0.56.0Python 2020.11.371526539Jupyter 2020.11.399280825Amazon EMR 6.1.0Spark 3.0.0Python3.7.8ARISE Analyticsでは大量のデータを処理するため、Amazon Web ServiceのEMRを使っています。システム開発にあたって、EMRに接続して簡単にコードを実行できるように設定する必要がありました。今回のシステム開発では、メインのエディタとしてVSCodeを使用することになったので、VSCodeの拡張機能であるRemote-SSHを使用して、VSCodeから直接EMRを触れるようにしました。しかしながら、Remote-SSHはホストまたはEMRのどちらかがインターネットに接続する前提で作成されているため、今回のようなオフライン開発環境の場合、そのままでは上手く動作しません。そこで、オフライン環境でも利用できるようにRemote-SSHを改造し、使用するまでの手順を以下に示します。まずはインターネット環境で必要なファイルをダウンロードします。ARISE Analyticsの分析環境はRedHat系のLinuxなので、VSCodeの公式サイトから本体のrpmファイルをダウンロードします。また、こちらのページの右側にある、Download ExtensionをクリックしてRemote-SSHのvsixファイルをダウンロードします。今回のシステム開発ではPysparkを使用して開発を行うため、先ほどと同様にしてPythonとJupyterの拡張機能をダウンロードします。これらのファイルをオフライン環境に移送出来たら、「sudo yum localinstall」を使ってVSCodeのrpmファイルをインストールします。次に、VSCodeを起動し、拡張機能タブの右上から「install from VSIX…」または「VSIXからのインストール」を選択して、先ほどのvsixファイルをインストールします。ここまでで、オフライン環境のローカルでVSCodeを利用する準備が整いました。Remote-SSHは接続先にVSCodeのサーバー版をwgetやcurlでダウンロードしてからインストールする仕組みになっているため、ここまでの作業ではサーバー版VSCodeがダウンロードできず、利用することができません。そこで、先ほどオフライン環境にインストールしたVSCodeに対応したファイルをインターネット環境でダウンロードする必要があります。インストールしたVScodeを開き、Help>Aboutをクリックすると、VSCodeのバージョン情報が表示されます。Commitとなっている部分の文字列を確認し、以下のようなリンクから対応したバージョンのファイルをダウンロードします。(もしくは、VSCodeのGitHubリポジトリから各バージョンに対応したコミットidを確認してもよいです。)https://update.code.visualstudio.com/commit:{$コミットの文字列}/server-linux-x64/stable例えば、バージョン1.5.1の場合https://update.code.visualstudio.com/commit:e5a624b788d92b8d34d1392e4c4d9789406efe8f/server-linux-x64/stableのようになります。このファイルをオフライン環境で利用できるWEBサーバに移送しておきます。次に、Remote-SSHにオフライン環境用の改造を施します。「~/.vscode/extensions/ms-vscode-remote.remote-ssh-{$Remote-SSHのバージョン}/out/」にある「extension.js」をエディタで開いてください。そして、「DOWNLOAD_URL=」と検索して出てきた部分を先ほどオフライン環境に移送したファイルのURLに変更します。(バージョンによっては変わっているかもしれません。(画像は別環境のVSCodeのRemote-SSH 0.63.0の場合ですが、0.56.0でも同様の箇所を変更します。)これでRemote-SSHの改造が完了しました。Remote-SSHは「~/.ssh/config」を読み込んで、接続先の一覧を表示します。そのため、以下のようなEMRの接続設定を追記する必要があります。(configファイルがなければ作りましょう。)以上までの作業を行うと、VSCodeのRemote Explorerタブに上記で設定したEMRが表示されるようになります。EMRをクリックして、接続できるようになっていれば成功です。VSCodeにはPythonコードの実行やデバッグ、テストなどで便利な機能がたくさん搭載されています。しかしながら、EMRのデフォルトのSparkの設定ではpysparkコマンドやspark-submitコマンドから実行することが前提になっているため、PySparkを使ったPythonコードはそれらの便利な機能を使用することができません。ここでは、通常のpythonコマンドでもPySparkを実行できるように環境変数を設定し、VSCodeの便利な機能を使用できるようにする方法を紹介します。pysparkコマンドやspark-submitコマンドでは、Sparkのインストール先にあるPySparkをPythonが読み込むパッケージに追加してからコードを実行しています。しかしながらこの方法では、通常のpythonコマンドやエディタの補完機能などはPySparkパッケージを発見できず、使用できません。そこで、環境変数のPYTHONPATHに必要なパッケージの場所を追加します。また、PySpark実行時にはSPARK_HOME、PYSPARK_PYTHON、PYSPARK_DRIVER_PYTHONの3つを設定する必要があります。EMRでPython3を使用したい場合、これらの環境変数を以下のように設定する必要があります。(Sparkやpythonのインストール先や設定によって異なるので、適宜読み替えてください)ところで、VSCodeでコードを実行する場合、統合ターミナルで直接実行する方法とデバッグ/テスト用のターミナルで実行する方法の2つがあります。bashrcで設定する場合は問題ないはずですが、VSCode上で設定したい場合、統合ターミナル、デバッグ/テスト用ターミナルそれぞれで設定が必要です。VSCodeの統合ターミナルは設定ファイル(プロジェクトのパス/.vscode/setting.json)で環境変数を設定できます。setting.jsonを開き、以下の内容を追記します。デバッグ/テストの際は、上記設定が適用されません。そのため、デバッグ/テスト用の設定ファイルを作成します。デバッグタブを開くと、launch.jsonを作成するといった趣旨の文が出てくるので、クリックします。選択肢がいくつか出るので、Pythonを選択します。次に、環境変数を書き込んだファイル(.env)をプロジェクトルートに作成します。launch.jsonはデフォルトでこのファイルを読み込んで環境変数を設定してくれます。.envファイルの内容は以下のようにします。これで、デバッグ/テストでもPySparkを使用できるようになりました。今回は、EMR、PySpark、VSCodeを組み合わせて使用する方法を紹介しました。SparkやEMRの設定は難しく、日本語の情報も少ないので今回の記事が皆様の役に立てば幸いです。
https://www.ariseanalytics.com/activities/report/20210326/,web閲覧履歴を用いた広告ターゲティングモデルとは?,初めまして、2020年4月に新卒として入社したMaketing Solution Division (MSD) 所属の小林と申します。普段の業務ではweb広告の支援をしております。具体的にはターゲティング広告の支援を行っており、web閲覧履歴データ・機械学習の手法を用いてモデルを作成し、どのような人にweb広告を表示させるべきかという広告配信リストを提供しております。本記事では広告ターゲティングモデルがどのようなものなのかを紹介したいと思います。web広告業界において機械学習がどのように応用されているか知っていただけると幸いです。ターゲティングモデルの紹介に当たり、2つのトピックに分けて説明していきたいと思います。1.web閲覧履歴データとは2.ターゲティングモデルについて・目的変数作成への活用・特徴量作成への活用・作成したモデル紹介この章ではどのようにしてユーザごとのweb閲覧履歴データを収集しているかについて説明していきたいと思います。まずデータ収集の説明の前にwebバナー広告の仕組みについて紹介します。あるwebサイトにバナー広告が表示される際に、広告枠を獲得するため広告主同士間で競売（オークション）が行われます。競売の中で最高入札金額を提示した広告主の広告がサイト訪問者へ表示されます。この一連の流れを図とフローで表すと以下になります。ユーザーがwebサイトにサイト訪問【webバナー広告の仕組み図】この競売が発生したタイミングで広告表示枠が存在するweb閲覧ページのURL情報が蓄積されます。このような流れでユーザーのweb閲覧履歴データが収集されます。この章ではweb閲覧履歴データのモデル作成への活用方法・実際に作成したモデルを紹介したいと思います。ターゲティングモデルは、広告反応しやすい人を当てるという目的で作成しています。そのようなモデルを構築しスコアが高い人に限定して広告配信（ターゲティングリストを作成）をすることで、無作為に広告配信するよりも小さいコストで大きな広告効果を得ることができます。ターゲティングモデルは基本的に2値分類問題を想定し作成されます。2値分類問題とはある事象に対して正解・不正解のラベル付けが行われ、それらの予測を行う問題になります。ラベルに相当するデータを目的変数といいます。では、目的変数の定義・作成のプロセスを紹介したいと思います。現在行っている方法は2種類あります。①特定のサイトを閲覧したかどうか②タイトルに特定ワードを含むwebページを複数閲覧したかどうか①については、バナー広告をクリックすると広告主が作成したランディングページ(LP)に遷移されるためその訪問履歴を正解データとして目的変数を作成することがあります。その際にもいくつかパターンがあり、LP訪問を正解としたり、広告主が設定したコンバージョン(CV)ページへの訪問を正解としたりします。広告でのCVというのは、広告主が設定したユーザーの最終アクションのことを意味しています。例えば、端末購入完了やLPからプラン料金シミュレーションページへの遷移などがあります。目的に即した正解ラベルを設定することで、バナー広告をクリックしてLPに訪問したり、CVする人の類似者を見つけ出したりすることができます。②については、webページにはタイトルがあるため特定ワードがタイトルに含まれるページを閲覧した人を正解データとして目的変数を作成します。例えば「5G」という単語を特定ワードとしてモデルを作成した場合、「5G」へ興味がある人たちを見つけ出すことができます。特定ワードの設定により、①の方法では見つけられなかった広告反応潜在層を特定可能になります。また、特定ワードを変えるだけで様々なモデルを作成できるため、あらゆる広告施策に対応可能な汎用性も兼ね備えています。web閲覧履歴データをモデルに特徴量として組み込む際に数値データに落とし込んでいます。そこで使用するデータ加工方法がリフト値を用いたターゲットエンコーディングになります。リフト値とは条件X（目的変数が1）のときに事象Y（あるドメイン・URLを閲覧する）が起こりやすいかどうかを評価する値となっています。リフト値はターゲットユーザーが見やすいURL・ドメインを定量化するために用いられています。具体的には、各webページのURL・ドメインごとにリフト値を算出し、それらのリフト値表から各ユーザーごとに閲覧URL・ドメインの抽出およびリフト値の総和を取ることで、ユーザー単位でのリフト特徴量が作成されます。フローにすると以下のようになります。データ収集期間を指定し、予測対象者の閲覧ログデータを抽出リフト値の算出方法について説明します。リフト値は以下の計算式で算出されます。計算式から導かれるように、目的変数1のユーザーがよく見るサイトかつ目的変数が0のユーザーが全く見ないサイトの場合、リフト値が高くなります。そのため、リフト値が高いURL・ドメインを確認することでどのようなweb閲覧傾向があるかを知ることもできます。具体的な例としてプログラミングに興味がある人を予測モデルを作成する場合を紹介します。この場合の目的変数を1 : プログラミングの教育コンテンツを受講した人0 : プログラミングの教育コンテンツを受講していない人にします。サイトA : プログラミング言語に関するサイトサイトB : 食べ物に関するサイトがあり、下表のような統計情報があったとします。■webページ閲覧統計情報それぞれのサイトA・サイトBのリフトを算出するとサイトA_リフト値 = (85 / (85+186)) / (41231 / (41231+1550562)) = 17.64サイトB_リフト値 = (5 / (5+43207)) / (41231 / (41231+1550562)) = 0.004となります。各サイトのリフト値より、サイトAを訪問しているユーザーはプログラミングに興味がある、とモデルは判別するようになります。業務において作成したモデルを一部紹介したいと思います。紹介するモデルは携帯のオンラインショップ興味予測モデルになります。※機密性の観点からデータは実在しないページタイトルにしています。(ダミー化)こちらのモデルはオンラインショップを利用してくれそうな人を予測するモデルになっています。目的変数はオンラインショップにて買い物をしたかという条件にして作成しています。このような目的変数からweb上位リフトを表にしました。リフト値表から、オンラインショップで買い物をした人がスマホ端末のレビューやオンラインショップの利用方法ページを見ている傾向にあることが分かります。web閲覧傾向からオンラインショップにて購入をする人がどのような端末を検討しているかプランを検討しているかといった行動を把握することができます。またオンラインショップで買い物をする人が閲覧しそうなページがリフト上位に来ているため、オンラインショップ予測モデルとして上記リフト値を特徴量として組み込んだ場合にオンラインショップ興味者をターゲティングできるのではないかということが分かります。広告ターゲティングモデルがどのようなものなのか・web広告業界において機械学習がどのように応用されているかという観点で書いてきました。作成したモデル紹介からも分かる通り、目的変数をしっかり決めることさえできれば、様々なモデルが作成可能です。またwebリフト値作成の方法はどのモデルも同じなので1つのコードを使いまわすことができ、モデル作成効率もよいです。今後も様々なモデルを作成し、ターゲティング広告支援を引き続き行っていきたいと思っています。
https://www.ariseanalytics.com/activities/report/20210409/,めっさ分かりやすい因果推論 (その1) 概論とMeta-learner系手法,"ARISE Tech Blog2021.04.09Tweetめっさ分かりやすい因果推論 (その1) 概論とMeta-learner系手法はじめにこんにちは、Customer-Analytics-Divisionの徳山と申します。本稿では因果推論手法の全体像を解説します。筆者は英語の通訳やカスタマーサクセス職を経てARISE analyticsに入社し、クライアント企業様の主力事業における販促施策について、施策に対する反応(『この施策によってどれくらいこのお客様はXXXという行動を取るようになったか？』)を推定するような取り組みを行っています。因果推論は多くの分野で活用されていますが、用語のとっつきにくさと内容の難しさが絡み合うことで初学者(∋筆者)の学習を大きく妨げていると感じています。本稿がそうした方の入門の一助になれば幸いです。因果推論の概説『因果推論(causal inference)』を人間味のある形に言い換えるならば、「集団に対して何らかの『介入』を行った時に、その介入で特定の変数(目的変数)がどれほど押し上がったか、その押し上がり具合を『推定』する方法」だと言い換えられます。この押し上がり度合いには『CATE(Conditional Average Treatment Effect ケイト：条件付き処置効果)』という名前がついています。例えばあなたが自社サービスをより多く購入してほしいと思うマーケティング担当者で、メールで割引クーポンを送る場合、メールを送ることが介入で、商品売上などが目的変数に据えられます。ここで、キャンペーンによってどれくらい売上が伸びたかを当然知りたいと思うでしょう。これは以下の式で表現できるはずです：Aさんに対するメール施策(介入)による売上(目的変数)の押し上がり効果 =『①Aさんにメールを送った(介入した)場合の売上の値』 -『 ②Aさんにメールを送らなかった(介入しなかった)場合の売上の値』ここで問題が生じます。Aさんは一人しか存在しないので、Aさんに対して①か②のどちらかしかデータ上観測することはできず、①-②を実際には計算できないのです。これを『因果推論の根本問題(*1)』といいます。では掲題の押し上がり効果をどう測るかというと、因果推論の分野ではAさんと類似した人でメールを送ってはいないBさんを探し、『推定』値として以下を算出します：Aさんに対するメール施策(介入)による売上(目的変数)の推定押し上がり効果 = 『③Aさんに介入した場合の目的変数の値』 – 『④Aさんと似ているBさんに介入しなかった場合の目的変数の値』冒頭の因果推論の定義で押し上がり効果を『推定』することと書いたのは、この算出値があくまで『予測値』でしかないことを強調したかったためです。この推測こそが因果推論であり、いかに妥当な推定効果を求めるかがデータサイエンティストの腕の見せどころでもあります。因果推論手法の概説(図：筆者作成の社内LT資料より抜粋)因果推論の各手法は、まず機械学習の用い方でMeta-learner系とCausal-Tree系に大別できます。Meta-leranerは、因果推論を行うための論理のうち、多くて2箇所にピンポイントで機械学習を施す処理のことです。第一に、施策の対象になった人(TG[Treatment Group]といいます*2)とそうでない人(CG[Control Group]といいます)がどのように振り分けられているのか、両者の属性的傾向を学習するモデルが必要です。言い換えると、施策による介入有無(TGかCGか)を目的変数とした予測モデルということです。例えば先のメールのキャンペーンですと、メールを送る対象は(メールを開いてくれそうで、商品を買ってくれそうな)購入経験の多い若者に寄りがち、一方でメールを送らなかった人全体はその逆の中高年に寄りがち、といったバイアスが生じている可能性があります。『③-④』の計算は感覚的にも分かる通り(*3)似ている顧客どうしで行う必要があり、その似ている度合いを定量表現する手段が必要そうですよね。それがこの第一のモデルです。因果推論の分野では『傾向スコア(propensity score)』という名称がついています。(*4)正確に言いますと、傾向スコアは③の人と④の人の類似度を直接評価しているわけではなく、それぞれが『(典型的な)TGにいそうな顧客なのかどうか』を表現しています。傾向スコアを以て『TGにいそう(=あぁこの方はこれまでXXをたくさん購入しているしキャンペーンとの相性がよさそうだったからキャンペーンメールを送ったのだった)』な程度が同じお客さん同士をあぶりだし、③にTGに『TGにいそうでやっぱりTGにいたお客様』、④に『TGにいそうだけどCGにいたお客様』をあてがって計算を行うイメージです。第二に、TG内、CG内での目的変数を予測する学習モデルが必要です。再び『③-④』の式から察していただける通り、最終的にはTG対CGで目的変数のスコアを比較することになるので、そのための前準備という意味合いです。他方のCausal-Tree系は、因果推論の論理フローをまるっと決定木ベースの教師あり学習手法で置き換えた手法群です。本稿では触れませんが、いずれ別の機会に寄稿したいと思います。Meta-learner別の解説S-learner (Single-learner)S-learnerは、介入されたかどうかを特徴量に含めた目的変数予測モデルをつくり、Aさんについて『TGだった場合のスコア』-『CGだった場合のスコア』を算出し、その差分を施策介入による押し上がり具合(=CATE)とみなす手法です。S-learner以外の手法でもそうですが、ここで用いられる機械学習モデルに特に制限はありません。Xgboostであれ線形回帰的な手法であれ適用可能です。Meta-learnerの各手法のパフォーマンスを比較した論文があり(*5)、これによるとS-learnerはCATEが多くの顧客において0であろうという仮説が経っている場合に有力な手法だと示されています。キャンペーンは送っているが売上はほぼ増進しないだろう、クーポンをお送りしても解約リスクはほぼ押し上げないだろう、といった仮定ですね。また、S-learnerはTGとCGの同一データセット内に取り組んで学習をさせるため、一般論としてはいずれかのデータセット件数が著しく少ない場合には向いていません。TGとCGの件数が概ね一致する場合によく効果を発揮する手法だと整理できるでしょう。T-learner (Two-learner)T-learnerではTが示すとおり機械学習モデルを2つ用います。すなわち、TG・CGそれぞれで目的変数の予測モデルを構築し、Aさんについて以下をCATEとみなします。AさんのCATE = 『TGモデルで算出した目的変数予測スコア』 – 『CGモデルで算出した目的変数予測スコア』このモデルが優れているのは、TG・CGで別個のモデルを使うために、両者に属性的な類似性が担保できていない場合でも活用ができるという点です。TGは20代中心、CGが50代中心という構図をとってしまった場合、目的変数(売上、解約、etc.)に対する反応性が両者で大きく異なりそうなことが推測されます。ここでS-learnerであれば、両者の区分けが施策介入有無という1つの特徴量でしかモデルに与えられませんが、T-learnerはそもそもの学習モデルを分けるため、TG・CGそれぞれにおいて目的変数に対する反応性を十分に学習できることが期待されます。また、T-learnerはTG・CGどちらかの件数が少ない場合にでも活用できる手法です。キャンペーンメールを送ったTG顧客が1,900人で、同時期にメールを送らなかったCG顧客が100人だったという場合、S-learnerにそのままこのデータを食わせてしまってはCG側の顧客特性をモデルが十分に学習できない可能性があります。T-learnerであればあくまでCG内での顧客特性を学習したモデルを使用できるので、そのような懸念をはじめから払拭することができます。X-learnerX-learnerは上記2手法よりも複雑で、以下の5手順をもってCATEを算出します。(1). 顧客がTGであるか、CGであるか(=傾向スコア)を学習するモデルを構築する(2). TG・CGそれぞれで目的変数の予測モデルを構築する(s-learnerと同じ)(3). (2)のモデルを使って以下を計算する。D1：TG顧客に対し、『目的変数 – (2)のCGモデルによる目的変数予測スコア』の値D0：CG顧客に対し、『(2)のTGモデルによる目的変数予測スコア – 目的変数』の値(4). (3)の計算結果を使って以下のモデルを構築するM1: TG顧客に対してD1を予測するモデル、すなわち、『自分がTGだったら目的変数の押し上がり効果がどれくらいですスコア』を予測するモデルM2: CG顧客に対してD2を予測するモデル、すなわち、『自分がCGだったら目的変数の押し上がり効果がどれくらいですスコア』を予測するモデル(5). 各顧客に対しM1とM2からスコアを算出し、(1)のモデルの結果で重み付けを行う。すなわち、TGっぽい顧客ほどM1が、CGっぽい顧客ほどM2のスコアに影響されるように調整する。(3)において、D1は『自分がCGでなくTGであることで、どれくらい目的変数が押し上がったか』、D2は『自分が母集団ではなくCGであることで目的変数が押し上がったか』を意味しています。(4)でそれらを予測するモデルをつくり、(5)でそれらにTG/CG振り分け確率をあてがうことで、最終的に『自分がTGだったら/CGだったら目的変数がどれくらい押し上がるのかスコア(=CATE)』を算出します。その他のlearner他にもr-learner等のmeta-learner系手法があるようで、筆者も勉強中です。終わりに本稿では因果推論の概論とMeta-learner系の各手法の全体像の説明を図りました。読者諸賢がこの分野を学び始める際の一助となれば幸いです。次の執筆機会には、因果推論手法のもう一つの体系であるCausal-tree系手法について論じる予定です。ARISanalyticsの中で筆者が担当している分析支援のプロジェクトでは、非常に大量の顧客データからこのような分析を行うことが多く、常々ビジネススケールの大きさを感じることができます。拙稿をご覧くださった方に、実際に弊社でこういった分析を行うことに関心を持っていただけたら望外の喜びです。最後までご覧下さり、ありがとうございました。(拝)(補記・引用元メモ)*1: 安井翔太・株式会社ホクソエム(2020).『効果検証入門 – 正しい比較のための因果推論／計量経済学の基礎』. 技術評論社.P.7より*2: 全くの余談ですが、TGはTreatment GroupであってTarget Groupではありません。*3:感覚的に整理すれば、『サッカー好きのおじさんについての情報 – 野球好きの若者の情報』の計算結果は何の意味もなさそうではないですか？サッカー好きのおじさんに比べられるべきはあくまでサッカー好きのおじさんであって、その似ている度合いを定量評価するものが傾向スコアです。*4:英語には少しのこだわりがあるのでこの語の和訳のされかたについて1点主張させていただきます。propensityの元の形容詞である‘propense’という語には、~しがちな(類：inclined)、前のめりな、勉強熱心なといった意味があります(pro-は前にという意味の接頭語でproactiveのproと同じです。かたや、『傾向』という語は、『人口が増加傾向にある』『私はいつも右足から立ち上がる傾向にある』というように行為(XXする)に対しての傾きの意で使われることが多いかと思われます。propensity scoreという語は、因果推論分野では本文中の議論の通り『TGにいがちなスコア』というニュアンスで理解されるべきだと思われます。このpropensity scoreを『傾向』スコアと訳してしまうと、この語が何に対しての傾きを示しているのか分かりづらくはないでしょうか。あえて言えば、筆者としては『傾向スコア』は誤訳であり、語の本来の意味を尊重すると『偏向スコア』または『典型性スコア(=TGにいがちであるスコア)』といった日本語訳をあてるべきではないかと考えています。*5: Soeren R. Kuenzela , Jasjeet S. Sekhon, Peter J. Bickela , and Bin Yu (April 25th 2019.)Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning.arxiV. Available at: https://arxiv.org/pdf/1706.03461.pdf (Retrieved April 8th 2021.)(おしまい)"
https://www.ariseanalytics.com/activities/report/20210423-2/,【効果検証】差分の差分法とは？,ARISE Tech Blog2021.04.23Tweet【効果検証】差分の差分法とは？Marketing Solution Division所属の長谷井です。Marketing Solution Divisionでは、主にKDDI関連会社に対し、データ分析観点でのコンサルティング、ソリューションの提供などを行っています。今回のトピックマーケティングの領域ではユーザーに対して、施策を実施します。施策の結果を確認する際、効果検証は重要なプロセスです。効果検証を行う上で様々な手法が昔からあります。みなさんは差分の差分法（Difference in difference : 以下DIDと呼びます）という効果検証方法をご存知でしょうか？古くから存在し、認知されている手法で、アイデア自体はいたってシンプルです。本記事では筆者が過去分析案件でDIDを活用して効果検証を進めた経験を活かし、皆さんにその活用方法や注意点をお伝えします。DIDとはなにかこちらの過去記事でも記載している通り、効果測定では複数の個体に対してなんらかの介入をした時に、介入を受けていない個体との比較により介入の影響の大きさを推定します。先述の過去記事では、介入群とその比較対象となる対照群の間でサンプルの特徴に偏りがある場合においてこれに起因するバイアスを除去するため、傾向スコアを用いて特徴の似たサンプル同士で介入効果を比較する方法を紹介しました。しかしこの手法では、介入の影響範囲が広い場合にバイアスの除去が難しい問題があります。例えば、対象の地域全体で販売促進施策を実施しており、施策対象と異なる地域の店舗を効果測定の比較対象とせざるを得ず、地理的な差に起因する影響を取り除くことが難しい、といったケースです。このようなケースに対して、DIDでは目的変数の時系列推移が介入群と似ているグループを比較対象として、介入前後の目的変数の変化を比較することで対処します。ここでは小売業における販売個数の効果検証を例に、DIDによる効果検証の方法について解説します。DIDにおける検証では効果を測定したい施策対象に対し、その対象と同様のトレンドで推移することが見込まれる対照群を設定します。たとえば小売業で特定商品の売り上げを上げたい時に施策を打つとします。DIDで検証する場合、まずは店舗の最寄り駅からの距離や店舗所在地の地域特性、近隣で突発的なイベントの発生状況が類似している店舗を複数探し出し、施策を実施する店舗と実施しない店舗（対照群店舗）に分けます。その後、施策効果を以下の手順で計算します。施策実施店舗と対照群店舗の施策前の売上差　（②－①）・・・A施策実施店舗と対照群店舗の施策中の売上差（④－③）・・・B上記の差　（B-A）　・・・施策効果※図の見方①：施策実施前の合計販売個数（対照群店舗）②：施策実施中の合計販売個数（施策実施店舗）③：施策実施前の合計販売個数（対照群店舗）④：施策実施中の合計販売個数（施策実施店舗）DIDを使う条件DIDには比較対象の2郡が互いに平行なトレンドを持つという前提条件があります。つまり、DIDを行う前に2郡の平行トレンドを証明しないといけません。言い方を変えると、施策を実施した店舗と似たような販売個数のトレンドを持つ店舗を対照群としないと、比較できなくなります。平行トレンドは特定の期間の販売個数をグラフで可視化し、確認できます。たとえば以下の図のように、平行トレンドが成り立っている2店舗の販売個数は時間経過とともに平行に変化していきます。図ではわかりやすいように常に上昇するような例になっていますが、実際には販売個数は上がったり下がったりと波のようになるケースが多いです。明らかに平行であれば、可視化するだけでも十分平行トレンドを確認できますが、目視で確認できるほど明らかな平行トレンドの見つけ出しが可能なケースは稀です。そこで、数値で確認できるユークリッド距離やコサイン類似度等を用いてこれらを確認し、トレンドがより似ている店舗を対照群とします。DIDを実務で使う際に苦労する点施策を実施する店舗と類似する対照群店舗は簡単には見つけられません。たとえば分析可能なデータの期間が限られている場合、トレンド確認に必要なデータが十分に確保できないかもしれません。もしくは十分なデータがとれていても、販売個数に大きな影響を与える突発的なイベントが一部の店舗に発生し、平行トレンドが確認できない場合もあります。想定外の事態にDIDは対応しきれません。シンプルな計算法で効果検証をできるというDIDのメリットを生かすために、販売個数に影響を与えうる因子が発生しないように分析設計をする必要があります。おわりにDIDの使い方と注意点について簡潔に説明しました。本記事が少しでも読者の皆様の役に立てれば幸いです。
https://www.ariseanalytics.com/activities/report/20210521/,物体検知モデルを実用向けに速度チューニングする,"ARISE Tech Blog2021.05.21Tweet物体検知モデルを実用向けに速度チューニングするARISE analyticsの秋元です。画像処理システムの開発では様々な画像処理技術をシステムに組み込んでいきますが、システムの要件に応じて適切なモデルや実行方法を選択する必要があります。今回は画像処理技術の一つである物体検知のモデルを実用向けに速度チューニングするという例を通して、画像処理システム開発の裏側の努力をご紹介します。物体検知とは物体検知とは画像の中から「犬」や「自転車」といった特定のオブジェクトを検出する技術です。例えば、下図は有名な物体検知モデルであるYoloの物体検知結果の例です[1]。物体検知モデルは画像を入力としてBounding Boxという物体を囲む矩形とそれに対応するクラスラベルを出力します。物体検知は画像処理技術の中では基本的なタスクの一つで、物体追跡や姿勢検知など様々な応用タスクの土台となる技術です。Yoloによる物体検知結果（出典：https://pjreddie.com/darknet/yolo/）近年ではYolo、Faster-RCNN、SSD、RetinaNet、CenterNet等様々な手法が提案されており、日々State-of-the-artを更新しながら多くの研究者が高精度で高速な物体検知モデルを発表しています。物体検知の精度としては既に実用に足る水準に達しつつあり、実際、画像処理技術を応用したソリューションが次々と発表されています。研究成果と実用化の間の壁さて、このように進歩が著しい物体検知モデルですが、実際の業務における実用化を考えた場合には研究成果として発表されているモデルの性能と実用レベルの性能との間には高い壁が存在します。特に動画像をリアルタイムで処理するシステムの場合、モデルの「実行速度」が問題となる場合が多く、モデルの推論にかかる時間をどの程度高速化できるかというチューニングが重要になります。ここでいう実行速度とはモデルに画像を投入してから物体検知結果を返すまでの推論にかかる時間をさしますが、なぜこれが高い壁なのかを説明します。例として、次のようなリアルタイムの画像処理システムを考えます。クラウド上に物体検知モデルを搭載した画像処理システムを構築し、顧客の現場に固定カメラを設置して撮影した動画をクラウドに伝送し物体検知の推論を行います。推論した結果はユーザの手元のタブレット等にリアルタイムで表示させるようにします。このシステムでは画像送信処理を開始してからユーザに結果が届くまで、図のように「画像送信処理」→「ネットワーク伝送」→「物体検知推論処理」→「ネットワーク伝送」→「結果表示処理」というプロセスを経由します。処理に必要な時間はとなります。システムはリアルタイムを目指しているのでこの処理時間の和をできる限り0に近づけたいところですが、実際にはネットワーク伝送など物理的な距離に依存する部分も存在するため、現実的には画像送信処理から結果表示処理まで数秒以内（例えば3秒）を目標としてみます。画像送信処理やネットワーク伝送には意外と時間がかかる上、物体検知推論処理でも映像のデコードや検知結果の後処理といった複数のプロセスが必要なため純粋な物体検知にかけられる時間はかなり限られます。今回は物体検知にかけられる時間（t2）は画像1枚当たり最大0.5秒を目標としましょう。目標の推論時間が定まったので、次は画像処理システム上で動作する物体検知推論処理のアーキテクチャを検討します。研究成果として報告されている数値は多くの場合、高性能なGPUを用いた推論結果であることが多く、例えば物体検知モデルの一つであるYolov4の論文では50FPS～100FPS以上という非常に高速な結果が出ると記載されていますが、推論には「GeForce RTX 2080 Ti」というGPUが用いられています[2]。問題は、画像処理システムをソリューションとして提供する場合に推論用のマシンにGPUを使用するとランニングコストが非常に高価になるということです。そのため今回のようなソリューションに組み込む画像処理システムは、多くの場合、より安価なCPUを搭載したマシンで稼働させる必要があり、マシンスペックに制約がある状態で高速な推論を実現しなければなりません。これが、画像処理の実行速度における研究領域と実用化との間の高い壁です。速度チューニングの前準備ここからは、実際に最新の物体検知モデルの一つであるYolov4を使ってCPUマシンで速度チューニングをしていきます。チューニングにあたって、まずは速度を検証するための評価データおよび評価方法、CPU環境を確認しておきます。評価データにはMicrosoft COCOデータセット[3]を使用します。MS COCOデータセットは物体検知など画像処理タスクのために構築されたデータセットで、33万枚もの画像が含まれます。今回はこのうちval_2017の評価データ5000枚の中からランダムに500枚を抽出して速度計測および精度評価を行います。また、精度の評価指標にはAP（Average Precision）と呼ばれる指標を用います（今回の記事では詳しく説明しませんが、物体検知ではよく使われる指標で数値が高いほど精度が良い指標です）。速度のチューニングを行う場合は、最終的には推論精度とのトレードオフになります。つまり、どこかの時点で「これ以上推論速度を上げる場合には推論精度を落とさなくてはならない」というポイントが来ます。今回、推論速度だけではなく推論精度も一緒に計測していくのは上記のような理由によるものです。次に、評価用のマシンとしてIntelの「Xeon(R) Silver 4116 CPU @ 2.10GHz」を8コア積んだものを使用します。メモリは24GBです。Yolov4のモデルはこちら（https://github.com/AlexeyAB/darknet）を参考にしてMS COCOで学習済みのものを使用します。そのため、今回の検証ではモデルの構造には手を加えず、モデルのパラメータや実行フレームワークを中心としてチューニングを行っていきます。Yolov4ではよく使われる画像の入力サイズが何種類かありますが、まずは608×608のサイズで検証を行います。チューニングを始める前に、まずGPUを積んだマシンでYolov4がどの程度の精度・速度を出せるのか確認しておきましょう。Yolov4のモデルはDarknetというフレームワーク上で実行します。Tesla T4を搭載しているAWSのg4dn.2xlargeインスタンスで評価用データ500枚の推論処理を実行し、実行速度と検知精度を算出したところ、下記の結果となりました。今回のシステムの推論速度の目標値は0.5秒なので、GPUを搭載したマシンを用いてDarknetフレームワーク上で実行すれば軽く目標をクリアできることがわかりました。では、これからYolov4をCPUに載せ替えて推論速度を計測し、チューニングを行っていきます。まずはシンプルにCPU上で推論を実行まずは単純にモデルをCPU上で実行してみます。実行フレームワークはPyTorchを用います。実行速度だけを考えるとC言語で実装されたDarknetフレームワーク上で実行することも考えられるのですが、今回ターゲットにしているような画像処理システムは（弊社の場合）Pythonで実装されることが多く、モデルを実行するフレームワークもPythonで取り回せると都合がよいため、まずはPythonベースの深層学習フレームワークであるPyTorchに学習済みのモデルを載せて実行してみます。DarknetのパラメータをPyTorchで実行するやり方はこちら（https://github.com/Tianxiaomo/pytorch-YOLOv4）を参考にさせていただきました。評価用データ500枚に対して処理実行したところ、実行結果はとなりました。推論時間は画像1枚当たり1.118秒となり、目標値0.5秒に対して2倍以上かかってしまっています。単純にCPU上で実行しただけではシステムに搭載できるほどのパフォーマンスが出せないことがわかりました。ここから、試行錯誤を繰り返して目標値の推論時間0.5秒以内を目指します。推論プラットフォームを変えて最適化①：onnxruntime学習済みのモデルの推論時間をチューニングする場合、モデルの構造を変更することは基本的にできないので、モデルをより最適な推論プラットフォームにコンバートして各プラットフォーム上で推論を高速化できるようパラメータを調整していく流れになります。まずはCPU上での推論プラットフォームとしてよく取り上げられるonnxruntimeを試してみます。PyTorchで試したモデルをさらにonnx形式に変換します。PyTorch→onnxへの変換は簡単で、下記のようなスクリプトで変換可能です。model = Darknet(config.cfg)model.load_weights(config.darknet)input_names, output_names = ['input'], ['boxes', 'confs']x = torch.randn((1, 3, model.height, model.width))torch.onnx.export(model, x, <PATH TO ONNX FILE>, export_params=True, opset_version=11,do_constant_folding=True, input_names=['input'], output_names=['boxes', 'confs'], dynamic_axes=None)変換したonnx形式のモデルをonnxurntimeで推論させてみたところ下記のような結果となりました。PyTorchで推論した場合と比べて0.3秒ほど遅くなっています。モデルによってはonnx形式に変換することで実行速度が速くなるものもあるようですが、Yolov4の場合は逆に遅くなるようです。念のため、onnx形式のデータをonnxoptimizerで最適化したバージョンでも実行速度を計測してみます。最適化処理は下記のように行います。onnxoptimizerの最適化は計算グラフの中で推論時には冗長になる部分をよりシンプルな形に置き換えるといった最適化処理を行うもので、例えばBatchNormalizationをConvolutionに還元したり、一度だけの計算で済む処理を別のグラフに切り出して計算回数を削減したりといったことを行います。model = onnx.load(<PATH TO ONNX>)OPTIONS = ['eliminate_deadend','eliminate_identity','eliminate_nop_dropout','eliminate_nop_monotone_argmax','eliminate_nop_pad','eliminate_nop_transpose','eliminate_unused_initializer','extract_constant_to_initializer','fuse_add_bias_into_conv','fuse_bn_into_conv','fuse_consecutive_concats','fuse_consecutive_log_softmax','fuse_consecutive_reduce_unsqueeze','fuse_consecutive_squeezes','fuse_consecutive_transposes','fuse_matmul_add_bias_into_gemm','fuse_pad_into_conv','fuse_transpose_into_gemm','lift_lexical_references','nop',]model = onnxoptimizer.optimize(model, OPTIONS)with open(<PATH TO OPEIMIZED ONNX>, 'wb') as f:f.write(model.SerializeToString())onnxoptimizerによる最適化後の推論時間は下記のようになりました。最適化前に比べて若干速くなってはいますが、それでも目標速度である0.5秒には及びません。推論プラットフォームを変えて最適化②：OpenVINO次は、Intelのハードウェアに最適化された推論プラットフォームを持つOpenVINOを試します。記事執筆時点でOpenVINOによる推論を行う方法は、onnx形式のモデルを読み込ませて推論する方法モデルをIR形式に変換して推論する方法の2つの選択肢があります。後者の方法に関してはyolov4のパラメータをIR形式に変換する必要があり、こちら（https://github.com/TNTWEN/OpenVINO-YOLOV4）を参考にしてモデルの変換を行いました。2つの方法でそれぞれOpenVINOによる推論を実行したところ、処理時間は下記のようになりました。IR形式では若干精度が下がるものの、onnx形式に比べると多少は速度が速いようです。しかし、このままでは目標速度である0.5秒以内をクリアするのは難しそうです。推論精度を犠牲にして速度アップを目指すここまでで、精度を落とさずに目標速度0.5秒をクリアするのはかなり難しいことがわかってきました。そこで、次は多少精度を犠牲にしても速度を改善する方法を試していきます。ここまで使用していたYolov4モデルはインプットの画像サイズが608×608のものでした。これをインプットサイズを416×416に縮小したモデルに切替えます。解像度が低くなり画像の詳細な特徴はとらえられなくなりますが、計算量を削減できるため速度アップが見込めます。では解像度416×416版のYolov4をPyTorch、onnxruntime、OpenVINOでそれぞれ速度計測してみます。予想通り推論精度が多少下がっていますが、推論速度を大幅に改善することができました。一番早いものでPyTorchによる0.549秒まで推論速度が向上しています。インプットの解像度を落とすことで推論速度をかなり向上させることができましたが、目標速度の0.5秒まではもう一歩というところです。そこで次はOpenVINOのフレームワークを使用して、モデルをさらに最適化してみます。さらに高速化を目指す：INT8 量子化ここまでOpenVINOでの推論処理で使用していたIR形式のファイルはFP32という形式のもので、FPはFloating Pointの略称です。つまり、推論実行時にデータを浮動小数点で処理しています。OpenVINOにはDeep Learning Workbench[7]という機能があり、これを使うことによってFP32のモデルをIntegerに量子化することができます。推論時の演算をFP32ではなくINT8で行うことによって計算量を大幅に削減することができます。Deep Learningにおける演算ではFP32ほどの値の範囲は必要なく、INT8で表現できる範囲でも十分に計算が可能だそうです[8]。（出典：https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Introduction.html）FP32のモデルをINT8に変換して推論したところ、下記の結果となりました。FP32のモデルからあまり精度を落とすことなく、推論速度を向上させることに成功しました。1枚当たりの推論速度は0.349秒で、目標速度0.5秒をクリアしています。以上のように、画像処理の推論モデルを実際にソリューションで使える形に落とし込むにはなかなかの手間が必要です。今回は、様々なプラットフォームを試行錯誤し、推論精度を多少犠牲にして目標速度を達成することができました。なお、608×608でINT8化した場合を計測してみると推論時間は1秒前後となります。今回試したYolov4の場合は速度向上効果としては量子化よりも入力データサイズの縮小の方が効果が大きい結果となりました。おわりにさて参考までに、今回はIntelの「Xeon(R) Silver 4116 CPU @ 2.10GHz」x8コアというCPU上で推論速度の最適化を行いましたが、では、CPUのコア数やスペックをあげるとどうなるのでしょうか？OpenVINOによるINT8のモデルをAWSのc5.4xlargeインスタンスで試してみました。スペックは「Xeon(R) Platinum 8275CL CPU @ 3.00GHz」が16コアでメモリは32GBです。．．．！！！やはり高スペックなマシンを使用できればCPUでもかなりの推論速度を出せるようです。ちなみに、最初にGPUで計測した推論時間は0.032秒でした。ただし、このc5.4xlargeインスタンスはCPUではありますがGPUでの計測時に使用したg4dn.2xlargeよりも多少安い程度で、GPUインスタンスに迫る価格になっています。今回は例として架空のシステムを想定して目標速度に対するチューニングを行いました。実際のシステム開発においては、最終的にはサービスとして提供できる価格や推論精度、推論速度などのバランスを考慮してマシンスやモデルを選定することになるでしょう。以上、今回の記事では機械学習関連のモデルは開発が完了したものをそのまま実際のシステムに実戦投入できるわけではなく、処理性能やマシンスペック、サービス価格など様々な面でチューニングが必要になるというシステム開発の裏側をご紹介しました。参考文献・URL[1]https://pjreddie.com/darknet/yolo/[2] Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv:2004.10934. 2020.4[3] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV), pages 740–755, 2014. 5[4]AlexeyAB/darknet[5]Tianxiaomo/pytorch-YOLOv4[6]TNTWEN/OpenVINO-YOLOV4[7]https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Introduction.html[8]https://www.xlsoft.com/jp/products/intel/openvino/document/enhanced-low-precision-pipeline.html"
https://www.ariseanalytics.com/activities/report/20210528/,ARISE Kaggle部活動記 #2,OTHERS2021.05.28TweetARISE Kaggle部活動記 #2Kaggle部ARISE analyticsの越智です。ARISE Kaggle部の活動記、連載第2回目です！私たちは世界的に知名度の高いKaggleを中心に、国内外の分析コンペティションへ参加し、上位ランクインを目指して頑張っています。Kaggle部は2021年5月現在、参加人数は約38名で、業務に支障のない範囲で楽しく活動中です。部員はARISEのプロパー社員だけでなく、ARISEに所属している様々な勤務形態のメンバーで活動しています。ちなみに私もKDDIからの出向者ですが、所属関係なく仲良く切磋琢磨しています。今まで参加したコンペは下記の通りです。M5 Forecasting – Accuracy @ Kaggle (398/5598銅ランク)対戦ゲームデータ分析甲子園　＠space（50/567銀ランク、72/567銅ランク）今回は2チームで参加しそれぞれ銀・銅ランクになった対戦ゲームデータ分析甲子園(https://prob.space/competitions/game_winner)で取り組んだ分析内容と工夫した点についてご紹介します！対戦ゲームデータ分析甲子園　参戦(2020年9月～)今期はどのコンペに参加するか検討する中で、いくつかの候補のうちメンバーが最も興味を持ったものがProbspaceの「対戦ゲームデータ分析甲子園コンペ」でした。有名オンラインゲーム「スプラトゥーン2」の勝敗予測をするという内容です。前回同様メダル受賞を目標に3人ずつ2チームで参加しました。そして結果は、567チーム中ARISE glassチームが50位で銀ランク、ARISE yamaチームが72位で銅ランクでした。どちらもメダル受賞達成という素晴らしい結果でした！以下では、コンペの詳細と両チームの解法について説明していきます。本コンペについて概要今回は有名オンラインゲーム「スプラトゥーン2」の対戦の勝敗を予測するコンペでした。このゲームの経験者は両チームとも1名ずつでしたので、チーム内でのドメイン知識共有も鍵となりました。「スプラトゥーン2」についてオンライン上で8人で行うゲームで、プレイヤーは4対4でチームに分かれ5種類のルールの元で競うゲームです。データ構造バトルごとのユニークなテーブルデータで、内容はステージやプレイヤー情報となっています。2チームはA、Bという名前、各チームの4人のプレイヤーはそれぞれA1～A4、B1~B4という名前が振られています。このデータを用いてAチームの勝敗を予測します。勝敗予測モデル受賞した2チームの各モデルについて説明します。ARISE glassチームモデル(50/567銀ランク)ベースモデル今回は使い慣れていることもありLightGBMを用いたモデルを採用しました。決定木と勾配ブースティングを使った手法です。外部データ結合Probspaceには参加者がトピック投稿する場がありそこで許可のとれた外部データを使用することができます。今回利用できるのは3種類あり、ステージの面積のデータ(csv形式)プレイヤーの装備するブキに紐づくサブブキ(補助ブキ)・スペシャルウェポン(ゲージを貯めて使う特殊ブキ)のデータ(csv形式)ブキごとの性能が掲載されているwikiページのデータ(形式無し)でした。①、②は既に整形されているのでそのまま学習データに結合して使用できました。ですが③はWiki作成者が独自でパラメータを定義し測定したものでブキの種類により数値が複数あったりパラメータの数が異なっていたりと整形に時間を要しました。そのためwebスクレイピングによるデータの取得、Excelでの細かい整形をチームで分担して行いました。イメージのわかないパラメータは実際にゲームでそのブキを使ってみて採用する数値を決めるなど工夫しました。特徴量エンジニアリング仮説を元に特徴量を追加、修正していきました。プレイヤーのブキ×バトルルールの勝率を追加したことが最も効果的でした。その他：チューニングOptuna（オープンソースのパラメータ最適化フレームワーク）を用いてLightGBMのハイパーパラメータをチューニングすると決定木の分岐の個数3、末端ノードの最小データ数7が最適となりました。シンプルな決定木ということで精度が上がるのか半信半疑でしたがチューニングをしたモデルが最も良いスコアとなりました。ARISE yamaチームモデル(72/567銅ランク)ベースモデル本コンペのデータはカテゴリカル変数を多く含むためカテゴリカル変数の扱いに長けているCatboostを用いたモデルを採用しました。LightGBM同様、決定木と勾配ブースティングを使った手法です。外部データ結合ARISE glassチームモデル同様3つの外部データを結合しました。特徴量エンジニアリングARISE glassチームモデル同様に特徴追加、修正をしました。その他：条件ごとのモデル作成モデル予測結果から、バトルルール、プレイヤーランク帯域ごとに正答率に偏りがあることが判明しました。そのため比較的正答率の高い条件と低い条件を分け、それぞれの条件に特化したモデルを構築しました。考察今回のデータは情報が少なく1位入賞のモデルでもaccuracyが0.58という非常に推定しづらいデータでした。ARISE glassチームモデルで行ったチューニング結果を反映することでスコアが伸びたように、シンプルなモデルにすることで過学習を防ぐことが重要だったのだと考えます。またバトルルールによって有利な装備があること、熟練者と初心者で装備の傾向が異なることに目をつけて、ARISE yamaチームモデルのように条件ごとでモデルを使い分ける手法も効果的でした。他にも興味深かった解法の一つが、上位入賞者が行っていたA1プレイヤーの推定です。本コンペのデータの提供者はA1プレイヤーとしてデータに載っていることから、同一A1プレイヤーが何人かいるはずだとプレイ日時やレベルから推定していました。データ収集方法という観点でデータの特性を発見するのは盲点でした。今回の取り組みで工夫したことチームメンバーで協力して効率的にモデル精度を高めていくために、チーム運営として工夫した点を紹介します。頻繁な定例の実施チーム定例を週2で行いました。コンペの取り組みは日々の業務の合間に行うためどうしても優先度が低くなってしまいます。ですが頻繁に定例を行ったので、効果的なもしくは効果のない特徴量や手法についての情報を高頻度で共有でき、個人のトライ&エラーを素早く回すことができました。また、モチベーションの維持・向上に繋がりました。明確な役割分担による効率化本コンペに参加したのが残り1ヵ月のタイミングだったので、チームメンバーで協力して効率よく進めていくことが重要でした。提供データの構造理解、外部データの収集・整形、トピック投稿情報収集などモデルに関係なく共通する作業は役割分担をして実施していました。チーム内勉強会による知識の平準化コンペ中に活用したい各種ツールや手法について、習熟しているメンバーの知見を未習熟のメンバーに共有する勉強会を設定しました。これにより、メンバーの知識レベルを底上げすることができました。おわりに今回参加した両チームともにメダル受賞という目標を達成することができました。今回はチームごとに分析の進め方を自由に設定して行いましたが、これまでの経験を元にチームで分析に取り組む上でより効率的に進められる体制の整備もしていきたいと思っています。また今後Kaggle部として強力な分析基盤を導入予定ですので高スペック処理を必要とする分析コンペにも参加できるようになります。分析はチームワークが大事です。役割を決めお互いの強みを持ち合う事で、一人で行うより何倍も効率的に分析課題に取り組めます。我々と一緒にKaggle部を盛り上げてくださる方はこちら(https://www.ariseanalytics.com/recruit/)のページからご連絡お待ちしています！
https://www.ariseanalytics.com/activities/report/20210611-2/,Snowflakeが今熱いらしいので試しに使ってみた,"ARISE Tech Blog2021.06.11TweetSnowflakeが今熱いらしいので試しに使ってみたこんにちは。Customer Analytics Division所属データエンジニアの城戸です。最近Snowflakeというサービスが業界で話題になっています。社内の分析環境の選定にあたり、使い勝手やコストの観点からSnowflakeの導入検討を行ったので、その結果の一部をご紹介します。SnowflakeとはSnowflakeは、完全SaaS型のクラウドデータウェアハウスサービスです。SaaS型なので、ハードウェア、ソフトウェアの管理をする必要なく、すぐに利用できるというメリットがあります。また2020年9月に米ソフトウェア業界史上最大規模でIPOするなど勢いがあり、楽天、アドビ、アシックスなど世界中の顧客にサービスを提供しています。Beyond “Modern” Data Architectureただ、SaaSであるがゆえに、データもSnowflakeが管理するクラウドプロバイダ（AWS/GCP/Azure）アカウント配下に置かれてしまうので、特に機密度の高い顧客データなどを扱う際は会社のセキュリティーポリシーに違反しないかなど十分に考慮する必要があります。さっそく使ってみるSnowflakeはWebブラウザで操作できるUIが提供されており、アドホックな分析目的ではそれを使うのが手っ取り早いです。ログインURLにアクセスし、アカウントにログインします。検証用ユーザー作成Snowflakeでは、ユーザーを作成して権限管理することができます。せっかくなので、検証用にユーザーを作成して、検証用リソースを利用する権限を付与します。# 検証用のデータベースとウェアハウスを作成use role sysadmin;create database kido_db;create or replace WAREHOUSE kido_wh WITH WAREHOUSE_SIZE = 'XSMALL';# 検証用ユーザーに割当てるロールを作成use role securityadmin;create or replace role kido_role;grant role kido_role to role sysadmin;# 作成したロールに検証用リソースの所有権を移すuse role sysadmin;grant ownership on database kido_db to role kido_role;grant ownership on schema kido_db.public to role kido_role;grant ownership on warehouse kido_wh to role kido_role;# 検証用ユーザーを作成use role useradmin;create or replace user kido password='password' default_role='kido_role' must_change_password=TRUE;# 検証用ユーザーにロールを割り当てるuse role securityadmin;grant role kido_role to user kido;Snowflakeの権限モデルは、私が業務で慣れていたAWSのIAMと異なる部分が多かったので最初は戸惑いましたが、こちらの公式ドキュメントが分かりやすくて良かったです。https://docs.snowflake.com/en/user-guide/security-access-control-overview.htmlテーブルの作成とデータのロード検証用ユーザーとリソースが作成できたので、検証用ユーザーでログインし直してまずは実験用のデータをSnowflakeに取り込みます。今回はデータはSnowflakeがサンプルで提供しているものを利用しました。https://s3.amazonaws.com/snowflake-workshop-lab/Snowflake_free_trial_LabGuide.pdfuse role kido_role;use database kido_db;use schema public;use warehouse kido_wh;# テーブルを作成create or replace table trips(tripduration integer,starttime timestamp,stoptime timestamp,start_station_id integer,start_station_name string,start_station_latitude float,start_station_longitude float,end_station_id integer,end_station_name string,end_station_latitude float,end_station_longitude float,bikeid integer,membership_type string,usertype string,birth_year integer,gender integer);# S3に置いてあるデータを指定create or replace stage citibike_trips URL = 's3://snowflake-workshop-lab/citibike-trips';# データのファイルフォーマットを指定create or replace file FORMAT CSV TYPE = 'CSV' COMPRESSION = 'AUTO' FIELD_DELIMITER = ',' RECORD_DELIMITER = '\n' SKIP_HEADER = 0 FIELD_OPTIONALLY_ENCLOSED_BY = '\042' TRIM_SPACE = FALSE ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE ESCAPE = 'NONE' ESCAPE_UNENCLOSED_FIELD = '\134' DATE_FORMAT = 'AUTO' TIMESTAMP_FORMAT = 'AUTO' NULL_IF = ('');# テーブルにデータをロード（実行時間は数秒かかります）copy into trips from @citibike_tripsfile_format=CSV;これで準備完了です。クエリの実行作成したテーブルに対して、いろいろクエリを投げてみます。select count(*) from trips;結果は61,468,359行でしたが、処理時間は108msと一瞬でした。これは、Snowflakeが代表的な統計値はメタデータとして保持しているためです。select count(*) from trips group by usertype;この処理は682msかかりましたが、2回目以降は70~80msでした。これは、Snowflakeが実行結果をキャッシュしているためです。基本性能検証Snowflakeは、バーチャルウェアハウスという計算用リソースの上で処理が行われます。ワークロードに合わせて柔軟にウェアハウスサイズを変更できるというメリットがあるのですが、1つサイズを上げるごとに単価は2倍になります。そこでサイズの変化に対して性能はどれほど向上するのか、実験してみました。実験対象クエリは、1~80GBのテーブル全部で10個をユニークなキーでjoinしていく処理です。グラフから、1つサイズが上がるごとに、2倍とはいかないまでも性能が上昇し、ウェアハウスの最大サイズの4xlまで性能が伸び続けているのが分かります。最大サイズで49秒程度の処理でこの結果なので、もっと重い処理の場合はより性能向上が顕著になるのではないか？と思っています。Data Sharing機能Snowflakeの主要機能の一つに、Data Sharingがあります。これは、Snowflakeアカウント間でデータを共有する機能です。実際にデータはコピーされない、即時反映、読取専用であるなどの特徴があります。また、共有方法も簡単で、共有する側とされる側で数行のコードを書くだけで実現できます。例えば、以下の図のようにアカウントab01234からアカウントxy56789にデータを共有したい場合のコードを示します。Provider Account側create share my_share;// 共有するオブジェクトを指定grant usage on database kido_db to share my_share;grant usage on schema public to share my_share;grant select on table trips to share my_share;// 誰に共有するか指定alter share my_share add accounts=xy56789;Consumer Account側create database shared from share ab01234.my_share;select * from shared.public.trips limit 10;まとめSnowflakeの調査・試用結果の一部を共有しました。全体的にみて、完全SaaS型サービスであることからくる高いユーザビリティと機能面のメリットが大きいと思いました。一方で、競合となるDWH（弊社であれば、AWS環境と親和性の高いRedshift）との比較（特に性能/コスト面）は一度検証しておく必要があると思っています。また、Snowlfakeは今勢いがある企業なため、今後の成長にも期待したいです。2021年4月現在、Snowflakeは30日限定のフリーアカウントを提供しています。お試し的に使ってみてはいかがでしょうか。"
https://www.ariseanalytics.com/activities/report/20210625/,Flutter#2　〜Flutterの状態管理パターン （Riverpod）〜,"ARISE Tech Blog2021.06.25TweetFlutter#2　〜Flutterの状態管理パターン （Riverpod）〜はじめまして、Advance Tech Divisionでモバイルエンジニアをしている中塚です。今回はFlutterシリーズの第２弾ということで、Flutterでの状態管理パターンについて書こうと思います。Flutter での状態管理は様々なパターンが存在しており、当社でもプロジェクトによって利用しているパターンが異なっております。その中で、今回は状態管理で最近よく使われているRiverpodについて記載していきたいと思います。第１弾の記事については、こちらを参照ください。（Flutter#1 〜ARISE analyticsのプロジェクトでFlutterを採用してみました〜 | 株式会社ARISE analytics（アライズ アナリティクス）)状態管理パターンとは？以下の動作を管理することを状態管理パターンといいます。別々の画面で状態（値）を共有することができる状態の変更に応じて、UIが更新されるFlutterでの状態管理パターン現在Flutterでは、全てではありませんが以下のような状態管理パターンが使われております。今回はその中でも比較的新しくて、最近よく使われている StateNotifier + freezed + Riverpod + FlutterHooks のパターンについて紹介していきます。StatefulWidgetInheritedWidgetBLoC + ProviderChangeNotifier + ProviderStateNotifier + freezed + ProviderStateNotifier + freezed + Riverpod + Flutter Hooks （今回の紹介内容）StateNotifierChangeNotifierでは、状態の変更を通知するために、状態を変更するたびに notifylisteners関数を呼ぶ必要があります。この煩わしさを解消してくれたのが、StateNotifierになります。StateNotifierは、一つの状態（state）しか持つことができません。そのため、複数の状態を管理したい場合は、オブジェクトを作成して管理することになります。オブジェクトに作成については、後述するfreezedを参照ください。下記は、カウンターの例になります。stateとしてint型を宣言し、0で初期化します。increment関数が呼ばれるたびにstateに1が足され、自動的にリスナーに変更が通知されます。※使用している言語はdartclass CounterStateNotifier extends StateNotifier<int> {CounterStateNotifier(): super(0);void increment() {state++;}}freezedStateNotifierで利用するState用のクラスをimmutableにすると、StateNotifier側の記述が冗長になってしまいます。これを解決するのがfreezedパッケージになります。（freezed – Dart API docs）State用に作成したimmutableなクラスの例になります。@immutableclass CounterState {CounterState({this.count = 0,this.isEnabled = true,});final int count;final bool isEnabled;}StateNotifierでStateを変更するたびにオブジェクトを再作成する必要があり、countのみを更新したいのにisEnableも定義する必要があり、記述が冗長になってしまいます。class CounterStateNotifier extends StateNotifier<CounterState> {CounterStateNotifier(): super(CounterStateNotifier());void increment() {state = CounterState(count: state.count + 1,isEnabled: state.isEnabled,);}void disableCounter() {state = CounterState(count: state.count,isEnabled: false,);}}freezedは、後述するRiverpodの作者が作成したパッケージになります。freezedを利用することで様々な関数を自動生成してくれます。これらを利用することで、冗長なコードが簡潔になります。copyWithJsonのパース==toString()遅延初期化などfreezedを利用したState用のクラス例になります。@freezedclass CounterState with $_CounterState {factory CounterState({int? count,bool? isEnabled,}) = _CounterState;}ターミナルで以下のコマンドを実行すると、@freezedが付ているクラスを自動で検出し、コードを自動で生成してくれます。flutter pub run build_runner build --delete--conflicting-outputsfreezedを用いた場合のStateNotifierは、先ほどのコードに比べると記述が簡潔になっていることが分かると思います。class CounterStateNotifier extends StateNotifier<CounterState> {CounterStateNotifier(): super(CounterState(count: 0, isEnabled: true));void increment() {state = state.copyWith(count: state.count + 1);}void disableCounter() {state = state.copyWith(isEnabled: false);}}RiverpodRiverpodとは、状態管理パッケージで、Providerと同じ開発者が作成したものになります。Providerの欠点を補った改良版のProviderです。Riverpodという名前もProviderのアナグラムになっております。（Provider, but different | Riverpod）Riverpodは、3種類あります。Riverpodの開発者は、hooks_riverpodの利用を推奨しております。hooks_riverpodで利用するFlutter Hooksについては、後述で説明します。Riverpodのメリット・デメリットとしては以下があげられます。Flutter HooksReact HooksのFlutter版になります。こちらもRiverpodの開発者が作成しており、Riverpodとの併用を推奨しております。Hook Widgetを継承することで、便利なuseXXX関数を利用することができます。（flutter_hooks – Dart API docs）Flutter HooksとRiverpodを組み合わせることで、state取得時にselectを利用することができます。Flutter Hooks を使わない場合、ConsumerWidgetを継承するか、Consumerを利用することになります。final provider = StateNotifierProvider<CounterStateNotifier, CounterState>((ref) => CounterStateNotifier());class CounterWidget extends ConsumerWiddget {@overrideWidget build(BuildContext, context, ScopedReader watch) {final CounterStateNotifier notifier = watch(provider.notifier);final CounterState state = watch(provider);}}FlutterHookを利用する場合、HookWidgetを継承してuseProviderを利用することになります。状態が変更された際に自動で再ビルドされます。final provider = StateNotifierProvider<CounterStateNotifier, CounterState>((ref) => CounterStateNotifier());class CounterWidget extends HookWidget {@overrideWidget build(BuildContext, context) {final CounterStateNotifier notifier = useProvider(provider.notifier);final CounterState state = useProvider(provider);}}おわりに今回はFlutterの状態管理パターンであるStateNotifier + freezed + Riverpod + Flutter Hooks について紹介しました。今回の例は、一つのパターンに過ぎず、今後多くのパターンが登場されると思います。その中でお気に入りのパターンを見つけていただければと思います。"
https://www.ariseanalytics.com/activities/report/20210708-2/,全社でコーディング力向上を目指せ！～社内競技プログラミング大会の参加者にインタビュー～,"ARISE Tech Blog2021.07.08Tweet全社でコーディング力向上を目指せ！～社内競技プログラミング大会の参加者にインタビュー～皆さまこんにちは。Marketing Solution Division, Partnering Solution Unitに所属している伊藤と申します。今回は私も運営メンバの一員である、社内競技プログラミング大会について紹介します。ARISE analyticsでは今年1月から社内で競技プログラミング大会が開催されています。ARISE analyticsは数多くのデータサイエンティストが所属する会社であり、分析業務が活発な会社です。また、分析業務のほかにも、その知見を活かしたソリューション開発が盛んにおこなわれております。こうした分析業務や開発業務で役立つ経験が得られる他、普段お互いあまり関りがない分析業務が中心の人や開発業務が中心の人が所属組織をまたいで交流をすることで、参加者が楽しみながらスキルアップができるような環境づくりを目標に競技プログラミング大会を運営しております。この取り組みはPartnering Solution UnitというKDDIパートナー企業へのソリューション提供を中心に行うグループのリーダーである鴨居が、チーム全体のコーディング力を向上させたいという思いを元に始めたものでしたが、参加者がどんどん増えていき全社的に大会を実施するようになりました。その結果、今では研修中の新卒3期生も含めた幅広いメンバが参加するようになりました。今回は参加メンバのうち4人になぜ社内競技プログラミングに参加しているのか、参加して得られたことはあるかなどを聞くインタビューを実施したので、このインタビューを通してARISEanalyticsのメンバがどのように競技プログラミングに取り組んでいるのかをお伝えできればと思います。社内競技プログラミング大会の様子。AtCoder Problemsを利用し大会の問題を選出。 梅津さんが解法を説明中。競技プログラミングとは競技プログラミングとは、与えられた問題に対して適切な答えを出力するプログラムをいち早く完成させることが目標となっている競技です。出題される問題はアルゴリズムの知識が問われるものが多く、例えば、目的位置、さらに移動経路ごとのコストが入力として与えられ、初期位置から目的位置に移動するために最小のコストを出力するプログラムを作成せよといったものがあります。このような問題は時間計算量を意識し適切なアルゴリズムを選択することが重要であるため、アルゴリズム知識だけに論点が集まりがちですが、それ以外にもプログラムを素早く実装し、バグを取り除く力が求められます。ARISE analyticsの業務では直接競技プログラミングで使うようなアルゴリズムの知識を求められることは少ないですが、素早く実装する力や、バグを取り除く力は業務にも繋がるスキルではないかと私は考えています。参加者インタビューさて、ここからは社内競技プログラミング大会の参加者である、Marketing Solution Divisionの梅津、現在研修中の新卒3期生の岩野と江澤、Marketing Solution Divisionの鴨居の4人にインタビューした内容を紹介していきます。インタビューを通してARISE analytics内で競技プログラミングがどのように捉えられているかをお伝えできればと思います。プロフィール梅津2020年にARISE analyticsに入社。Marketing Solution Divisionに所属し、ECサイトへのレコメンド提供と精度評価業務に携わっている。ポケモンユナイトに興味津々。岩野2021年に新卒3期生としてARISE analyticsに入社。学生時代はRNAを用いた医薬品の開発を情報学的な立場で研究。競技プログラミングは学部卒業後の空き時間に始めた。ロードバイクが趣味。江澤2021年に新卒3期生としてARISE analyticsに入社。学生時代はブロックチェーンを用いた認証・認可システムについて研究。競技プログラミングは研究室の友人と一緒に始めた。美味しいものを食べるのが趣味。鴨居2018年にARISE analyticsに参画。Marketing Solution Division内のPartnering Solution UnitのリーダーとしてKDDIパートナー会社との協業案件に関わる分析や開発業務のリーダーを担当。機械学習を活用したソリューションのデータETLやバックエンド開発などを2年ほど担当したのちに、現在は位置情報データを活用マーケティング分析や海外クライアント向けのコンサルティングなどに従事。登山・フットサルが趣味。インタビュアー：伊藤2020年にARISE analyticsに入社。Marketing Solution Divisionに所属し、位置情報を活用したマーケティング分析やKDDI提携企業のデータ活用にむけた分析などに携わる。最近は対戦格闘ゲームに入れ込んでいる。Q1. 競技プログラミングに興味を持ったきっかけは梅津: 普段はSQL集計がメインの業務でありPythonコーディングするタイミングが無いので、コーディング習慣をつけたいという理由で競技プログラミングを始めました。―――SQLはデータ分析の基本ですよね。ちなみに梅津さんの所属するチームではどんな分析業務を行っているんですか？梅津: 私の所属するチームでは、グループ会社が運営するECサイトのレコメンドを担当しています。ECサイトによくある ”この商品を買った人はこの商品も買っています” みたいなやつですね。チームではレコメンドする商品を選定するシステムを運用しており、私は主にレコメンドに出した商品が実際にクリックされているかどうかを調査する業務を行っています。江澤: 私はもともと大学院時代から競技プログラミングをやっていました。コンペティションで競うのが好きで、やっていくうちにアルゴリズムが身についていくのが楽しかったです。―――競争心に駆り立てられているうちにアルゴリズムや実装力が身についているというのが競技プログラミングの良いところですよね。競技プログラミングを通じて初めて学んだアルゴリズムで興味深いものなどありますか？江澤: 累積和ですかね。シンプルに頭いいなと感じました。競技プログラミングでは大量のデータが与えられることが多く、様々なアルゴリズムを駆使して計算量を減らす工夫をしなくてはいけないのですが、累積和はそのアルゴリズムの一つです。この累積和のスマートさに魅力を感じて、少しずつ競技プログラミングにハマっていきました。ARISE社内の勉強会資料。累積和やいもす法について。岩野: 以前はプログラミングスキルを評価できるサービスで自分のスキルを測っていましたが、バイトの同期が競技プログラミングをやっていたので興味を持って始めました。数学力には自信があったのもあり、最初のうちはスコアがどんどん伸びていき、成長が見れて面白かったです。伸び悩んだ時期もあり少し離れていましたが、社内競技プログラミング大会の取り組みを通じ、同期と一緒にまた始めました。―――身近に競争相手がいるとモチベーションが湧きますよね（笑）江澤さんと岩野さんは2021年4月に新卒入社された新卒3期生同士ですが、互いにライバルですか？江澤: 岩野さんはライバルではなく先生みたいな感じですね（笑）私は新しい物事を理解するのが遅めなので、わからないことがあるたびに岩野さんに手助けしてもらっています。そのおかげで少しずつコーディング力が上達しているのでとても感謝しています。岩野: 私もライバルとは考えていなくて、友達みたいな感じですかね。何かを継続して続ける時に同期や仲間がいるのは良い事であると感じています。最近は『競プロ典型90問』を研修開始前に毎日取り組んでいるのですが、この取り組みを続けられているのは江澤さんのような同期や仲間がいるおかげだと思います！※競プロ典型90問（競プロ典型 90 問 – AtCoder）―――同期で一緒に切磋琢磨できる相手がいるのはいいですね！鴨居さんはいかがでしょうか？鴨居: 私は就職活動時のホワイトボードコーディング面接対策でアルゴリズムの勉強をしており、その一環で競技プログラミング初めて触れました。ARISEに参画後、一人でバックエンド開発をまるっと任されることがあり、自分の実装スピードがプロジェクト推進のボトルネックなっていることに気づいたことがきっかけで休日は競技プログラミングの大会に出場するようになりました。―――いきなり一人でバックエンド開発を任されちゃうって裁量が大きいですね。うまく開発できたのでしょうか？鴨居: これまでに書いたことがないScalaという言語での開発であり納期も短いという正直キツい状況でしたが、上司のサポートをもらいつつ頑張ってキャッチアップし、なんとか納品することができました。江澤: 鴨居さんでもキツいと感じる業務があるんですね、私もそういった忙しい状況でさえスマートに対応できる人間になりたいと思います。岩野: Scalaを書いたことは無いのですが、競技プログラミングで培った実装力とかって言語を問わず活用できるのではないかと思っています。もし短い納期で新たな言語を使用することが必要になったとしても、落ち着いてキャッチアップしたいなと思います。鴨居: お二人とも頼もしいですね。来るべき状況へ向けて準備くださっているみたいで非常に頼りになります（笑）―――梅津さんも2020年に入社されたばかりですし、岩野さんや江澤さんは2021年に入社されてからまだ研修を受けている最中ですよね。これから先にARISEで取り組みたい領域などはありますか？新卒採用者向けトレーニング内容梅津: 現在担当しているレコメンドがおもしろいなと感じているので当分は継続していきたいですね。あと個人的に興味のある領域は生体情報ですね。扱えるようになるのは先の未来だと思いますが（笑）岩野: ARISEはKDDIのグループ会社ですし、スマートフォンや基地局などの位置情報データなど、他社では得ることが出来ないデータの分析がしたいですね。あとはヘルスケア領域などにも興味があります。江澤: 私はまだ明確に何かあるわけではないですが、データサイエンスの活用によって何か深い洞察を得ることが出来たらいいなと思っています。それこそ、簡単な集計では発見することができないような真実を見つけ出すみたいな……Q2. 競技プログラミングをしていて業務で役にたったこと、または自分の成長が感じられたことは？梅津: 業務でコーディングしないのでアルゴリズム知識はあまり役立たないです（笑）SQLの計算量は意識するようになったのでそこは成長した部分だと思います。今後は開発業務に携わるので、そのタイミングでアルゴリズム知識を役立てたいです。もちろん、ロジックを考える過程とバグを取り除く過程は現在でも役立っていると思います。ARISE社内の勉強会資料。時間計算量の概要について。―――競技プログラミングでは計算量の意識だけでなく、テストケースを作成してバグを取り除く意識も大切ですよね。梅津: そうですね。AtCoderでは与えられたテストをクリアするだけじゃ仕様を満たさない場合が多いので、必然的に自分でテストケースを考えることが多くなりますね。社内勉強会でテスト駆動開発を学んだ時にまんまこれじゃん！と思いました。―――江澤さんや岩野さんはいかがでしょうか？業務でなくとも、これまでに役立った経験などはありますか？江澤: まだ研修で分析アルゴリズムを習っている段階だからなんとも言えない（笑）楽しくプログラミングできているので競技プログラミングの取り組みがあって良かったと思っています。岩野:大学では高いスペックのコンピュータを使っていたので特に意識していなかったのですが、研修では計算機の性能を十分に引き出すために工夫してプログラムを書く必要があり、競技プログラミングで培った計算量の見積もりやプロファイリングなどが生きている気がします。また、この取り組みのおかげで楽しくプログラミング出来ていること自体も良かったと感じています。―――鴨居さんはいかがでしょうか？鴨居: 要件や仕様をみてからコードを書く手が動き始めるまでのスピードが格段に上がりました。実務では自分がコードを書くことはほとんど無くなりましたが、たまに書くときに一瞬でサンプルコードを書けるようになっていたり、パフォーマンス上起きうるリスクみたいなものが事前に検知できるようになっていたりなど、開発の推進に活用出来ていると感じています。―――開発や技術に詳しい方が率いているチームって魅力的ですね！鴨居: クライアントへ提案を行う際、細かい技術的な知識が意外と役立つんですよね。クライアントからの期待に正しく応えるには必要になるというか。ですので、自分自身でも技術的なスキルアップは怠らないように意識しています。私自身はあまり開発メインな働き方ではなくなってしまったのですが、チームメンバの分析や開発とか技術的な取り組みに対して、メンバ視点で相談に乗ってあげられるようなリーダーを目指しています。インタビューに答える鴨居さんQ3. ARISEでは勉強会やクラブ活動が盛んですよね。他にも何かの活動に参加されたりしていますか？梅津: 伊藤君を含め同期とチームを組んでkaggleコンペに参加しています。機械学習モデルの構築や、データの基礎分析などARISEでの業務と共通する部分が多いですし、自分の実業務以外の分野に気軽に関わることが出来るのがいいですね。※Kaggle部の活動ARISE Kaggle部活動記 | 株式会社ARISE analytics（アライズ アナリティクス）江澤: 私もKaggle部の活動に参加しています。週に1回ほどのミーティングでKaggle上のディスカッションを読み込んだり、岩野君と一緒に好きなことをやっています。岩野: 私も同じくKaggle部に参加しています。他の勉強会なども興味はあるのですが、まだ研修中ということもあり参加出来ていません。業務と全く関係ありませんがモンハン部というのにも所属しているので、夜な夜な狩りを続けています（笑）鴨居: 勉強会ではないのですが、私はアウトドア部やボードゲーム部などにも参加しています。クラブ活動に参加することはメンバ外の知り合いを増やすというだけでなく、メンバ間の仲を深め仕事の成果も上げることにも繋がると感じていますので、私は社内交流を大切にしています。おわりにARISE analyticsで競技プログラミングの取り組みを始めてからもうすぐ半年となりますが今回のインタビューでは、担当業務で出来ない経験をしたり、業務貢献に繋がるスキルを獲得するなど、参加者それぞれでメリットを感じていることがわかりました。ARISE analyticsでは競技プログラミングの他にも有志による社内勉強会やKaggleなどのデータ分析コンペティションも盛んに行われています。もし機会があれば他の活動についても紹介出来れば幸いです。"
https://www.ariseanalytics.com/activities/report/20210813/,組織拡大に伴う、分析プロジェクトの品質平準化に向けた取り組み,"ARISE Tech Blog2021.08.13Tweet組織拡大に伴う、分析プロジェクトの品質平準化に向けた取り組み初めまして、Digital Consulting Sector（DCS）, Marketing Solution Division所属の後藤です。DCSでは、これまでに多くの分析プロジェクトを推進してまいりました。今回は、分析プロジェクトの品質平準化に向けた、過去知見の体系化・方法論構築の取り組みについてご紹介します。背景と課題組織の規模が拡大し多様なテーマの分析案件を扱うようになりましたが、その中には比較的スムーズに成果につながった案件もあれば、目指していた成果に到達せずに案件終了したものも多くあります。そういった中で、案件担当者によってプロジェクトの進め方が異なり、品質にばらつきがあることが課題視されるようになりました。案件担当者や分析者に依存することなく、高い品質でスピーディにクライアントにアナリティクスの価値をご提供するために、DCSでは、Analytics Delivery Method(ADM)という社内プラットフォームを整備し、過去に培った分析プロジェクト知見の体系化を進めています。Analytics Delivery Method（ADM）分析プロジェクトは、大きくアセスメントフェーズPoCフェーズ運用フェーズの大きく3つのフェーズに分かれます。それぞれのフェーズで実施すべき内容や必要なスキルも異なってきます。人員の配置・タスクの優先度を、フェーズによってダイナミックに変えていくことが重要です。さらに、分析基盤・・・アセスメント・PoC・運用フェーズそれぞれのタスクの性質に合わせた適切な環境の整備データガバナンス・・・分析業務や分析関連サービスの品質向上に向けたガバナンス業務・体制といった各フェーズに共通する要素も重要です。これらが整備されることで、分析プロジェクトのよりスピーディかつ確実なデリバリーが実現できます。ADMでは、これらの3フェーズ+共通要素をさらに細かなステップに分け、それぞれのステップで、何を達成する必要があるかどのようなタスクを実行すべきかどのような点に注意すべきか過去にどんな事例があり、どのような学びがあったかなどをコンテンツとして体系化しています。ハニカムのうちの一つ一つのセルがプロジェクトのステップを表しており、それぞれのステップはさらに細かなトピックに分かれ、体系立ってまとめられています。例えば、「分析設計」のステップは、以下のような3トピックに分かれています。これらのコンテンツの拡充のため、定期的に社内有識者を集めた討議会を実施し、コンテンツのアップデートを行っています。また、単なる体系化で終わらせないために、各トピックに関連した社内・社外事例の投稿も実施し、より実践的なコンテンツとしています。おわりに社内の分析プロジェクト方法論整備の取り組みについてご紹介しました。今後もお客様にスピーディかつ確実にアナリティクスの価値をご提供できるように、方法論のさらなる整備を進めていきたいと思います。"
https://www.ariseanalytics.com/activities/report/20211008/,ARISE analytics流SaaS開発・運用術 #1 TerraformとArgoCDで開発効率化,"ARISE Tech Blog2021.10.08TweetARISE analytics流SaaS開発・運用術 #1 TerraformとArgoCDで開発効率化こんにちは。Marketing Solution Division(MSD)でDevOpsエンジニアと活動している山中です。ARISE analyticsの提供するSaaSプロダクトを「1.新規顧客に迅速に提供(オンボーディング)する」「2.継続的なアップデートを提供(CD)する」ためにTerraformとArgoCDを活用した社内システム統合管理環境を構築しました。今回はSaaSプロダクトの開発・運用という観点から、統合管理環境の機能とメリットを解説します。Terraform, ArgoCDとは？まずは統合管理環境の要であるTerraformとArgoCDについて見ていきましょう。すでにご存知の方は「統合管理環境とは」のセクションまでスキップしてください。TerraformTerraformはInfrastructure as Code(IaC)を実現するツールです。 詳細については素晴らしい解説記事がありますのでここでは割愛します。ARISEのSaaS開発で導入する主なメリットは下記が挙げられます。1.プロダクトのインフラを迅速に構築できるTerraformはソースコードでインフラリソースを定義するため、別のテナントに対してもコードの使い回しが可能です。ARISE analyticsのSaaSプロダクトはAWS上で稼働しています。またテナント分離[^1]には、テナント毎に異なるAWSアカウントを利用するシングルテナントの形態を取っています。Terraformを用いることで新規顧客のオンボーディングの際には任意のAWSアカウントにリソースを自動構築することが出来ます。2.クラウドリソースを宣言的に管理できるTerraformではクラウドインフラの所望の状態をソースコードで記述(=宣言)します。ソースコードを確認することでクラウド上の現在のリソース全体像を把握することが出来ます。そのため別途ドキュメントなどでリソース管理などを行う必要が無くなり管理コストを削減できます。ArgoCDArgoCDはKubernetes(K8s)クラスタ上で動作する、GitOpsを実現するCDツールです。ArgoCDの詳細についても他の解説記事に譲るとして、開発・運用に導入するメリットを取り上げます。図1:ArgoCDの操作画面GitOpsとGUI操作による運用負荷の削減K8sクラスタ上のアプリケーションの開発・運用はK8sについてやマニフェスト、専用UI(kubectl等)の知識が必要となります。そのためK8sに馴染みのないアプリ開発者や運用者がDevOpsを実現する際の障害になり得ます。ArgoCDを用いることでマニフェストの変更やkubectlなどの操作を隠蔽し、GUIの操作でデプロイ・ロールバック・ヘルスチェックなどの操作を完結する事ができます(図1)。そのためK8sに習熟してないアプリ開発者や運用者だけでも円滑なDevOpsを実現できます。統合管理環境とは統合管理環境はTerraform・ArgoCDに加えてAWSリソースを組み合わせることで、それぞれを単体で利用する以上のメリットを実現します。統合管理環境の下記2つの機能について解説します。1.基盤構築機能2.アプリケーション管理・開発機能1.基盤構築機能図2:基盤構築機能構成基盤管理機能はTerraformを用いて、K8sクラスタ(EKS)を含めたAWSリソースを任意のAWSアカウントに構築します。下記が主な特徴です。Terraformソースコードと依存関係をDocker imageとしてパッケージングしているAWS Batchをトリガーとして、fargate(ECS)でコンテナを稼働してTerraformの実行するTerraformをローカル環境ではなくコンテナ化する[^2]メリットとして運用者がローカル実行環境を構築不要な点や、運用者間で環境差分が発生しない点が挙げられます。ローカル実行環境ではTerraformクライアントのバージョンや依存関係の差分が個々の運用者間で発生する可能性があります。またAWS Batchを用いることのメリットはオペレーションミス防止への貢献です。実行時にはAWSコンソールにログイン後パラメータをjobに追記してsubmitするだけで良いため、運用負荷は少なくて済みます。2.アプリケーション管理・開発機能図3:アプリケーション開発管理機能構成アプリケーション管理・開発機能はArgoCDを用いて基盤管理機能で構築されたK8sクラスタ上に継続的なアプリケーションデプロイ(CD)を行います。下記が主な特徴です。複数のK8sクラスタを一元管理するArgoCDが稼働するK8sクラスタ(ソースクラスタ)とアプリケーション用のK8sクラスタ(ターゲットクラスタ)を、Transit Gatewayを用いて1:多関係のプライベートネットワークで接続する各ターゲットクラスタのアプリケーションをソースクラスタ上のArgoCDで一元管理することでSaaS提供先テナントが増加しても管理コストの増加を抑えることが出来ます。仮にテナントが5社、各テナントにつき開発・検証・本番の3環境が必要な場合は3 * 5 = 15(個)の環境を個々に管理する必要があり、個別に操作する場合は管理コストが増大します。ArgoCDから各ターゲットクラスタを接続すれば全ての環境を一箇所で操作・管理することが可能になれば管理コストを抑えることが出来ます。(図4)図4:ArgoCDによる複数クラスタ・アプリ管理また各クラスタのVPCはTrasit Gatewayを用いて、ソースクラスタを中心としたスター型のネットワーク・トポロジーで接続されています(図5)。Trasit Gatewayにより、ソースクラスタとターゲットクラスタの通信はインターネットを経由しないプライベートネットワーク接続となるためセキュリティ上安全な通信を確保できます。もちろん個々のターゲットクラスタ同士のネットワークルーティングは行っていないため「テナントAのK8sクラスタ <-> テナントBのK8sクラスタでの直接的なプライベート通信」といったインシデントは発生しません。図5:クラスタ間のネットワーク構成おわりに今回はArgoCD・Terraformやそれらを活用した統合管理環境について機能・構成・メリットを「SaaSプロダクトの開発運用の効率化」という観点で解説しました。#2では実際に、「異なるAWSアカウント上に存在するソースクラスタとターゲットクラスタをTransit Gatewayでプライベートネットワーク接続する方法」を解説します。お楽しみに！またARISE analyticsではモダンなDevOps技術を用いてより良いプロダクト開発を実現する仲間を募集しています！ご興味のある方はこちらのページからご連絡をお待ちしています！脚注[^1]: テナントとは、SaaS を利用する顧客の契約の単位です。またテナント分離とはテナント毎にどのようにアプリやインフラを分割するかという考え方です。詳細はこちらのページを参照下さい。[^2]: Terraformの実行環境にはHashiCorpが提供するTerraform Cloudがありますが、今回の統合管理環境では「機能をAWS上で完結させる」という要件があったため本記事で紹介する構成としました。"
https://www.ariseanalytics.com/activities/report/20211022/,Infrastructure as Codeの冪等性とプロジェクトの関係性,"ARISE Tech Blog2021.10.22TweetInfrastructure as Codeの冪等性とプロジェクトの関係性初めまして、New Business Sector (NBS), Advanced Tech Division所属の坂本です。プロジェクトではインフラエンジニアを担当しております。現在、私が所属しているチームではヘルスケア領域におけるプロダクトを開発しており、その中で今回はバックエンド、特にインフラ領域に特化したIaCの管理手法をお話をさせていただければと思います。はじめに現在、AWSなどのクラウドサービスを使っているのであればTerraformなどの構成管理ツール≒IaCを導入するのがスタンダードになりつつあります。しかし、プロジェクトが始まると様々な要因によりコードと実環境の乖離が大きくなっていきます。それにより最終的にはコードが全く使えなくなったり、コードのためのメンテナンスが発生してしまい何のためにIaC化しているか不明になってしまうこともあるかと思います。では、何故IaCはもてはやされるのにも関わらず実際のプロジェクトでは維持に苦労するのか考えてみたいと思います。エンジニアとしてIaCに求めることまず初めに、何のためにIaCを導入するのかを個人的な視点でまとめると下記の観点が大きいと考えます。※今回は各種ツール特性のお話しは致しません。汎用的なツールを使用する際の例として記載します。冪等性(べきとうせい)が担保されること冪等性とは何度実行しても結果が変わらないことを指します。インフラの構築は初回に初期化が必要なものもあり、実行の度に初期化されるわけにはいきません。その中でどのような状態から何度実行しても既存の設定に変更が加わることがなく結果は等しくなるというのは非常に重要な事となります。クラウド化している以上簡単に構成変更できるのが強みとなるため、一度作ったIaCは何度も変更を加えつつ使用できるのが最低条件となります。各環境に同じデータが入れられること本番環境のみという環境は基本的にはあまり存在しないことと思います。しかし、各環境の構築までのタイムラインは差があり、その間に様々な事情による構成変更も多々入ってきます。そのすべてを手順書に起こしたり、GUI上の証跡を残そうとしても抜け漏れを完全になくすのは難しいでしょう。しかし、IaCの場合はコード化されているので各環境に流しなおせば基本的に同じものが再現できます。少なくとも全てIaCでしっかり管理していれば抜け漏れはおきません。上記を実現するためにも環境ごとに変数を準備して、コード自体は各環境同一である必要があります。環境とGitで管理しているコードに差分がないことIaCは文字通りコードになりますので、現在ではGitで管理するのが主流になっています。それを考慮するとmasterブランチは常に商用環境と同一であるのがの理想となります。そうすることで、masterブランチを流しなおせば直ぐに現行の商用環境が再現できます。設定ミスや予期せぬ変更が行われた際にすぐ元の商用環境に戻せる=システム安定性を高めることができますし、バグ調査のために試験環境を合わせることもできます。2つ目のメリットとしては設定変更の際にコードを正として考えられるので毎回本番環境の設定を確認したり、商用環境の再現に差分を考慮したりといったことが不要となります。本番リリースの際の手順書作成や事前確認まで考えると小規模リリースを繰り返す昨今では工数としてはかなりの削減となります。上記のメリットを達成するためにも、できればCI/CDといったツールと組み合わせて、git-flowなどのGitのお作法に乗っ取り管理することが、工数を削減するためにも、安定性を高めるためにもベストとなります。図1: Gitと各環境に対するツールの組み合わせ例。ツール自体は条件を達成できるのであれば種類は問いません。プロジェクトからインフラに求められること上記のIaCのメリットとしてはGoogle等で調べてもほぼ同様のメリットがでてくると思います。しかし、プロジェクトがインフラに求めていることと、IaCが実現していることは同じなのでしょうか。構成変更しやすい環境であることプロジェクトにおいてインフラは上にアプリが乗って初めて完成となります。現在はある程度SaaSサービスがそろってきたとはいえ、やはり最終的な部分は自分たちで構築することが多いです。そのため、工数としてはどうしてもアプリ改修>インフラとなってしまうことが多く、アプリで問題が起きた時にインフラで吸収できることは吸収するというのがプロジェクトトータルで見た時に効率の良い方法かと思います。※ただし、セキュリティや今後の拡張性などを考えた時、受け入れられない変更はあってしかるべきかと思います。工数と安定性を天秤にかけることシステムには当たり前ですが安定稼働が求められます。しかし、全システムがSLA99.999%を求められているわけでもありません。また、ユーザ体感としても基本的にはメンテ時間がない代わりに機能が少ないアプリよりも、毎月1時間のメンテ時間がある代わりに様々な機能が追加され使い勝手の良いアプリの方が使用し続けてくれる可能性は高いかと思います。そういった中でインフラの過剰な安定性に工数をかけるよりはその分の工数をアプリにかけた方がプロジェクトとしても継続しやすいかと思います。もちろん、そのために将来の工数を減らすべくIaCを導入するのはもちろんですが、逆に過剰な管理が足を引っぱらないかは考える必要があります。プロジェクトとIaCの折り合いでは、過剰な管理にならないために。構成変更しやすい構築とはどのようなものでしょうか。実際にプロジェクトで管理している際に基準にしていること、実施していることをまとめてみました。細かいケース、一時ケースは考慮しないことプロジェクトから求められる一時的な即時対応と、IaCは相性があまりよくありません。現場ではバグなどの関係で一時しのぎを本番のみ適応というケースも実情としてあるかと思います。その中で特定の環境専用の特殊な処理を一々書いているとコードを直している時間の方が多いですし、せっかくの各環境の統一性を崩してしまいます。複雑なコードはそれだけバグを生みやすくなってしまうので、そういったケースはIaCにこだわらず手で管理し、アプリの改修を待って元に戻した方が結果的に事故も少なく、工数の面でも優秀と言えるでしょう。実際にterraformのチュートリアルページでもエッジケース, 条件分岐は管理しないことが推奨されています。参考URLデプロイ粒度を調整できるようにすること。本来、一括で管理できるのがIaCの魅力でもありますが、上記の一時対応などにより、全体でリリースするとIaCの特性により一時変更部分が戻ってしまうことが考えられます。対策としては2案考えられ、1つは恒久的なエッジケースとなってしまうのであればIaCのデプロイ範囲から外すこと、2つ目はデプロイ事態を全体と個別に行えるようにすることです。1つ目はそのまま上記にもあるように恒久的なエッジケースになるのであればIaCからは外すべきです。2つ目は柔軟なリリース手段を持っておくようにするのが良いかと思います。一時的な変更であればIaCから外すよりも一旦別リリースできるようにしておけば元に戻すときもIaCの力を借りれますのでこちらが便利かと思います。図2:GitHub Actionsによるパイプライン例。moduleを小分けにすることでデプロイ単位を調整可能としています。工数と労力を比較することでは一時的な変更は全てデプロイから外すべきかというとそんなことは有りません。もちろん、改修の優先度が低く数か月かかる案件であったり、IaCの変数の変更程度で済むのであれば改修を行うべきかと思います。小さな改修に手間がかかってしまうようであればそれはgitのフローやCI/CD側を見直す必要があるかと思います。改修までの時間、直すべきコードの量、IaCから外した際の管理コストをトータルで見て管理するかしないかを決めるべきでしょう。IaCはインフラを管理しやすく、安定性を高めるためのツールなのでIaCに固執しすぎることで工数の増大の促してしまったり、コードを複雑化させて安定性が犠牲になってしまうことを避けられればと思います。終わりに今回はIaC周りとプロジェクトについてお話しさせていただきました。結論としてはIaCに固執しすぎても身動きが取れなくなりますし、IaCから管理を外しすぎてしまうとIaCの恩恵を受けられなくなってしまいます。先ほどのterraformの公式でもmoduleは8割で動くものという記載があり、逆に言えば残りの2割はIaCで管理しなくともいいとも取れます。理想は全て管理しきることですが、それを実施することでIaCとしてのデメリットが多く出てしまう場合、または管理しきれなくなって設計書代わりにしかなっていない場合は管理部分を見直してみるのはいかがでしょうか。私自身もこれがベストだ!というものを試行錯誤していますので汎用的にできるメソッドを整理していきたいと思います。"
https://www.ariseanalytics.com/activities/report/20211027/,生存時間分析入門,"ARISE Tech Blog2021.10.27Tweet生存時間分析入門初めまして。Social Innovation Divisionの木本と申します。 普段の業務では、ヘルスケアアプリで収集されるデータの分析を行っています。本記事では生存時間分析について概要を説明した後、通信会社の顧客離脱に関するサンプルデータを用いたマーケティング分野での活用例を紹介します。生存時間分析とはあるイベント(例えば疾病発症)が起きるまでの時間に焦点を当てる分析方法を生存時間分析といいます。例えばAさんが何年後に「疾病を発症するか」知りたいとします。生存時間分析を行うと、Aさんについて、①生存関数と呼ばれる時点ごとの生存確率の予測値が取得できます。また、②生存確率に対する発症タイミングを予測することができます。生存時間分析の流れあるデータに対して生存時間分析を用いる場合、生存期間とイベント発生有無について整理する必要が出てきます。 ここでイベント発生無しと定義したデータを打ち切り（※）と呼び、対象にイベントが起こらずに観察期間が終了した場合やなんらかの理由で対象を観察できなくなってしまったなど、生存時間が正確にわからない場合のイベント定義を指します。※正確には、不完全な観測値には打ち切りと切断の2つのメカニズムが存在しますが、この記事では取り扱いません。一般的なモデルとの違い通常、観測データには、打ち切りデータが存在します。一般的な回帰モデルや分類モデルでは打ち切りデータを扱うことができないため、分析対象から除外することになりますが、生存時間分析のモデルは打ち切りデータを扱えるという特徴があります。これにより、特に打ち切りデータが多いデータに対しては、妥当な分析ができると考えられます。生存時間分析の適用例生存時間というと、医療・ヘルスケアに限った分析手法として見られがちですが、イベントの定義を工場の機械の故障やECサイトにおけるサービスの退会とすると、製造業やサービス業においても有用なモデリング手法として用いることができます。生存時間分析で用いられるモデルPing Wang et al. (2019)上記は生存時間分析に関するサーベイ論文[1]で述べられている生存時間分析の分類になります。生存時間分析で用いられるモデルとしては、大きく以下の4つに大別されます。ノンパラメトリックセミパラメトリックパラメトリック機械学習これらのモデルでは生存時間の確率変数Tを用いて以下で定義されるハザード関数を導入します。定義からわかるように、これはある時点tまで生存したという条件のもとで、微小時間内に死亡する確率のようなものと捉えることができます。セミパラメトリックモデルのCox比例ハザードモデルは最もよく用いられており、多くの医学系論文では本手法を用いて生存時間の推定が行われています。Cox比例ハザードモデルでは、このハザード関数を以下のように定式化した上でモデルのフィッティングを行います（x_1, x_2…x_pは説明変数）。 βは部分尤度の最大化により推定されます（最尤推定）。このモデルは、ハザード比（≒リスク比）が時間によらず一定であるという重要な特徴を持っています。この特徴を比例ハザード性と呼びます。例として x_1=𝑎 と 𝑥_1=𝑏 のハザード比を考えると、この特徴のお陰で、Cox比例ハザードモデルを用いてモデリングを行うと、ある共変量が異なった場合のリスク比が定量化できるようになり解釈が容易となります。顧客離脱を対象とした生存時間分析最後に、弊社らしく通信会社のデータを用いて生存時間分析で示唆を出してみましょう。サンプルデータについてサンプルデータとして、今回はKaggleで公開されているTelco Customer Churn[2]を用います。 各行は顧客を表し、各列にはメタデータ列に記述されている顧客の属性が含まれています。データセットには、以下の情報が含まれています。顧客が各種サービスを契約したか否か：電話サービス、複数回線、インターネット、オンラインセキュリティ、オンラインバックアップ、テクニカルサポート、テレビや映画のストリーミング等顧客情報：顧客であった期間、契約、支払い方法、ペーパーレス決済、月額料金、および合計料金基礎情報：性別、年齢層、およびパートナーと扶養家族がいるかどうかCox比例ハザードモデルを用いたモデリングPythonでは、lifelinesというパッケージを用いると、生存時間分析を簡単に実装することができます。以下で使用しているinput_dfは、上述した公開データを整形して作成しました（ダミー化及び欠損除去）。# lifelinesをインポート
import lifelines
# lifelinesパッケージからCox比例ハザードモデルを使用 
cph = lifelines.CoxPHFitter()
# duration_colに生存時間の列を、event_colにイベントの列を指定 
cph.fit(input_df, duration_col='tenure', event_col='Churn', show_progress=False) 
# 回帰係数、Zスコア、p-value等をまとめた表を表示
cph.print_summary()結果では、exp(coef)に着目します。モデルの定義から、これが1を超えると、その説明変数が高くなった場合にイベント発症確率を高くする方向に寄与、1よりも低いとその説明変数が高くなった場合にイベント発症確率を低くする方向にその共変量が寄与していると考えることができます。例えばPartner（パートナー有無）のexp(coef)は0.82となっています。ここから、パートナーがいる場合（Partner=1）の解約のしやすさはパートナーがいない場合（Partner=0）と比較して0.82倍となっており、パートナーがいる方がやや解約しにくいことがわかります。Forest Plotの描画上述したような係数の解釈を視覚的に行うため、各変数の係数とその信頼区間を高い順にプロットします。このプロットをForest Plotと呼びます。# 係数とその信頼区間を描画
fig_coef, ax_coef = plt.subplots(figsize=(12,7))
ax_coef.set_title('Forest Plot') 
cph.plot(ax=ax_coef)契約形態ごとに生存関数を描画Contract変数は契約形態を表しており、月ごと契約（Month to Month）、1か年契約（One year）、2か年契約（Two year）の3種類存在します。今回のデータではContract変数をエンコーディングしたContract_One year、Contract_Two yearというダミー変数に変換されています。Contract_One year、Contract_Two yearの値から、解約率抑止に最も寄与しているContract（契約形態）変数について、他のすべての変数を一定としながら単一の共変量を変化させた場合の生存曲線を描画します。cph.plot_partial_effects_on_outcome(['Contract_One year', 'Contract_Two year', ], values=np.eye(2))生存関数を40か月経過後で比較すると、月毎契約（ベースライン）の人の継続確率はほぼ0%となっているのに対し、1か年契約の人は30%程度、2か年契約の人は約90%程度となる推定がされていることがわかります。その他の変数についても同様に見ていくことで、その影響度を可視化し示唆につなげることができます。終わりに今回は単純なCox比例ハザードモデルを例に、分析例を見ていきましたが、 最近では、Random Survival Forests[3]やDeep Surv[4]といった生存時間分析に機械学習を適用している手法も現れてきています。イベント発生に関するデータがあれば、是非一度利用を検討してみて下さい。参考文献[1] Ping Wang, Yan Li, and Chandan K. Reddy. 2019. Machine Learning for Survival Analysis: A Survey. ACM Comput. Surv. 51, 6, Article 110 (February 2019), 36 pages. DOI:https://doi.org/10.1145/3214306[2] https://www.kaggle.com/blastchar/telco-customer-churn[3] https://arxiv.org/pdf/0811.1645.pdf[4] Katzman, J.L., Shaham, U., Cloninger, A. et al. DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC Med Res Methodol 18, 24 (2018). https://doi.org/10.1186/s12874-018-0482-1"
https://www.ariseanalytics.com/activities/report/20211118/,Scala×Sparkによる位置情報分析例のご紹介 part2,"ARISE Tech Blog2021.11.18TweetScala×Sparkによる位置情報分析例のご紹介 part2こんにちは。Marketing Solution Divisionに所属している2年目データサイエンティストの山嵜です。私はデータ分析を通してマーケティングのサポートを行うBIツールの開発チームに3ヶ月ほど在籍したのち、位置情報を扱う分析チームに移籍しました。こちらのチームではデータ分析として広く一般的に扱われているPythonのみならず、Scala×Sparkを用いて位置情報データ分析を実施しております。前回の記事ではScala×Sparkを使った位置情報データ分析の例として、Sparkのwindow関数とudfを用いて2点間の距離を求めるコードの実装例をご紹介させて頂きました。今回の記事では、より位置情報特有の話題として、緯度経度⇔メッシュコードの相互変換の実装例をudfを用いずにご紹介したいと思います。位置情報の利用前回の記事でも述べましたが、最近では位置情報データを使ったマーケティング活動が注目されてきております。弊社でも、近年位置情報データを用いた分析を幅広くおこなっております。そこで本日は位置情報をより扱いやすいデータに集約するための方法として、GPS位置情報データから取得される緯度・経度をメッシュコードに相互変換する処理をご紹介したいと思います。さらに、こちらの実装はScala言語と並列分散処理フレームワークSparkを用いており、位置情報のような大規模データを高速に処理することができるので、同じような大規模データを扱っている方は参考にしてみてください。緯度経度とメッシュコードまず実装をご紹介する前に、あまり馴染みのないメッシュコードに関して簡単に説明いたします。メッシュコード（引用文中では「地域メッシュ統計」と言及）とは、総務省統計局によると、以下のように定義されております。地域メッシュ統計とは、緯度・経度に基づき地域を隙間なく網の目（Mesh）の区域に分けて、統計データをそれぞれの区域に編成したものです出典：https://www.stat.go.jp/data/mesh/pdf/gaiyo1.pdfつまり、メッシュコードは、緯度経度のような「点」とは異なり、ある程度の幅を持った四角形の「範囲」を示しております（図1参照）。図1：緯度経度とメッシュコードの違い（イメージ図） ※図中の緯度経度とメッシュコードは実際と異なる場合があります。また、いくつかの緯度経度の「点」をメッシュコードの「範囲」に変換することで、地域ごとに情報の集約が可能になります。したがって、大規模データをある程度集約した扱いやすい状態にすることができます。また、メッシュコードは4桁～11桁の数値列から成ります。各桁ごとに区画が定義されており、上位桁から第1次地域区画、第2次地域区画・・・と広域の区画から狭域の区画に絞り込む形で領域が定義されています（表1参照。詳細に関して出典中「４ 地域メッシュの区分方法」を参照）。表１：メッシュスケールごとの大きさと対応する桁緯度経度⇒メッシュコードの変換それでは、実際の実装コードをご紹介いたします。まずは、緯度経度から11桁のメッシュコードへの変換です。総務省統計局の定義に従って実装していきます。今回は緯度経度の表記方法として、10進法を採用します。import org.apache.spark.sql.Column
  import org.apache.spark.sql.functions._
  import org.apache.spark.sql.types._
   
  /**
    * 緯度・経度からメッシュコードを計算する。
    *
    * @param latitude 緯度
    * @param longitude 経度
    * @return メッシュコード
    */
  def coordinatesToMeshCode(latitude: Column, longitude: Column): Column = {
    val decimal = new DecimalType(10, 7)
    val lat = latitude.cast(decimal)
    val lon = longitude.cast(decimal)

    def splitByDecimalPoint(input: Column): (Column, Column) = {
      val integerPart = input.cast(IntegerType)
      val decimalPart = input - integerPart
      (integerPart, decimalPart)
    }

    // 緯度の計算
    val (latInt80km, latDecimal80km) = splitByDecimalPoint(lat * 1.5) // 1次メッシュ
    val (latInt10km, latDecimal10km) = splitByDecimalPoint(latDecimal80km * 8) // 2次メッシュ
    val (latInt1km, latDecimal1km) = splitByDecimalPoint(latDecimal10km * 10) // 3次メッシュ
    val (latInt500m, latDecimal500m) = splitByDecimalPoint(latDecimal1km * 2) // 1/2メッシュ
    val (latInt250m, latDecimal250m) = splitByDecimalPoint(latDecimal500m * 2) // 1/4メッシュ
    val (latInt125m, _) = splitByDecimalPoint(latDecimal250m * 2) // 1/8メッシュ

    // 経度の計算
    val (lonInt80km, lonDecimal80km) = splitByDecimalPoint(lon - 100) // 1次メッシュ
    val (lonInt10km, lonDecimal10km) = splitByDecimalPoint(lonDecimal80km * 8) // 2次メッシュ
    val (lonInt1km, lonDecimal1km) = splitByDecimalPoint(lonDecimal10km * 10) // 3次メッシュ
    val (lonInt500m, lonDecimal500m) = splitByDecimalPoint(lonDecimal1km * 2) // 1/2メッシュ
    val (lonInt250m, lonDecimal250m) = splitByDecimalPoint(lonDecimal500m * 2) // 1/4メッシュ
    val (lonInt125m, _) = splitByDecimalPoint(lonDecimal250m * 2) // 1/8メッシュ

    // メッシュコードの計算
    val meshCode80km = (latInt80km.cast(LongType) * 100 + lonInt80km).cast(StringType)
    val meshCode10km = concat(abs(latInt10km).cast(StringType), abs(lonInt10km).cast(StringType))
    val meshCode1km = concat(abs(latInt1km).cast(StringType), abs(lonInt1km).cast(StringType))
    val meshCode500m = (latInt500m * 2 + lonInt500m + 1).cast(StringType)
    val meshCode250m = (latInt250m * 2 + lonInt250m + 1).cast(StringType)
    val meshCode125m = (latInt125m * 2 + lonInt125m + 1).cast(StringType)

    val meshCode = concat(meshCode80km, meshCode10km, meshCode1km, meshCode500m, meshCode250m, meshCode125m)

    meshCode
  }メッシュコード⇒緯度経度の変換続いて、11桁のメッシュコードから緯度経度への変換です。このとき注意しなければならないのは、メッシュコードは「範囲」にあたるため、中心緯度経度をそのメッシュコードの緯度経度として扱っている点です。また、今回は対応しておりませんが、4～10桁のメッシュコードから緯度経度への変換は、中心座標への補正を適宜調整することで実装することができます。例えば、10桁のメッシュコードから変換する場合には「４分の１地域メッシュ」の中心座標を緯度経度とみなす必要があります。import org.apache.spark.sql.Columnimport org.apache.spark.sql.functions._import org.apache.spark.sql.types._import shapeless.Tuple/*** 11桁メッシュの中心緯度・経度を計算。** @param meshCode 11桁メッシュコード* @return (latitude, longitude) 緯度・経度のタプル*/def meshCodeToCoordinatesGPS(meshCode: Column): (Column, Column) = {// ２分の１地域メッシュ以降のメッシュコードに対応した座標を算出def encodeMeshValue(input: Column): Column = {when(input === 1, lit(Array(0, 0))).when(input === 2, lit(Array(0, 1))).when(input === 3, lit(Array(1, 0))).when(input === 4, lit(Array(1, 1)))}// 緯度の計算val lat = (substring(meshCode, 1, 2) * 2 / 3 + //1次メッシュsubstring(meshCode, 5, 1) * 1 / 12 + //2次メッシュsubstring(meshCode, 7, 1) * 1 / 120 + //3次メッシュelement_at(encodeMeshValue(substring(meshCode, 9, 1)), 1) * 1 / 240 + //1/2メッシュelement_at(encodeMeshValue(substring(meshCode, 10, 1)), 1) * 1 / 480 + //1/4メッシュelement_at(encodeMeshValue(substring(meshCode, 11, 1)), 1) * 1 / 960 + //1/8メッシュ1.0 * 1 / 1920 //中心座標に補正)// 経度の計算val lon = (substring(meshCode, 3, 2) + 100 + //1次メッシュsubstring(meshCode, 6, 1) * 1 / 8 + //2次メッシュsubstring(meshCode, 8, 1) * 1 / 80 + //3次メッシュelement_at(encodeMeshValue(substring(meshCode, 9, 1)), 2) * 1 / 160 + //1/2メッシュelement_at(encodeMeshValue(substring(meshCode, 10, 1)), 2) * 1 / 320 + //1/4メッシュelement_at(encodeMeshValue(substring(meshCode, 11, 1)), 2) * 1 / 640 + //1/8メッシュ1.0 * 1 / 1280 //中心座標に補正)(lat, lon)}おわりに今回は、ScalaとSparkを用いて、緯度経度⇔メッシュコードの相互変換の実装例を紹介しました。このようにデータを相互に変換することで、目的に沿った粒度で分析を実施することができます。もしも位置情報データの扱い方で良いアイデアがあれば、ぜひご意見を頂ければと思います。最後になりますが、もしよろしければ、私が過去に執筆したDjango REST Frameworkの記事もご覧いただければと思います。"
https://www.ariseanalytics.com/activities/report/20211210/,PySparkで線形回帰モデルを作成する,"ARISE Tech Blog2021.12.10TweetPySparkで線形回帰モデルを作成するこんにちは、Customer Analytics Divisionの石川航作と申します。Customer Analytics Divisionはお客様が展開するサービスの改善を目的としたコンサルティング業務を行っております。このサービスは1,000万人以上の会員様にご利用頂いており、一般的なデータ分析でお馴染みのpandasやscikit-learnを用いて分析することは時間的制約から困難です。そこで、弊社では分散処理システムであるSparkとそのpython APIであるPySparkを用いて諸々の分析を行っております。本記事では、PySparkを用いて線形回帰モデルを実装する方法をご紹介します。特に、PySparkならではの処理、線形回帰モデルを作成する上で必要な前処理、モデル作成後の評価についても詳しい説明を記載するように心がけました。なお、本記事は筆者が個人的に書いたQiitaの記事をTech Blogのために加筆・修正したものです。データの読み込み本稿では、Boston Housingをサンプルデータにして説明していきます。下記のようにして、scikit-learnからBoston Housingのデータセットを読み込み、pyspark.DataFrameに変換しておきます。なお、scikit-learnにおいてはBotson Housingはv1.0から非推奨となっており、v1.2で利用不可となるそうです。利用の際は注意していただければと思います。(参考URL：sklearn.datasets.load_boston)# pyspark関連のライブラリ
import findspark
findspark.init()
from pyspark import SparkContext
sc = SparkContext.getOrCreate()

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

# Boston Housingデータセットデータセットの読み込み
from sklearn.datasets import load_boston
dataset = load_boston()

# Boston Housingデータセットをpandas.DataFrameに変換
import pandas as pd

# 説明変数取得
data_x = pd.DataFrame(dataset.data, columns=dataset.feature_names)

# 目的変数を取得
data_y = pd.DataFrame(dataset.target, columns=['target'])

# 説明変数と目的変数を結合して、1つのDataFrameにする
data = pd.merge(data_y, data_x, how=""left"", left_index=True, right_index=True)

# pysparkDataFrameに変換
df = spark.createDataFrame(data)データの前処理本節では、以下の3つの順にデータの前処理についてご説明します。基本統計量(各カラムの行数・平均・分散・最小値・最大値・欠損値の個数)出力の実装方法分析結果の解釈性を高めるための標準化の実装方法多重共線性の評価に用いるVIF出力の実装方法基本統計量出力の実装方法モデルを作る前に基本統計量を確認しておくことは、データの状態を確認しておくという点で非常に重要です。ここでは、pyspark.DataFrameに含まれる各カラムの行数・平均・分散・最小値・最大値・欠損値の個数を計算するスクリプトをご紹介します。まずは、行数の確認です。# 行数の確認
print(""行数:"", df.count())次に、平均・分散・最小値・最大値を計算します。pyspark.ml.stat.Summarizerを使います。from pyspark.ml.stat import Summarizer
from pyspark.ml.feature import import VectorAssembler
from pyspark.sql.functions import col

# 基本統計量を指定
summarizer = Summarizer.metrics(""mean"", ""variance"", ""min"", ""max"")

# featuresを作成
vector_assembler = VectorAssembler(
    inputCols = df.columns, # 各基本統計量を計算したいカラムを指定
    outputCol = ""features""
)

df_features = (
    vector_assembler
    .transform(df)
)

# 基本統計量の計算
df_statistics = df_features.select(summarizer.summary(col(""features"")))出力したdf_statisticsに、各カラムの基本統計量が保存されます。カラム数が多い場合はjupyterで全て表示されないので、csvファイル等に出力して観察すると良いです。また、スクリプト中に「# featuresを作成」という箇所がありますが、これについては後述の「線形回帰モデル作成」のパートでご説明します。最後に欠損値の個数です。# 各列の欠損値の個数の確認
from pyspark.sql.functions import col

null_count_list = [df.where(col(c).isNull()).count() for c in df.columns] #各列の欠損値の個数を計算
null_count_df = pd.DataFrame(columns = df.columns) #欠損値の個数を格納するdataFrameを作成
null_count_df.loc[""null_count""] = null_count_list標準化の実装方法データを以下の式に基づいて、平均0、分散1となるように変換することを標準化と呼びます。なお、今後特別に断らない限り、Nはサンプルサイズを表します。線形回帰モデルにおいて、インプットとなるデータを標準化することで推定された回帰係数の大きさを比較することが可能になります。PySparkにはpyspark.ml.feature.StandardScalerというメソッドが既に実装されており、これを用いることでデータの標準化を実行することが可能です。しかし、各カラムの名前をコントロールでき、かつ変換を実行するスクリプトがわかりやすくなるように、今回は各カラムを一つずつ標準化する方法をご紹介します。# IDを振っておく(便利のため)# importfrom pyspark.sql.functions import monotonically_increasing_id#IDを連番で振るdf_id = df.withColumn('ID', monotonically_increasing_id())# 標準化# importfrom pyspark.sql import functions as Ffrom pyspark.sql.window import Window# 目的変数、説明変数の抽出var = df_id.columnsvar.remove(""ID"") #ID列を排除# 標準化z_df = df_idfor c in var:tmp1_z_df = (df_id.withColumn('mean', F.mean(c).over(Window.orderBy())).withColumn('std', F.stddev(c).over(Window.orderBy())).withColumn(c+'_z_scaled', (F.col(c) - F.col('mean')) / F.col('std')) # 実際に標準化している箇所はココ！)tmp2_z_df = tmp1_z_df.select(""ID"", c+""_z_scaled"")z_df = z_df.join(tmp2_z_df, [""ID""], ""left"")VIF出力の実装方法線形回帰モデルには多重共線性という問題があります。多重共線性とは、相関が高い2つ以上の特徴量を線形回帰モデルにインプットしたときに、推定されたパラメータが不安定になってしまうという問題です。ここでは、VIF(Variance Inflaction Factor)という指標を用いて多重共線性が発生しそうな変数を排除します。VIFとは、各説明変数が他の説明変数でどれくらい説明できるかを表す指標です。VIFは2つのステップを経て計算されます。まず、説明変数X_kに対して、X_kを目的変数、X_1, …, X_k-1, X_k+1, X_pを説明変数とした線形回帰モデルを作成します。次に、上記の線形回帰モデルから得られた決定係数R_kを用いてVIFを計算します。このようにして得られたVIFについて、一般的に分析者が設定する閾値を超える説明変数があったときに、その変数を説明変数から除外することで多重共線性を回避します。今回は閾値としてよく使われる10を超えた変数は存在しなかったため、変数の除外は行いませんでした。VIFについての詳細はKutner, M., et al(2005)を参照してください。VIFをPySparkで計算するスクリプトは下記です。なお、スクリプト中に「# featuresの作成」とありますが、これについては後述します。# importfrom pyspark.ml.feature import VectorAssemblerfrom pyspark.ml.regression import LinearRegression# VIFを格納するリストVIF_list = []# 説明変数の取得tmp_ex_var = z_df.columnsex_var = [ev for ev in tmp_ex_var if ""_z_scaled"" in ev] # 標準化した変数のみ取得ex_var.remove(""target_z_scaled"") # 目的変数を排除# VIFの計算for c in ex_var:# featuresの作成##VIF計算時に説明変数となる変数の取得VIF_ex_var = [c2 for c2 in ex_var if c2 != c]## VectorAssemblerインスタンス作成vector_assembler = VectorAssembler(inputCols = VIF_ex_var,outputCol='features')## 線形回帰モデルのインプットとなるpyspark.DataFrameの作成VIF_z_df = (vector_assembler.transform(z_df).withColumnRenamed(c, ""label""))# 線形回帰lr = LinearRegression(regParam = 0.0, fitIntercept = False)lrModel = lr.fit(VIF_z_df)trainig_summary = lr.summary## VIFの計算VIF_list.append(1/(1-trainig_summary.r2))## 今何を計算しているか可視化print(""目的変数:"", c, ""\n"")print(""説明変数:"", VIF_ex_var, ""\n"")# pandasDataFrameに格納してVIFを見やすくするVIF_df = pd.DataFrame(index = ex_var)VIF_df[""VIF""] = VIF_listここでは、VIFが10を超える変数が存在しなかったので、説明変数を取り除くことはせずに次に進みます。線形回帰モデル作成前処理が済んだらいよいよ線形回帰モデルを作成します。ここでは、pyspark.LinearRegressionを使います。ただし、LinearRegressionを使う際には、pyspark.DataFrameを変形する必要があります。下図を見てください。左側が特に何も処理をしていないpyspark.DataFrameで、右側がLinearRegressionのインプットとなるDataFrameです。右側のDataFrameには説明変数をリスト化した値を持つfeaturesというカラムが追加されています。実は、pyspark.LinearRegressionは説明変数の入力をリストで受け付けるので、データセットもそれに合わせて変換する必要があります。上記スクリプト内で「# featuresの作成」という処理がありましたが、そこでは説明変数を一つのカラムにまとめる作業を行っています。なお、説明変数をリスト化した値を持つカラムの名前は自由につけられますが、LinearRegressionのデフォルトがfeaturesを説明変数として線形回帰モデルを作成するメソッドであるため、今回はfeaturesというカラム名にしています。featuresを作成するためのスクリプトは下記です。VectorAssemblerを使います。# import
from pyspark.ml.feature import VectorAssembler# featuresの作成vector_assembler = VectorAssembler(inputCols = ex_var, # featuresに入れたいカラムをリストで指定outputCol = 'features')feature_z_df = (vector_assembler.transform(z_df) # featuresが含まれたpyspark.DataFrameの作成.withColumnRenamed(target_var, ""label"") # LinearRegressionで指定する目的変数のデフォルトがlabelなのでlabelにカラム名を変えた)ここまで来たら、後はfeature_z_dfをLinearRegressionメソッドの引数とすることで、線形回帰モデルを作成することができます。スクリプトは下記です。# 線形回帰モデルの作成
lr = LinearRegression(regParam = 0.0, fitIntercept = True) # 定数項がある場合
# lr = LinearRegression(regParam = 0.0, fitIntercept = False) # 定数項がない場合

lrModel = lr_intercept.fit(feature_z_df)また、作成したモデルに対して予測値を計算するスクリプトは下記です。# 予測値の計算
predict_summary = lrModel.evaluate(feature_z_df)モデル評価線形回帰モデルの作成まで終わりました。本節では、作成したモデルの評価を行う方法についてご説明します。以下の2つの手順でモデル評価をしていきます。1.作成したモデルが線形回帰モデルの仮定を満たしているか確認・残差の分散は均一か確認・残差が正規分布にしたがっているか確認2.モデル選択基準の計算・AIC・汎化性能線形回帰モデルの仮定を満たしているか？残差の分散は均一か？ここでは、残差の分散が均一であるかどうかを確認します。ただし、PySparkよりも散布図のプロットを簡潔に記載できるpandas.Dataframeにより可視化します。# Pandasに変換sampled_predict_residuals_pd_df = (predict_residuals_df.select(""prediction"", ""residuals"").toPandas() #Pandasに変換)# 散布図(予測値×残差)作成import matplotlib.pyplot as pltplt.scatter(sampled_predict_residuals_pd_df[""prediction""], sampled_predict_residuals_pd_df[""residuals""])plt.axhline(y = 0, xmin = min(sampled_predict_residuals_pd_df[""prediction""]), xmax = max(sampled_predict_residuals_pd_df[""prediction""]), color = ""black"", linestyle = ""dotted"")上記でpandas.DataFrameに変換していますが、データが巨大な場合、pandas.DataFrameへの変換が困難なこともあると思います。その際は、pyspark.DataFrameに含まれるすべてのデータを確認するのではなく、ランダムに抽出した一部のデータを確認することをお勧めします。下記のスクリプトでデータをランダムに抽出することができます。# ランダムサンプリングする割合rate = 0.1# ランダムサンプリング & Pandasに変換sampled_predict_residuals_pd_df = (
    predict_residuals_df
    .select(""prediction"", ""residuals"").sample(rate) # ランダムサンプリングしている箇所はココ！.toPandas() #Pandasに変換)描いた散布図は下記です。残差が正規分布にしたがっているか？ここでは、QQプロットを作成して確認します。# QQプロット作成
import scipy.stats as stats
stats.probplot(sampled_predict_residuals_pd_df[""residuals""], dist = ""norm"", plot = plt)
plt.show()作成した図は下記です。モデルの選択基準ここでは、モデルの選択基準であるAIC(Akaike’s Information Criterion)と汎化性能の測定をします。なお、今回は汎化性能はRMSE(Root Mean Squared Error)を指標としてホールドアウト法で実装しています。また、本来は複数のモデルを作成し、比較することでAICや汎化性能などの指標は意味を持ちます。しかし、本記事の趣旨は実装方法を説明することなので、モデルの比較は行いません。AIC線形回帰モデルにおけるAICを計算するスクリプトは下記です。誤差項が平均0、分散一定の正規分布にしたがっていることを仮定しています。ちなみに今回のモデルは、AIC=780.3でした。# サンプルサイズN= feature_z_df.count()# パラメータの個数k = len(ex_var)+1 # 定数項がある場合# k = len(ex_var) # 定数項がない場合# 残差の2乗和の計算var_hat = (predict_residuals_df.withColumn(""residulas_2"", col(""residuals"")**2).agg(F.sum(""residulas_2"").alias(""sum_residulas_2"")).collect()[0].sum_residulas_2)/(N-k)# AICの計算from math import log, sqrt, piAIC = -2*N*log(1/sqrt(2*pi*var_hat))+N+kprint(""AIC = "", AIC)汎化性能汎化性能を計算するためのスクリプトは下記です。今回のモデルにおいてはRMSE=0.45でした。# 訓練データとテストデータに分割feature_z_df_train, feature_z_df_test = feature_z_df.randomSplit([0.8, 0.2], seed=2021)# 訓練データについて定数項ありで線形回帰モデルの作成lr = LinearRegression(regParam = 0.0, fitIntercept = True)lrModel_train = lr_intercept.fit(feature_z_df_train)# テストデータについて予測値の計算predict_summary_test = lrModel_train.evaluate(feature_z_df_test)predict_df_test = predict_summary_test.predictions# RMSEの計算## 予測値と実現値を一つのdataFrameにまとめるfeature_z_df_test_id = feature_z_df_test.select(""ID"", ""label"")predict_df_test_id = predict_df_test.select(""ID"", ""prediction"")# 実現値・予測値の両方が入ったDataFrameの結合predict_act_df_test = (feature_z_df_test_id.join(predict_df_test_id,[""ID""],""left"").select(""label"", ""prediction""))# RMSEの計算from pyspark.mllib.evaluation import RegressionMetricsmetrics = RegressionMetrics(predict_act_df_test.rdd)print(""RMSE = %s"" % metrics.rootMeanSquaredError)終わりに本記事では、線形回帰モデルをPySparkで実装する方法についてご説明しました。ちなみに、私が調査した限りでは、日本語でここまで網羅的にまとめた記事はありませんでした。冒頭でも述べた通り、ARISE analyticsではサンプルサイズが1,000万を超えるようなデータを分析する機会が多いです。線形回帰モデルをはじめとした基本的なモデルにおいても、大規模データを分析できるような並列処理に適したライブラリが存在しなかったり、日本語でまとめられた記事が存在しなかったりと苦労することが多いです。しかし、このような大きいデータから知見を抽出することは、データサイエンティストにとって非常に魅力的な作業の一つです。この作業の助けになればと思い、本記事を執筆しました。本記事を通じてPySparkに対する知見が深まり、また、ARISE analyticsに興味を持ってくださる方がいらっしゃれば筆者として喜ばしい限りです。最後までお読みいただきありがとうございました。参考文献Kutner, M., Nachtsheim, C., Neter, J., & Li, W. (2005) Applied Linear Statistical Models. McGraw-Hill Irwin, New York"
https://www.ariseanalytics.com/activities/report/20211224-2/,Concentration Timeを活用した分析コンペ参加体験記,ARISE Tech Blog2021.12.24TweetConcentration Timeを活用した分析コンペ参加体験記こんにちは。2020年4月に新卒として入社したMaketing Solution Division (MSD) 所属の小林と申します。前回の記事では、私が従事していたWEB閲覧履歴によるターゲティングモデルについての技術紹介をさせていただきました。今回の記事は、業務技術紹介ではなく分析スキル向上のため同期と参加したKaggleの分析コンペ参加体験記となっております。記事の内容としては、分析コンペ参加動機・コンペ概要・コンペ内で使用した一部技術紹介になります。ARISEanalytics社内の若手データサイエンティストがどのようにスキル向上を図っているか知っていただけると幸いです。分析コンペ参加動機分析コンペに参加しようと思ったきっかけは、業務では扱わない技術やデータに触れて自分のデータサイエンス・エンジニア力を伸ばして行けたらよいなと思ったことです。また、自分が所属しているチームが現在取り組んでいるプロジェクトのビジネス要件にマッチしない機械学習手法やデータがあり、その周辺知識を別途獲得したいとも感じていました。そこで、同期がkaggleコンペに参加する話があったので合流することにしました。またARISE社内ではConcentrationTime*という制度があり、私はその制度を活用して分析スキル向上を図りました。*Concentration TimeについてARISE analytics社内では毎週金曜日の午前中は自己研鑽の時間に割り当ててもよいという制度があります。その時間は各部署や全社にて勉強会・LTが活発に行われており、社員は運営や受講者として参加しています。また、今回私が取り組んだような分析コンペ参加に向けて、技術力向上のために自己学習する社員も多いです。◆Concentration Timeをはじめ、ARISE analyticsの成長環境や制度についての記事がこちら社員の成長を加速させる！独自の教育体系「ARISE university」とは‥？アウトプットによりインプットを深化！全社横断組織で運営する社内勉強会『ARISE university Training』とは？参加した分析コンペについてコンペ概要今回参加したKaggleのコンペはGoogle Smartphone Decimeter Challengeという位置情報推定をするコンペです。アメリカ州内で車を走行させ、Androidスマートフォンによって収集されたデータ(GPS衛星からの信号、加速度計・ジャイロスコープの読み取り値)を分析し、精度よく位置情報(緯度・経度)を推定する内容となっています。使用データデータ収集方法は種類の異なる複数のAndroid端末を車両に搭載させ、異なる日付と走行ルート(一般道、高速道路、並木道)にて車を走行して収集するという方法です。各データの概要は以下の表になります。評価指標予測するデータは各Android端末の時間ごとの位置情報(緯度・経度)です。評価方法は正解データと予測データの50パーセンタイル誤差と90パーセンタイル誤差の平均値となります。具体的には各Android端末ごとに日付が異なる位置情報データがあるので、その端末内での50パーセンタイル誤差と90パーセンタイル誤差の平均を求めます。その後に各端末の50パーセンタイル誤差と90パーセンタイル誤差の平均に対して平均値を取ることで評価値を算出する形になります。扱った技術紹介(カルマンフィルタ)今回のコンペでメインに扱った手法はカルマンフィルタです。カルマンフィルタというものは状態空間モデルの1種であり、航空・ロボット・信号・画像処理などの幅広い分野で活用されている技術になります。具体的な例ではロケットやロボットの自己位置推定が挙げられます。今回のコンペでは、baseline_locationsというデータが既に高い精度で位置情報を予測していたため、このデータをカルマンフィルタで補正してあげる(自己位置推定)というアプローチを取りました。そのため、簡単にカルマンフィルタについて紹介します。カルマンフィルタは以下の式のような考え方になります。補正後の状態　＝　補正前の状態 ＋ カルマンゲイン ×（本物の観測値－補正前の状態）状態方程式と呼ばれる式から1つ前の状態を基に現在の状態を計算し、実際の観測値と状態の誤差を反映し状態を更新するという流れになります。カルマンゲインは補正度合いを調整する倍率となります。今回のコンペの場合は以下のような式になります。誤差修正した位置情報推定値 = 1時刻前の情報から算出した現時刻位置情報推定値 + カルマンゲイン × (baseline_locationsの位置情報ー1時刻前の情報から算出した現時刻位置情報推定値)このようにすることで各時刻のbaseline_locationsの位置情報推定時に発生するノイズ(正しい位置情報との誤差)を修正することで、より真の位置情報に近づけることができます。終わりに私は、ARISE analytics特有の制度であるConcentration Timeを活用し同期と一緒にKaggleコンペ(位置情報予測)に参加しました。今回挑戦したコンペでは位置情報の膨大データ探索技術と位置情報補正技術であるカルマンフィルタの知識を得ることができました。引き続き別のコンペにも参加し、自身のサイエンス・エンジニア力を伸ばしていけたらと思っています。
https://www.ariseanalytics.com/activities/report/20220128/,社内勉強会・資格取得制度を用いてAWS SAAの資格を取得した話,ARISE Tech Blog2022.01.28Tweet社内勉強会・資格取得制度を用いてAWS SAAの資格を取得した話初めまして、Marketing Solution Divisionの原田と申します。ARISE analytics新卒1期生として入社し、すでに2年以上が経過しました。時間が経つのは一瞬ですね。さて、今回の記事では、ARISEanalyticsの数あるスキル研鑽の取り組みの一つである有志勉強会で行われた「AWSカスケードトレーニング」に参加し、資格取得制度を用いてSAAの資格を取得した件についてご紹介したいと思います。ARISE universityとは？データサイエンティストとして第一線であり続けるためには常に最先端の知識を習得し、スキルをupdateし続ける必要があります。一方で忙しく働いているとスキル研鑽の時間を確保することが困難であり、会社として従業員のスキル研鑽を支援する必要があるとARISEanalyticsは考えています。そこでARISEanalyticsは強みであるOJTを支える教育のしくみ・制度の体系全般をARISE universityと定義し、社員のスキル研鑽を支援する制度を策定しました。全体像としては4つのカリキュラムがOJTを支えています。👋On-Boarding（オンボーディング）：入社時の戦力化及び定着支援📖Learning（ラーニング）：インプットにかかる書籍・論文、外部研修受講などの費用支援💪Training（トレーニング）：インプットしたことの知識・スキルを深化させるためのアウトプットの支援🍀Career（キャリア）：会社という場や社内制度を使いながら、自分でキャリアを築いていくための支援制度の中には毎週金曜日の午前中をスキル研鑽に投資する時間とする「Concentration Time(Concentration Timeを活用した分析コンペ参加体験記)」や書籍・論文購入の「ARISE library (ARISE library#1　〜エンジニア ＋ 総務 ＋データサイエンティストが携わる社内図書システム〜)」、Division持ち回りで行っている各種講座・LTなどがあります。また、「各Division企画」ではTeam内独自の企画が複数あり、以前ご紹介した「競プロ(全社でコーディング力向上を目指せ！～社内競技プログラミング大会の参加者にインタビュー～)」やLT会、輪読会、WS、特定のテーマに対する勉強会、他社様との合同勉強会(リンクが多いのでページ下部に載せます！)など、本当にたくさんのスキル研鑽の取り組みが毎週のように開催されています。今回ご紹介するAWSカスケード式トレーニングはLearning内の「有志勉強会」の位置づけで行われました。AWSカスケード式トレーニングとは？講師による口頭試問を通じてクラウド分野の知識を習得し、AWS Certified Solutions Architect – Associate資格(AWS-SAA)の取得を目指す勉強会です。私が参加した会では、週に2回、3か月間ほどかけて業務開始前の時間帯でトレーニングを行いました。トレーニング終了後にARISE universityの「資格取得」制度を利用し、会社の費用負担により無事資格を取得することができました。AWSカスケード式トレーニングの運営はAccenture Cloud Capability Group(Cloud CG)が行っています。Cloud CGはクラウドケイパビリティの向上を目的としたアクセンチュア社員有志の部門横断のコミュニティです。ARISEanalyticsにおけるカスケード式トレーニングは、アクセンチュア社内でカスケード式トレーニングを受講したメンバーが中心となり実施しています。アクセンチュア社内で構築したカスケード式トレーニングの仕組みとアセットはCloud CGの承認を得たうえで、ARISEanalytics社内におけるクラウドケイパビリティ向上に活用されています。カスケード式トレーニングについて、詳細は以下の記事をご覧ください。Cloud CGメンバーのブログにて、カスケード式トレーニングの仕組みを公開しています。クラウド人材育成　組織のAWSケイパビリティをカスケード式に拡大する｜アクセンチュアAWS Certified Solutions Architect – Associate資格とは？その名の通り、AWSの各サービスの知見を有し、ベストプラクティスに沿って適切なソリューションを設計することができることを認定する資格です。高可用・高パフォーマンスで、セキュアなシステムをユースケースに応じて設計できる能力が求められます。出題範囲が広く、AWSの様々なサービスやクラウドに関する知見も必要になってきます。詳しくは公式ページをご覧ください。(https://aws.amazon.com/jp/certification/certified-solutions-architect-associate/)そもそもトレーニングに参加すると or SAA取れると何が嬉しいの？下記、実際にトレーニングに参加して資格を取得したカスケードチルドレンの声です！↓『ソリューションを作るためのアーキテクチャーの全体像をつかめるようになる』（データサイエンティストSさん）『課題→解決策(=ソリューション)の視座転換の訓練を通じ、周りの人の悩みの解決サポート、及び専門家の巻き込み方の見当が付くようになる』(AIエンジニア Tさん)『業務でなんとなく使っていた・聞いたことがあったAWSサービスについて機能詳細やその真価が理解できるようになる』(データサイエンティスト 原田)私自身分析基盤やそのアーキテクチャーの知識が浅かったため勉強負荷が少し高かったのですが、試験勉強を通じて業務で触れるサービスの理解が高まったので、「クラウドリテラシーを高めたい」と思われている方にはSAAはお勧めです。終わり今回はARISEanalyticsの勉強会や資格取得制度を活用して試験取得をした件についてご紹介しました。あまりプライベートでは勉強したくないタイプなので、会社でこのような制度が整っていると個人的にはすごく助かります。似たような方は是非採用ページ見てみてください！笑 (採用 | 株式会社ARISE analytics（アライズ アナリティクス）)また、記事内でもメンションさせていただきました他社様との合同勉強会については下記にも記事がございますので是非読んでみてください。第一回 合同勉強会「ALBERT×ARISE analytics合同勉強会を開催しました」第二回 合同勉強会「Supership×ARISE analytics合同勉強会を開催しました」第三回 合同勉強会「ちゅらデータ×ARISE analytics合同勉強会を開催しました」第四回 合同勉強会「ALBERT×ARISE analytics合同勉強会を開催しました」第五回 合同勉強会「KDDI総合研究所×ARISE analytics合同勉強会を開催しました」第六回 合同勉強会「KCCS×Rist×ARISE analytics合同勉強会を開催しました」
https://www.ariseanalytics.com/activities/report/20220304/,因果推論の先へ―機械学習で因果効果を予測する『反実仮想機械学習（Counterfactual Machine Learning）』入門,"ARISE Tech Blog2022.03.04Tweet因果推論の先へ―機械学習で因果効果を予測する『反実仮想機械学習（Counterfactual Machine Learning）』入門はじめにARISE analytics の近藤です。本記事では、次世代の意思決定技術として注目されている反実仮想機械学習（Counterfactual Machine Learning：CFML）を紹介します。本記事は、CFMLを日本語で体系的に整理し、初学者の理解を手助けすることをねらいとして執筆しました。本記事の理解促進につながるように、ベースとなった勉強会資料を記載します。こちらも併せて閲覧いただくことで理解の助けになれば幸いです。Counterfaual Machine Learning（CFML）のサーベイfromARISE analytics目次・ はじめに・ Counterfactual Machine Learning(CFML）・ Off-Policy Evaluation（OPE）・ CFMLを支える技術（オープンデータとツール）・ おわりにCounterfactual Machine Learning(CFML）CFMLをめぐるトレンドとビジネスCFMLは産業界・学術界でも注目を浴びており、非常にホットな技術です。産業界では、レコメンド領域で活用されています。たとえば、Netflixのような動画視聴サービスやZOZO TOWNのようなECサービス上で動作するレコメンドアルゴリズムの評価にCFMLが導入されています。学術界では、推薦システムのトップカンファレンスであるRecSysでもCFMLを扱った論文”Pessimistic Reward Models for Off-Policy Learning in Recommendation”がBest Paperに選出されました。それだけではなく、説明可能なAI（XAI：eXplainalbe AI）にも導入が進んでおり、説明性の向上を目的としてCFMLを統合したCounterfactual XAIが提案されています。ARISE analyticsでは、「au PAY」や「auスマートパス」を提供しているKDDIのライフデザイン事業のマーケティング支援を行っています。マーケティングにおいては、顧客体験向上を目的としたプロモーションやキャンペーン（施策）が重要になります。そこで、ARISE analyticsではより効果的な施策提案を目的としたCFMLのビジネス適用を検討しています。マーケティングにおいて課題となるのが施策実行コストです。一般的に、プロモーションやキャンペーンはある程度の期間にわたって行われ、費用もかかります。日経広告研究所の報告では、国内の有力企業3258社の広告宣伝費総額は6兆1275億円と報告されています。企業によって差がありますが、KDDIのようなBtoC企業では特に広告宣伝費が大きくなる傾向にあります。そのため、広告宣伝費の有効活用が求められています。費用対効果の大きい施策に絞り込むことで広告宣伝費を1％でも削減することができれば、金額的に大きなインパクトが期待できるでしょう。この背景を踏まえ、施策実行前の施策効果測定による費用対効果の算出が求められています。これを実現するための技術が、反実仮想機械学習（Counterfactual Machine Learning：CFML）です。CounterfactualとはCFMLを説明するにあたり、まずはCounterfactual（反事実）がどういったものかを説明します。Counterfactualは観測され得たけど、実際には観測されなかったデータのことを指します。別の言葉では、Potential Outcome（ポテンシャルアウトカム）とも呼ばれます。例示は理解の試金石という言葉があるように、具体的な事例は理解促進につながります。ここでも事例を挙げてCounterfactualを考えてみましょう。ケース・何らかの病気症状を持つ個体（ここでは個体Aとする）がいる。・個体に対し、特定の薬を投薬するか否かを選択できる。個体Aは「投薬あり」を選択することにしました。この結果、個体Aの病気症状は改善されました。さて、ここで懐疑的な見方をすると、以下のような疑問が生じます。今回投薬した薬は、個体Aが罹患していた症状に対してほんとうに効果があったのか？「投薬なし」を選択していても、今回の症状は治癒していた可能性があります。薬が本当に効果があったのかを確認するためには、「投薬なし」の結果も観測する必要があります。しかし、個体Aはすでに「投薬あり」を選択していたため、時間を巻き戻して「投薬なし」の結果を取得することはできません。このように、実際に取った行動とは異なる選択肢から得られる結果（観測されなかった結果）を、Counterfactualと呼びます。Counterfactualと統計的因果推論統計的因果推論はCounterfactualを疑似的に観測して評価するための方法のひとつです。以下の資料が詳しいため、本記事では簡単な説明に留めます。https://www.sas.com/content/dam/SAS/documents/event-collateral/2020/ja/japan-sas-users-group-forum/2-01-yada-intro-causal.pdfCounterfactualを観測するためには同一の個体を複数用意し、それぞれ別の行動を取らせる必要があります。しかし、現実的にはできません。そこで、ある程度似た特徴を持つユーザ集団（例：80代・男性）を用意します。このとき、評価内容には影響がなさそうな細かい条件の違い（例：眼鏡の有無、髪の長さ、など）には目をつむります。このユーザ集団を無作為に分割（サンプリング）して２つの集団に分けます。このようにして作成された２つのユーザ集団は、どちらも似た要素を持っているため統計的には同じものとみなすことができます。したがって、ユーザ集団それぞれに異なる選択をとらせることで、選択の効果（＝因果効果）を測定できます（いわゆるA/Bテストとも呼ばれます）。Counterfactual Machine Learning(CFML）とは統計的因果推論がうまく適用できないケースも存在します。たとえば、ユーザ属性に基づいたレコメンドやクーポン配布です。レコメンドを例にして説明します。ECショッピングのレコメンドでは、ユーザの年齢や性別、商品の閲覧履歴といった特徴に基づいて商品のレコメンドを行うことがほとんどです。すなわち、似た特徴を持つユーザに対しては同じようなレコメンド結果しか返ってきません。具体的には、以下の表のようなケースです。ユーザAとユーザDは似た特徴を持っていたため、レコメンドアルゴリズムはアイテム１を推薦しています。統計的因果推論を用いて因果効果を測定するためには、似た特徴を持つユーザ集団を無作為分割して異なる選択肢を取らせ比較する必要があります。しかし、レコメンドでは似た特徴を持つユーザに与えられる選択肢はすべて同じです。したがって、異なる選択肢を取らせることができません。CFMLでは観測範囲が限られたログデータから未観測範囲（Counterfactual World）を疑似的に観測することを目指します。これにより、レコメンドアルゴリズムや施策の本来の効果（因果効果）を測定できるようにします。そして、新しいアルゴリズムや施策を導入し、全体最適化を狙います。これがCFMLの目的です。Off-Policy Evaluation（OPE）この章以降は数式や記号が頻出するため、ここで先に整理します。公開サイトの仕様上、下付き文字などの添え字・数式表記が難しいため多少見づらいところがあるかと思いますが、ご容赦頂ければ幸いです。OPEの考え方CFMLの目的は、未観測領域を含めた全体最適化です。そのため、レコメンドアルゴリズムや施策（＝Policy）の本来の効果を測定するだけでは不十分です。全体がより良くなるようにPolicyを改良して、初めて目的が達成されます。ただし、事前に十分なテストをしないままいきなり本番環境にPolicyを導入するのは非常にリスクのある行為です。そのため、事前にA/Bテストなどの試験を用いて評価することが求められます。ここでよい結果が得られたら、本番環境に導入します。しかし、このような試験には時間も手間もかかります。そこで、試験を仮想的に行うことでコストを抑えるOff-Policy Evaluation（OPE：オフ方策評価）が提案されました。OPEでは過去のPolicyの運用で蓄積されたログデータを活用し、新しいPolicyの評価を行います。したがって、（筆者の知る限りでは）最初に運用したPolicyにはOPEの適用ができない点には注意が必要です。OPEでは、目的とする試験に合わせて評価方法を設計します。設計には様々な考え方がありますが、今回はA/Bテストを例に挙げ、同等の評価ができることを考えます（今回はA/Bテストを例に挙げましたが、ほかの方法を近似してもかまいません）。まずは、A/Bテストで得られる評価結果を定義しましょう。以下の図のように、リクエスト i があったとき、ユーザ情報 x に基づき特定のAction a を返すPolicy π を考えます。このとき、Action a に対するユーザの行動はReward r として定義できます。リクエストをn回繰り返したときのふるまいをログデータD = { (x, a_i, r_i) }^n_i=1 として定義します。1つのレコードごとに、ユーザ情報 x 、Action a 、Reward r が記録されています。Policy π_e に対する A/B テストを実行したとみなし、その時のログデータ D_e から Policy π_e の価値 V^(^)_(A/B) を計算してみましょう。以下の式で算出できます。この式では、Reward r の総和平均を取っています。別の言葉で言い換えると、ユーザに期待する行動をしてもらえた割合になります。つぎに、OPEでA/Bテストを近似することを考えます。古いPolicy π_0 がすでにシステム上で動いており、ログデータ D_0 が得られるケースを考えます。ログデータ D_0 を活用することで、新しい Policy π_e の評価ができればよさそうです。そこで、ログデータ D_0 と新しい Policy π_e を入力して価値V^(^)を算出することを考えます。ここで目的とするのは、本来の価値を算出することですから、以下のような近似を目指します。近似の評価は、以下のように平均二乗誤差（MSE：Mean Square Error）で算出します。OPEを用いて新しいPolicy π_eの価値を算出するための関数 V^(^)(π_e; D_0) をOPE Estiamteと呼びます。OPE Estiamte は、MSEが可能な限り小さくなるように設計することがポイントです。OPEの利点として、ここまでで説明したオフライン評価が可能な点のほか、並列処理が可能な点が挙げられます。OPEでは、ログデータ D_0 さえあればどんどんPolicy π_n を入れ替えて評価することが可能です。すなわち、以下のような並列評価が可能です。OPEは必要な前提条件が比較的多いため、気を遣わなければいけないことがたくさんあります。ただし、必要な前提条件を満たしたうえで評価関数を適切に設計できれば、作業コストの削減や安全なPolicyの更新、高速な評価などを実現できるため、非常に有効な武器になります。OPEの既存手法OPEに求められる ①必要な前提条件、 ②OPE Estiamteの適切な設計、を満たすための代表的な手法を紹介します。本記事で紹介する手法は、RecSysのチュートリアルで用いられているModel-Base OPE、Model-Free OPE、Hybrid OPEと同様に分類しています。詳細は後ほど記載しますが、以下に簡単な概要を記載します。・Model-Base OPE：報酬推定モデルを用いて未観測領域の疑似的な観測（推定）を行い、Policyを評価・Model-Free OPE：観測領域のバイアスを解消することで、Policyを評価・Hybrid OPE：未観測領域の疑似的な観測および観測領域のバイアス解消を同時に行い、Policyを評価なお、前提条件としてPolicyはThompson Samplingのような条件付き確率（傾向スコア）を出力するものとします。Model-base OPEもともと考えていたことは、Counterfactualな領域（未観測領域）も含めて評価することで、Policy の本来の価値を算出することでした。Model-base OPEの代表的な手法であるDirect Methodは、ユーザ情報 x とAction a を入力することでReward r を出力する報酬推定モデル r^(^)(x,a) を学習します（学習自体は単純な回帰モデルでかまいません）。報酬推定モデル r^(^)(x,a) に、未知の (x,a) の組み合わせを入力することで、疑似的にCounterfactualな領域を推定しよう、というものです。レコメンドを例にして言い換えると、人間の判断を模したモデルをつくり、かわりにフィードバックしてもらおう、というアイデアになります。未観測（Counterfactual）領域を含む以下のようなログデータを考えます。(x,a)の組み合わせのうち、未観測である組み合わせを選択し、報酬推定モデル r^(^)(x,a) に入力します。この結果得られたReward r を未観測領域の値に格納します。これにより、疑似的に未観測領域の推定が可能となります。報酬推定モデル r^(^)(x,a) が人間の判断に近いほど高精度であることが前提となりますが、この前提を満たしていればじゅうぶんA/Bテストを近似化できるでしょう。すべての未観測領域の推定が完了し、最終的に以下のような結果が得られたとします。これをもとに、Policy π_eの価値を算出します。Direct Methodが算出する価値は以下の式で計算できます。各記号の意味は以下です。Model-Free OPEModel-base OPEで紹介したDirect Methodでは、未観測領域の推定に用いる報酬推定モデルの学習にログデータを用いています。しかし、このログデータは前述したようにバイアスがあります。したがって、報酬推定モデルにも何らかのバイアスが生じていることが危惧されます。この場合、報酬推定モデルの出力分布は真の分布と食い違ってしまいます。そこで、Model-Free OPEでは報酬推定モデルを使わず、観測されたデータ（ログデータ）だけを用いたPolicy評価を考えます。ただし、観測されたデータはバイアスが残っているため、これを解消できるような調整を行います。この代表的な手法が、Inverse Propensity Score (IPS) Estimatorです。IPS Estimatorでは、Policy πから得られる傾向スコア（Propensity Score）を用います。IPS Estimatorの名の通り、傾向スコア（Propensity Score）の逆数を取り（Inverse）、重み付けに用いる手法です。たとえば、Propensity Score=0.7 であればIPS = 1/0.7 として重みに用います。ここでいう重み付けの対象は、新しいPolicy π_e が出力する傾向スコア π_e(a|x) になります。この傾向スコアを、古いPolicy π_0 が出力する傾向スコア π_0(a|x) で割ります。これにより、全体のバイアスが解消できます。直感的にはなかなかイメージしづらいですが、傾向スコア（≒Actionが推薦される確率）で割ることは全体の比率を均等にする効果があります。以下に例を示します。IPS Estimator （V^(^)_IPS）の具体的な式を以下に示します。各試行ごとにPolicy π_e が出力する傾向スコアπ_e(a|x)をπ_0(a|x)で割り、Reward r に掛けたうえで総和平均をとっています。ここで計算に用いるのは、Reward r が観測された領域だけ（未観測領域の傾向スコアは用いない）である点にご注意ください。IPSを用いた重みづけ例を実際に確認してみましょう。赤で塗りつぶされた部分は、Reward r が観測された領域を示しています。この部分のみを用い、V^(^)_IPSを算出します。上段は結果が芳しくないPolicy π_ea を評価したときの結果を、下段は改善のため更新したPolicy π_eb を評価したときの結果を表しています。観測されたrewardはログデータ D_0 から取得しています。まず、上段を確認してみましょう。新しいPolicy π_ea が出力する傾向スコア π_ea(a|x) は、以下のようになっています。xnとanはそれぞれユーザ情報 x とAction a を表しています。・x1-a1（Reward r = 0）：0.6・x2-a2（Reward r = 1）：0.2・x3-a3（Reward r = 1）：0.6x1-a1 の組み合わせにおける観測されたReward r は0です。言い換えると、ユーザ情報 x1 の時にPolicy π_ea はa1 に対し相対的に高い傾向スコアを出力しています。結果として、Actionとして a1 の実行を決断したにもかかわらず、得られたReward r は0（効果がなかった）ということです。そこで、 x1-a1 の組み合わせではActionとして a1 が選択されないよう、傾向スコアを低くする必要があります。逆に、観測されたReward r が1であるような組み合わせでは、その組み合わせに用いられたActionが実行されるように、傾向スコアを高くする必要があります。しかし、上段のPolicy π_ea ではむしろ逆の傾向でした。そこで、上段のPolicy π_ea は採用せず、別のPolicy π_eb に置き換えてみます。これにより、傾向スコアは以下のように変化しました。・x1-Act1（Reward r = 0）：0.3・x2-Act2（Reward r = 1）：0.7・x3-Act3（Reward r = 1）：0.8Reward r が 0 になる組み合わせでは傾向スコアが低く、Reward r が 1 になる組み合わせでは傾向スコアが高くなっています。これをもとに、上段と下段の V^(^)_IPS を算出した結果、Policy π_ea では0.42、Policy π_ea では0.92となり、期待通りとなったことがわかります。ここまでがIPSの基本的な考え方です。数式にすると難しそうですが、上記の計算はExcelで簡単に試すことができます。実際に計算してみると、ぐっと理解が進むため、ぜひ手計算してみることをお勧めします。IPS Estimatorの派生として、重み上限を設定する（傾向スコアが非常に小さいときに重みが大きくなりすぎないようにする）Clipped IPS（CIPS）Estimatorや、重みを正規化するSelf-Normalized IPS（SNIPS）Estimatorも存在します。こちらについては、冒頭に記載した勉強会資料で具体的に説明をしていますので、気になった方は是非ご覧ください。Hybrid OPEModel-base OPEとModel-free OPEは、以下のような違いがあります。・Model-base OPE（Direct Method系）：バイアス×、網羅性○・Model-free OPE (IPS系)：バイアス○、網羅性×Model-base OPE で用いている Direct Methodは、学習済みの報酬推定モデル r^(^)(x,a) を通して未観測領域を疑似的に推定する手法です。観測範囲が広がるため網羅性があるものの、報酬推定モデルの学習に用いるデータはバイアスがある可能性の高いログデータです。したがって、報酬推定モデルにも何らかのバイアスが生じている懸念があります。Model-free OPEは、バイアスの懸念がある報酬推定モデルを用いず、観測されたデータ（ログデータ）だけを用いたPolicy評価を行います。このとき、ログデータ自身のバイアスが解消されるようにIPSを用いた逆重み付けを行います。しかし、Direct Methodのように学習済みの観測用モデルを持ちるわけではないため、未観測領域の推定はできていません。この２つの違いを整理すると、以下の図のようになります。この課題を解決するため、Direct MethodとIPSを統合したハイブリッド手法Hybrid OPEとしてDoubly Robust Policy Estimatorが提案されています。以下に式を示します。非常に単純で、Direct MethodとIPSの式を組み合わせたものになります。ポイントは右辺の第二項です。この項ではIPSを用いて重み付けを行っています。もともとのIPSの式では、Reward r に対して重み付けを行っていましたが、この式では r-r^(^)(x,a) に対して重み付けを行っています。r^(^)(x,a) は Direct Methodで用いていた報酬推定モデル r^(^)(x,a) です。報酬推定モデル r^(^)(x,a) の出力とログデータ D_0 に記録されたReward r が同一のときは、 r-r^(^)(x,a) = 0 となり第二項は無効となることを表しています。言い換えると、IPSを使わないということです。Direct Methodは、報酬推定モデル r^(^)(x,a) がReward r を正しく推定できていることを前提としています。r-r^(^)(x,a) = 0 であれば、報酬推定モデル r^(^)(x,a) はReward r を正しく推定できているため、IPSを使わなくてもよい、という考えに基づいた設計になっています。逆に、ログデータに記録されたReward rとDirect Methodの観測用モデルの出力一致していないときは、IPSで補正するようになっています。この補正にあたっては、Clipped IPS EstimatorやSelf-Normalized IPS Estimator と同様に、重みが想定外の数字にならないような工夫を行ったDoubly Robust with Optimistic Shrinkage (DRos)  Estimatorも存在します。上記の内容をもとに手法をまとめると以下のようになります。それぞれの特性を理解して活用するとよいでしょう。CFMLを支える技術（オープンデータとツール）手法を学んだあとは、実際に動かして試してみましょう。ここではそのためのツールとデータセットを紹介します。いずれもZOZOが公開しているものであり、非常に使いやすいものとなっています。詳細はZOZOの紹介記事にお譲りし、ここでは簡単な紹介に留めさせていただきます。Off-Policy Evaluationの基礎とZOZOTOWN大規模公開実データおよびパッケージ紹介 – ZOZO TECH BLOGOpen Bandit DatasetZOZOが提供しているファッション推薦データです。ファッションのECサイトであるZOZO TOWN上で実際にレコメンドを行い、ユーザから得られたフィードバックを記録したデータセットになります。実世界上で収集したデータセットとして希少価値を持ちます。Open Bandit PipelineOPEの実験を容易かつ統一された設定で行うための基盤です。バンディットアルゴリズムを代表とするPolicyの実装や、OPEを用いたオフライン評価を可能としています。今回紹介したアルゴリズムをはじめ、代表的な手法がカバーされています。Open Bandit PipelineでOPEを用いたオフライン評価を行うときに求められるのはログデータです。そのため、新しいアルゴリズムを自分で実装したときも、出力ログをOpen Bandit Pipelineのフォーマットに合わせることでOPEを用いたオフライン評価を容易に実行できます。Open Bandit PipelineはGoogle Colaboratoryで簡単に試すことができるようにサンプルが整備されており、すぐに試行できることもポイントの一つです。おわりに本記事では、次世代の意思決定技術として注目されている反実仮想機械学習（Counterfactual Machine Learning：CFML）を紹介しました。先駆者が公開してくださったサーベイ・チュートリアル資料をもとに、日本語で改めて体系的に整理することで初学者の理解の手助けとすることをねらいとしました。サーベイ・チュートリアル資料を作成くださった多くの方に御礼を申し上げます。本記事を通してCFMLに対する知見が深まれば、筆者としては喜ばしい限りです。本記事では触れていませんが、冒頭に記載した勉強会資料では、バンディットアルゴリズムの基礎やCFMLをさらに発展させたCounterfactual XAIにも触れています。興味がありましたら、ぜひこちらもご覧ください。さいごに、ARISE analyticsではKDDIが保有する国内最大規模のユーザデータを対象とした分析に取り組んでいます。今回紹介した内容や、その他記事で触れている弊社の取り組みに興味がございましたらぜひお声がけください。採用 | 株式会社ARISE analytics（アライズ アナリティクス）参考文献執筆にあたり、以下の文献を参考にさせていただきました。本記事でCFMLの基本的な考え方をつかんだ後は、本リストの文献を読むことでさらに理解が深まると思います。ぜひご活用ください。特に参考にさせていただいた資料（この場を借りてお礼を申し上げます）バンディットと因果推論私のブックマーク「反実仮想機械学習」（Counterfactual Machine Learning, CFML）RecSys2021 Tutorial Counterfactual Learning and Evaluation for Recommender Systems: Foundations, Implementations, and Recent AdvancesKDD 2021 Counterfactual Explanations in Explainable AI: A TutorialKDD 2020 Tutorial on Causal Inference Meets Machine Learning日本語の解説資料CounterFactual Machine Learningの概要（反実仮想機械学習）YouTubeの推薦アルゴリズムの変遷を追う〜深層学習から強化学習まで〜DiCE: 反実仮想サンプルによる機械学習モデルの解釈/説明手法バンディット問題の理論とアルゴリズムチュートリアル・講義資料SIGIR 2016 Tutorial on Counterfactual Evaluation and Learning for Search, Recommendation and Ad PlacementICML 2017 Tutorial on Real World Interactive LearningCS7792 – Counterfactual Machine LearningOFF POLICY EVALUATION AND LEARNING FOR INTERACTIVE SYSTEM論文Counterfactual Visual Explanations（ICML 2019）Doubly Robust Policy Evaluation and Optimization（Statistical Science 2014）CoCoX: Generating Conceptual and Counterfactual Explanations via Fault-Lines（AAAI 2020）Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification（ICCL 2020）Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives（NeurIPS 2018）PRINCE: Provider-side Interpretability with Counterfactual Explanations in Recommender Systems（WSDM 2020）GNNExplainer: Generating Explanations for Graph Neural Networks（NeurIPS 2019）Off-Policy Evaluation Using Information Borrowing and Context-Based Switching（arXiv preprint 2021 ）A Generalization of Sampling Without Replacement from a Finite Universe（JASA 1952）Learning from Logged Implicit Exploration Data（arXiv preprint 2010）Doubly Robust Policy Evaluation and Learning（ICML 2011）The Self-Normalized Estimator for Counterfactual Learning（NeurIPS 2015）Doubly robust off-policy evaluation with shrinkage（PMLR 2020）その他資料awesome-offline-rlDoubly Robust Off-Policy Evaluation with ShrinkageArtwork personalization at Netflix日経広告研究所 有力企業の広告宣伝費 2020年版コード、ツールOpen Bandit Pipeline: a research framework for off-policy evaluation and learningDiverse Counterfactual Explanations (DiCE) for ML"
https://www.ariseanalytics.com/activities/report/20220527/,NeurIPS 2021 論文読み会,"ARISE Tech Blog2022.05.27TweetNeurIPS 2021 論文読み会こんにちは。Marketing Solution Division, Experience Design Unitの岩野です。今回は社内で行われたNeural Information Processing Systems (NeurIPS) 2021論文読み会のまとめをお届けいたします。NeurIPSは世界最大規模の機械学習の国際会議で例年12月に開催されています。今回の読み会の開催にあたっては、社内で論文読み会に参加したい人を募り、論文の選定にあたってはNeurIPS 2021 Award Recipientsの論文を中心に各々が気になる論文を読んで共有する形をとりました。今回はその中で5報ご紹介させていただきます。MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers紹介者：奥井恒■概要近年、BERT, GPT-3などテキスト生成モデルの進歩が話題になっています。モデルの性能向上はもちろん重要ですが、一方でモデルの性能を評価するための定量的な指標が必要になります。この論文では、自由形式のテキスト生成において、MAUVEという指標を提案しています。MAUVEはこれまでの評価指標と比較してデコードアルゴリズムの品質やモデルサイズの違いを考慮でき、かつ人間の評価に近いという実験結果も得られていることから、有効な指標になりそうです。【論文読み会】MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence FrontiersfromARISE analyticsAlias-Free Generative Adversarial Networks（StyleGAN3）紹介者：近藤真暉■概要動画像の連続的な変化に対しても自然な生成ができるようになったStyleGAN3を紹介します。本論文では、生成器の入力値を連続信号として扱うことで、平行移動・回転に対しても変化が生じないようにする工夫が取り入れられています。本技術による動画像生成の大きな発展が期待されます。​【論文読み会】Alias-Free Generative Adversarial Networks（StyleGAN3）fromARISE analyticsMoser Flow: Divergence-based Generative Modeling on Manifolds紹介者：伊藤光祐■概要実世界のデータには気候データなどのように、球体の表面といった複雑な図形上に分布するものが数多くあります。この論文ではそういったデータをモデリングできるように生成モデルの手法の一つであるContinuous Normalizing Flowを多様体上で適用できるように拡張した上で、さらに既存手法よりも計算効率が改善されたMoser Flowという手法を提案しています。【論文読み会】Moser Flow: Divergence-based Generative Modeling on ManifoldsfromARISE analyticsOn the Expressivity of Markov Reward紹介者：岩野夏樹■概要この論文では強化学習における報酬関数設計の問題としてマルコフ報酬関数の表現可能性を調べ、特定の種類のタスクではどのようなマルコフ報酬関数を設計しても一部のタスクを解くことができないことを示しました。また、与えられたタスクを解くことができる報酬関数があるかどうかを多項式時間で判定し、ある場合にはその報酬関数を構築するアルゴリズムを開発しました。この研究によって報酬関数の表現性の調査が進むことが期待されます。【論文読み会】On the Expressivity of Markov RewardfromARISE analyticsDeep Reinforcement Learning at the Edge of the Statistical Precipice紹介者：内田沙穂里■概要この論文では、強化学習における既存の評価指標の問題点と解決方法を提案しています。近年、ロジックが複雑になるにつれて複数回実行したうえでの評価が難しくなっている中、層別ブートストラップ法での信頼区間やInter Quantile Meanを利用した、実行回数を抑えつつ適正に評価する方法を提案しました。また、すでに報告されている評価数値の再検証を行い、既存評価の不確実性の証明も行っています。【論文読み会】Deep Reinforcement Learning at the Edge of the Statistical PrecipicefromARISE analyticsおわりに今回は受賞論文を中心に読んだこともあり、普段は触れることのない領域の論文を読んだ参加者も多かったようです。自分と直接関係ない領域の研究は新鮮な考え方が多く、取り入れられる部分も多くあると感じました。また、読み会は複数人で同時にキャッチアップするのでいろいろな興味のある分野を知れる上、発表者に質問を投げかけつつ全員で理解できていく感覚がなんとも楽しいです。今回取り上げた中でMoser Flowの論文の発表は特に興味深く、大学院でやっていた生成モデルの分野の勉強を再度始めようという気持ちになりました。強化学習の分野はまだまだ解析の余地があり、より複雑なタスクに適用可能になってきている一方で、複雑性が増して評価するのも難しくなってきていると感じました。より人間に近いタスクがこなせるようになるまでもう少し時間がかかりそうです。今後ICLRやICMLなどの国際会議についても論文読み会を実施する予定です。どんな技術や研究が出てくるのか今後も楽しみにしつつ、糧にしていき、全体としての技術力向上のため今後もキャッチアップできていければと思います。謝辞本記事の執筆にあたり以下のページを参考にさせていただきました第4回WWW2020論文読み会 開催 | CyberAgent Developers Blog"
https://www.ariseanalytics.com/activities/report/20220624/,ARISE Kaggle部活動記 #3,"OTHERS2022.06.24TweetARISE Kaggle部活動記 #3Kaggle部ARISE analytics 福嶋です。ARISE Kaggle部の活動記へようこそ。この部では、世界的に知名度の高いKaggleを中心に、国内外の分析コンペティションへ参加・上位ランクインを目指し、部員30名(2022年5月)で活動しています。今までに参加したコンペティションの結果は以下の通りです。M5 Forcasting – Accuracy@ Kaggle (398/5598 銅ランク)対戦ゲームデータ分析甲子園@ProbSpace (50・72/567銀・銅ランク2チーム)今回は、5月初旬までKaggleで行われていた、NBME – Score Clinical Patient Notesで銀メダル(71位/ 1471人)を獲得したので、取り組んだ分析内容・工夫した点について解説していきます。NBME – Score Clinical Patient Notes概要コンペの目標まず簡単に、今回のコンペの概要について説明します。NBME –Score Clinical Patient Notesは、米国医師免許試験で実際に行われる模擬診察において、受験者が記載した医療カルテ(テキスト)データの採点作業の効率化を目的としたコンペティションです。[1]コンペの参加者は、医療カルテと患者の症状が与えられ、医療カルテ内にある「症状と同じ意味合いを表す箇所」を特定することが目標になります。同じ症状、例えば「Weight-loss(体重減少)」という症状に対し、「loss weight」「clothes fit looser(服が緩くなった)」など受験者によって様々な記載パターンがあるため、これらを同じ意味と解釈できるモデルの作成が求められました。提供データセット次に利用できるデータセットについて説明します。今回利用可能なデータセットは、10名の患者のカルテ計約40,000件で、そのうち1000件(10名×100件)にはラベル付けがされています。ラベル付けは、各患者の症状とカルテ内の箇所がペアとなるように付与されています。今回のデータセットの大きな特徴は、ラベル付けされているデータがかなり少なく、その代わりにラベル付けされていないデータが非常に多くある、ということです。従って、何かしらの方法でラベル付けされていないデータにラベルを付け、学習データの増やす必要性があると考えられました。また、もう一つの特徴として、当たり前ではありますが出現単語が医学用語に偏っているということも挙げられます。したがって一般的な文章に適用されているBert系をそのまま利用しても精度が上がらないことが予想されました。これら二点の観点から、①ラベル付けされていないカルテにラベルを付けること、②医学系の単語に適用させる方法を検討すること、を主眼にコンペに挑みました。解法コンペの概要について理解できたところで、解法の解説に移りたいと思います。今回スコア算出の処理は大きく①テキスト前処理、②モデリング、③アンサンブルの3つに分かれていますので、それぞれ解説していきます！①テキスト前処理まず、モデリングする前に実施するカルテデータの正規化についてです。今回は、「文字の小文字化」のみを実施しています。最近のNLP系のモデル(BERT, RoBERTa, DeBERTa,etc)などでは、正規化を行わない方が、精度が良い場合[2]があるため、何も処理せずにトークン化することが多いです。しかし今回、カルテデータの一部に“文字がすべて大文字“になっているものが含まれていました。今回メインで使っていたDeBERTaは、サブワード(DeBERTaが持っている辞書に含まれるトークン)に分割の方法に、小文字・大文字で大きな差が生まれます。例えば“weight”と“WEIGHT”では“weight” ⇒ “weight”“WEIGHT” ⇒ “WE”, “IGHT”のように、大文字のWEIGHTが登録されていないため無駄に分割されてしまいます。分割されてしまうことがどれほどの影響を与えているのかは定かではないのですが、実際精度がわずかに上昇したため効果があったのではないかと推定されます。②モデリング次は、文字列を抽出するモデルの作成方法についてです。モデルは、公開Notebookを参考にしたトークンベースの二値分類(抽出範囲に各トークンが含まれる否か)をベースに行っています。加えてDiscussion*[3]内で有力視されていた、MLM(Masked Language Model)とPseudo Labelingを実施することが精度向上に大きく貢献しました。*コンペ参加者がKaggle内で情報共有するスペース今回の精度向上に大きく効いたと思われる、MLMとPseudo-Labelingについてのみ紹介します。（ア）MLM学習MLMは、文中の一部をマスクし、ほかの単語からマスクした単語を予測する事前学習方法の一つです。これにより、同じ意味合いの単語が同じような分散表現を獲得でき、精度向上に寄与することが見込まれます。DeBERTaの事前学習に利用されているWikipedia等[4]のデータセットと比較し、今回利用するカルテデータは前述の通り医学用語が多く出現しており、文構成・出現単語・単語のメインの意味が大きく異なることが想定されました。実際、MLM前後のembeddingを確認すると大きな変化があったことが確認できました。下記の図は、カルテ内に含まれていた単語のembeddingをMLM前後のdeberta-largeから抽出したものです。” smoking ”(煙、たばこ)付近の単語を確認すると、MLM前では、周辺に“burn”, “warm”など火や熱さに関する単語があったのに対し、MLM後では、”tobacco”や”cigarette”などのたばこの同義語や、”alcohol”,” drink”(飲酒)などの嗜好品の単語があるように変化していました。（イ）Pseudo-LabelingPseudo-Labelingは、Kaggle内でよく用いられる半教師あり学習方法の一つです。プロセスは、①ラベルありデータで学習したモデルをラベルなしデータに適用②スコアが高かったもの(例えば0.9以上)を正解として疑似ラベルを付け③元々ラベルがあったデータと合わせて再度学習、という流れで行います。概要のところでも述べましたが、今回は初期状態で学習できるデータセットが少なかったため、学習データを増やす必要がありました。ある程度の人数や時間がある案件であれば人力でのラベル付けも可能ですが、Kaggleではそこまで時間を割けないため、Kaggle上でも使われるPseudo-Labelingを実施することにしました。これにより、学習データが約40000件となり、安定した学習が行えるようになりました。③アンサンブル最後にアンサンブルです。アンサンブルは、Kaggle等コンペ特有?の複数モデルをよい塩梅で足し合わせることで精度を向上させる方法です。今回は、deberta-largeとdeberta-v3-largeの二つのモデルを0.5ずつ重みづけし足し合わせています。(いろいろ最適化したのですが、単純な平均より精度が上がらなかったです…)最終的には、アンサンブル後のスコアを用いF1スコアが最大化される閾値を算出、閾値を超えたトークンを答えとして抽出しています。精度最終的なスコアも確認しておきます。注目点は、MLM・Pseudo-Labelどちらか未実施の場合、銅メダル圏内にも入れなかったというところです。この二つの処理がいかに重要だったかということが分かります。しかしながら、金メダル圏にはまだまだ差があります。上位の解法と比べてもそこまで大きな差はなかったように思えたのですが、細かな処理の部分を突き詰めていく必要がありそうです。考察今回は、MLM・Pseudo-Labeling等、ラベルなしデータをどこまで活用できるかが実際に勝敗を分けたコンペでした。直近で行われていた同じくKaggleのNLP系コンペである、Feedback Prize – Evaluating Student Writing[5]ではMLMは効果がなかったので(時間とリソースを背景に)一瞬実施をためらいましたが、今回は文構成や単語も一般的な文章とは大きく異なってそうだったため、MLMを実施したのはよい判断だったと思います。また、NLP系のモデルの主流がRoBERTaからDeBERTaに移ってきたと感じたコンペでもありました。直前に行われていたFeedback Prize – Evaluating Student Writingと今回のコンペで上位層のモデルがほとんどDeBERTaになったことはDeBERTaの強さを物語っています。ただ、具体的に何が要因で精度の差が生まれているのかを理解はできていない・そもそも理解できるのかわからないため、今後深堀を行っていきたいと考えています。終わりに今回は銀メダルを獲得したNBME –Score Clinical Patient Notesの解法について紹介いたしました。今回のようなラベルのないデータが大量にあることは業務でも頻繁に起こりえます。今回のコンペでは、そういった状況でどのように精度を上げればよいのかを考え知ることができた良いコンペだったと感じています。Kaggle等のデータ分析コンペに参加することで、最新の技術から業務に活用できるTipsまで、様々な知見を得ることができるため、引き続き挑戦していければと考えています。ARISE analyticsでは、kaggle等の分析コンペティションで上位成績を残すと、インセンティブとして報奨がもらえるARISE Tech Master制度があるので引き続き金メダルを目指し、努力していきたいです。分析はチームワークが大事です。役割を決めお互いの強みを持ち合うことで、一人で行うより何倍も効率的に分析課題に取り組めます。我々と一緒にKaggle部を盛り上げてくださる方はこちらのページからご連絡お待ちしております!!!!"
https://www.ariseanalytics.com/activities/report/20220704/,【Go言語入門】「Go」の概要や特徴とは？社内のAPI開発プロジェクトで採用して分かったこと,"ARISE Tech Blog2022.07.04Tweet【Go言語入門】「Go」の概要や特徴とは？社内のAPI開発プロジェクトで採用して分かったことはじめまして。2022年4月に中途入社し、Analytics Delivery Divisionでフロントエンド/バックエンドチームに所属しているエンジニアの井上です。前職からWeb開発に従事しており、ARISEに入社してからはWeb/モバイルアプリ開発チームにジョインして、Go言語（以下Go）を使ったAPI開発を担当しています。今回はGoの概要や特徴、またARISEが開発言語にGoを採用した理由を知っていただきたいと思い、記事を作成しました。Go言語の概要GoはGoogleが作成したプログラミング言語で、静的言語なコンパイラ言語です。Robert Griesemer, Ken Thompson , Rob Pikeらによって2009年に公開されました。Goを使ったプロダクトにはDockerやTerraformなどがあります。Goが登場するまでに、ソフトウェアはJavaやC++などの1990年代に登場した言語によって開発されていました。1990年代からコンピューターは進化してきた一方で、プログラミングする行為自体はあまり進化してこなかったことにGoの開発者たちは課題を感じていました。例えばサーバーのソフトウェアを開発する言語を使用するために複雑な準備をする必要があることに不満を感じていたようです。そのため開発者の生産性を高くして、スケーラブルにしたいというコンセプトでGoを作成しました。Goのメリットは大きく2つあります。1つ目は構文が最小限のためシンプルな設計であることです。Goは他の言語によくあるwhileやforeachなどの構文が実装されていません。このシンプルさが開発者にとってラーニングコストが低い言語になっています。2つ目はGoは同じ書き方がされやすいことです。Goはgofmtという自動でソースコードを整形してくれる公式の機能があります。そのため開発者間で記述の差が少なく、読みやすく、メンテナンスしやすくなります。一方でGoのデメリットはシンプルな設計のためコードが長くなりやすいこと、クラスがないためクラスベースのオブジェクト指向の記述ができないといったことがあります。他言語との比較Web開発で使われるPHPとJavaと比較していきます。PHPは動的型のオブジェクト指向のインタプリタ言語です。動的に生成されるWebページを開発者が速やかに作成するというコンセプトで作成されています。世界の40%以上のサイトがWordpressというPHPを用いて開発されたサービスを使ってといわれるほど普及しています。PHPは動的型言語のため変数や引数に型の指定がなく、比較的簡単にプログラムが作成できます。一方で型のミスマッチにより実行時にエラーが発生する可能性もあります。また注意しないとXSSなどが起きる可能性があります。Javaは静的型のオブジェクト指向のコンパイラ言語です。JVMという仮想マシンがあるところではどこでも実行できるという特徴があります。金融システムやEvernoteがJavaを用いて作られています。Javaはクラスベースのオブジェクト指向を確立した言語で、広く普及していて使用者も多いことから大規模開発に向きます。またセキュリティが言語レベルで高いことがメリットですが、実行時に仮想マシンを動かす必要があるためメモリ消費が大きいというデメリットがあります。実装してみた。それではバブルソートを例に各言語を比較してみます。1から1000までの数をシャッフルして、バブルソートで並び替えをするプログラムをGo, PHP, Javaで実装してみました。まずはソースコードから。■Gopackagemainimport(""fmt""""math/rand""""time"")funcmain(){vararray[]intfori:=1;i<=1000;i++{array=append(array,i)}array=shuffle(array)start:=time.Now()bubbleSort(array)fmt.Println(time.Since(start).Microseconds())}funcshuffle(array[]int)[]int{rand.Seed(time.Now().UnixNano())rand.Shuffle(len(array),func(i,jint){array[i],array[j]=array[j],array[i]})returnarray}funcbubbleSort(array[]int){fori:=0;i<len(array);i++{forj:=1;j<len(array);j++{ifarray[j-1]>array[j]{tmp:=array[j-1]array[j-1]=array[j]array[j]=tmp}}}}■PHP<?php
function main () {
    $array = range(1,1000);
    shuffle($array);

    $start = microtime(true);
    bubbleSort($array);
    $end = microtime(true);
    echo ($end - $start) * 1000000;
}

function bubbleSort($array) {

    for($i = 0; $i < count($array); $i++) {
        for ($j = 1; $j < count($array); $j++) {
            if ($array[$j - 1] > $array[$j]) {
                $tmp = $array[$j - 1];
                $array[$j - 1] = $array[$j];
                $array[$j] = $tmp;
            }
        }
    }
}

main();
?>■Javaimport java.util.*;public class Main {final static int length = 1000;static int[] array = java.util.stream.IntStream.rangeClosed(1, length).toArray();public static void main(String[] args) throws Exception {int[] arrayShuffled = shuffle(array);long startTime = System.nanoTime();bubbleSort(arrayShuffled);long endTime = System.nanoTime();System.out.println((endTime - startTime) / 1000);}static int[] shuffle(int[] array) {for (int i = array.length - 1; i > 0; i--) {int r = (int) (Math.random() * (i + 1));int tmp = array[i];array[i] = array[r];array[r] = tmp;}return array;}static void bubbleSort(int[] array) {for (int i = 0; i < array.length; i++) {for (int j = 1; j < array.length; j++) {if (array[j - 1] > array[j]) {int tmp = array[j - 1];array[j - 1] = array[j];array[j] = tmp;}}}}}書き心地動的型のPHPは型を指定することなく変数定義とメソッドを作成できるため、比較的簡単にコードを書けると感じました。一方で静的型のJavaは変数定義もメソッドを作成も型を記述する必要があり、冗長に感じました。さて、静的型付け言語のGoの変数定義はメソッド内では “変数名 := 値“ と書くことで型推論をしてくれます。例えば数値型の0を値として持つ変数i, jを以下の2つの書き方で定義できます。variint=0j:=0書き心地が動的言語のようにできます。ちょっとしたことですが記述が簡単だと感じました。処理時間比較それでは気になる処理時間を見ていきましょう。単位はマイクロ秒です。インタプリタ言語のPHPはGoと比較して桁が一つ多くなっていました。そしてGoは同じコンパイラ言語のJavaよりも4倍程度速いですね。速度面でもGoは素晴らしい言語ですね。Go言語をARISEのプロジェクトで採用した理由我々のプロジェクトは長期間の開発が見込まれて、ある程度の開発者入れ替わりが考えられ、また参画するメンバー間の技術スタックが揃いにくい状況にあります。その条件下でGoはシンプルで理解しやすいこと、開発者間での表記ブレが起こりにくいこと、さらにシンプルで可読性が高いことが決め手となり採用されました。実際にプロジェクトに採用してみての良し悪し実際にGoで開発していて感じた良かった点は「エラー制御の可読性が高いこと」だと思います。他の言語ではtry – catch – finallyの形式のエラーハンドリングを使いますが、コードが複雑化して可読性が損なわれる可能性もあります。一方でGoはエラーを変数に持たせることができるためメソッドからエラーを返して制御することができます。例えば割り算をして結果を表示するプログラムを考えてみます。packagemainimport(""errors""""fmt"")funcdivide(a,bint)error{ifb==0{returnerrors.New(""0で割り算できません"")}fmt.Println(a/b)returnnil}funcmain(){err:=divide(4,2)iferr!=nil{fmt.Println(err)}err=divide(2,0)iferr!=nil{fmt.Println(err)}}0で割り算をすることはできないので、devideメソッドの第二引数に0が入った場合はエラーを返すようにしました。このコードを実行すると17行目で呼ばれるdevideは2を出力し、21行目呼ばれるdevideはエラーを返します。このようにエラー処置がとても分かりやすくて簡単に書けることが良い点ですね。逆に他の言語に比べて便利なライブラリが少ないことがデメリットだと感じました。JavaやPHPにはオープンソースの便利なライブラリが多数ありますが、それに比べるとGoは歴史が浅いせいかそのようなライブラリが少ないように思います。これから成熟していく発展途上な言語なのかもしれません。まとめ今回はGoの概要をJava, PHPと比較して紹介しました。この記事を読んで少しでもGoに興味を持っていただけると幸いです。次はGoの特徴の一つである並行処理についての紹介記事を書いていきます。お楽しみに！"
https://www.ariseanalytics.com/activities/report/20220729/,【数理統計】不偏推定量の中でも望ましい一様最小分散不偏推定量(UMVUE)とは？,"ARISE Tech Blog2022.07.29Tweet【数理統計】不偏推定量の中でも望ましい一様最小分散不偏推定量(UMVUE)とは？こんにちは。2021年4月に新卒入社したMaketing Solution Division (MSD) 所属の内藤と申します。「数理統計の奥深さにふれる」というテーマのもと、これから様々なトピックで発信していきたいと考えています。今回は点推定についてお話します*1。点推定の中でも今回は不偏推定について解説します。点推定の目的まず点推定の目的をお話します。結論から言えば、点推定の目的は「リスク関数を最小化するような推定量を求めること」です。点推定の目的を理解するため、定式化しつつ説明していきます。母集団から抽出された標本を特徴づけるものを\( X_1,\ldots,X_n \)とします。ここで\( X_1,\ldots,X_n \)は標本抽出にともなう確率変数です。\( X_1,\ldots,X_n \)はいわゆる生データでありそのままの形で活用することは少なく、平均値や中央値、分散を求めるなど何らかの処理をほどこして活用します。つまり\( X_1,\ldots,X_n \)の関数に着目します。\( X=(X_1,\ldots,X_n) \)の関数\( T(X) \)を\( X \)の統計量といいます。例えば標本平均\( \bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i \)は統計量の一つです。点推定とは未知のパラメータ\( \theta \)を統計量\( T(X) \)によって可能な限り正確に求めようとすることです。ここでのパラメータとは母集団における平均や分散などです。点推定において\( \theta \)を推定するときに用いる統計量\( T(X) \)を\( \theta \)の推定量といいます。ここで\( T(X) \)と\( \theta \)の近さを計るために損失関数\( l(\theta, T(X)) \)がよく使われます。損失関数\( l(\theta, T(X)) \)とは、\( T({X})=\theta \)のとき\( l(\theta, T({X}))=0 \)となるような非負の関数です。\( T(X) \)は確率変数であるため*2、\( T(X) \)が平均的に\( \theta \)に近くなるように推定することが”良い”推定であると考えられます。つまり\( R\left(\theta, T({X}) \right)=E\left[l(\theta, T({X}))\right] \)の値を小さくするような推定量が”良い”推定量と考えられます。ここで\( E\left[\cdot\right] \)は期待値を表す記号です。\( R\left(\theta, T({X}) \right) \)はリスク関数とよばれています。平均二乗誤差と不偏推定量次に点推定でよく用いられるリスク関数の平均二乗誤差を説明します。さらに平均二乗誤差と関連のある不偏推定量について見ていきます。平均二乗誤差とは損失関数\( l(\theta, T(X)) \)を二乗誤差\( \left\{ T({X})-\theta \right\}^2 \)とするリスク関数\( {\rm MSE}\left(T({X}), \theta \right)=E\left[\left\{ T({X})-\theta \right\}^2\right] \)です。ここで仮にすべての\( T(X) \)に対して\( \forall \theta, {\rm MSE}\left(T^{*}(X), \theta \right) \leq {\rm MSE}\left(T(X), \theta \right) \)が成立する\( T^{*}(X) \)が存在する場合、\( {\rm MSE}\left(T^{*}(X), \theta \right) =0 \)となります。これは\( T^{*}(X) \)が常に\( \theta \)の真値であることと同義であり、\( \theta \)が未知であることに矛盾します。よって一般に\( \theta \)が未知である場合\( {\rm MSE}\left(T({X}), \theta \right) \)を一様に最小にする推定量は存在しません。数理統計では伝統的に考える推定量のクラスを制限し、制限されたクラスの中で\( {\rm MSE}\left(T({X}), \theta \right) \)を最小にする推定量を考えます。ここでのクラスとはある条件を満たす集まりという意味です。推定量のクラスとして、今回は不偏推定量のクラスを考えます。不偏推定量とは推定量の期待値がパラメータの値に一致するような推定量です。すべての\( \theta \)に対して\( E\left[T({X})\right]=\theta \)が成り立つとき、\( T(X) \)は\( \theta \)の不偏推定量とよばれます。ここで推定量\( T(X) \)のバイアス\( b(T({X})) \)を\( b(T({X}))=E\left[ T({X}) \right] – \theta \)と定義します。\( T(X) \)が不偏推定量であるとき\( b(T({X}))=0 \)です。ここで平均二乗誤差\( {\rm MSE}\left(T({X}), \theta \right) \)は次のように分散とバイアスの二乗和に分解できます。\begin{equation*}\begin{split}{\rm MSE}\left(T({X}),\theta \right)&=E\left[\left\{ T({X})-\theta \right\}^2\right]\\&=E\left[\left\{ \left( T({X}) -E[T({X})] \right)+\left( E[T({X})] -\theta \right) \right\}^2\right]\\&=E\left[\left\{T(X)-E[T({X})]\right\}^2\right] + \{E[T({X})]-\theta\}^2\\&=V[T(X)]+b(T(X))^2 \qquad (1)\end{split}\end{equation*}そこで推定量がバイアスをもつとき、推定量を修正して\( b(T({X}))=0 \)に近づければ平均二乗誤差が小さくなると期待できます*3。一様最小分散不偏推定量(UMVUE)いよいよこの記事のメインテーマである一様最小分散不偏推定量について説明していきます。統計量\( T(X) \)が不偏推定量であれば\( b(T({X}))=0 \)なので、(1)式より\( {\rm MSE}\left(T({X}), \theta \right)=V[T(X)] \)となります。つまり不偏推定量に限れば分散が最小である推定量が望ましい推定量であることがわかります。このような特徴をもつ不偏推定量が一様最小分散不偏推定量です。正確に定義すると、すべての\( \theta \)に対して(i)\( E\left[T^{*}({X})\right]=\theta \)であり、(ii)他のどの不偏推定量\( T(X) \)に対しても\( V[T^{*}(X)] \leq V[T(X)] \)が成立する場合、\( T^{*}(X) \)を一様最小分散不偏推定量(uniformly minimum variance unbiased estimator, UMVUE)といいます。UMVUEが存在する場合、推定量としてUMVUEを用いることはそれなりに理にかなっています。与えられた不偏推定量がUMVUEであることを示すには、フィッシャー情報量に基づくクラメール・ラオの不等式を用いる方法があります*4。まずはフィッシャー情報量について説明します。\( X=(X_1,\ldots,X_n) \)の同時密度関数を\( f_n(x \mid \theta) \)とします*5。ここでは説明を簡潔にするため\( \theta \)は1次元とします。このときフィッシャー情報量は\( I_{n}(\theta)=E\left[\left\{ \frac{\partial}{\partial\theta} {\rm log}f_{n}(X \mid \theta) \right\}^2\right] \)と定義されます。ここで\( S_{n}(\theta, x)=\frac{\partial}{\partial\theta} {\rm log}f_{n}(x \mid \theta) \)とおけば\( I_{n}(\theta)=E\left[\left\{ S_{n}(\theta, X) \right\}^2\right] \)と表されます。\( S_{n}(\theta, x) \)はスコア関数とよばれています。フィッシャー情報量\( I_{n}(\theta) \)と不偏推定量\( T^{*}(X) \)の分散に関して、いくつかの正則条件*6のもとで次の不等式が成立します。\begin{equation*}V[T^{*}(X)] \geq 1/I_{n}(\theta)　\qquad (2)\end{equation*}この不等式はクラメール・ラオの不等式とよばれています。(2)式から明らかに不等式が成立するにはフィッシャー情報量が正であることが必要です。クラメール・ラオの不等式を証明してみましょう。いま\( T^{*}(X) \)は不偏推定量なので\( \theta=E\left[T^{*}(X)\right]=\int T^{*}(x)f_{n}(x \mid \theta)dx \)が成立します。この等式の両辺を\( \theta \)で偏微分すると、微分と積分の交換が保証されているという仮定のもとで\begin{equation*}\begin{split}1&=\frac{\partial}{\partial \theta} \int T^{*}(x)f_{n}(x \mid \theta)dx \\&=\int T^{*}(x)f_{n}(x \mid \theta) \frac{1}{f_{n}(x \mid \theta)} \frac{\partial}{\partial \theta}f_{n}(x \mid \theta) dx \\&=\int T^{*}(x)\frac{\partial {\rm log} f_{n}(x \mid \theta)}{\partial \theta} f_{n}(x \mid \theta) dx \\&=\int T^{*}(x)S_{n}(\theta, x) f_{n}(x \mid \theta) dx \\&=E\left[S_{n}(\theta, X)T^{*}(X) \right] \qquad (3)\end{split}\end{equation*}となることがわかります。また\( f_{n}(x \mid \theta) \)は確率密度関数なので\( \int f_{n}(x \mid \theta)dx=1 \)であり、この両辺を\( \theta \)で偏微分すると、同じく微分と積分の交換が保証されているという仮定のもとで\begin{equation*}\begin{split}\frac{\partial}{\partial \theta} \int f_{n}(x \mid \theta)dx&= \int \frac{\partial f_{n}(x \mid \theta)}{\partial \theta}dx = 0 \qquad (4)\end{split}\end{equation*}が成り立ちます。(4)式より\begin{equation*}\begin{split}E \left[ S_{n}(\theta, X) \right]&= \int \frac{\partial {\rm log}f_{n}(x \mid \theta)}{\partial \theta}f_{n}(x \mid \theta)dx \\&= \int \frac{\partial f_{n}(x \mid \theta)}{\partial \theta}dx = 0\qquad (5)\end{split}\end{equation*}が成り立ちます。この記法によるとフィッシャー情報量は\begin{equation*}\begin{split}I_{n}(\theta)&=E\left[ \{ S_{n}(\theta, X) \}^2 \right] \\&=\int S_{n}(\theta, x)^2 f_{n}(x \mid \theta)dx \\&=\int \{S_{n}(\theta, x) – E\left[ S_{n}(\theta, x) \right] \}^2 f_{n}(x \mid \theta)dx \\&=V[S_{n}(\theta, X)]\qquad (6)\end{split}\end{equation*}と書くことができます。(5)式より(3)式の右辺は\begin{equation*}\begin{split}E\left[S_{n}(\theta, X)T^{*}(X) \right]&=E\left[S_{n}(\theta, X)T^{*}(X) \right]- E\left[\theta S_{n}(\theta, X) \right] \\&=E\left[\left( T^{*}(X) -\theta \right) S_{n}(\theta, X) \right] \\&=E\left[\left( T^{*}(X) -E\left[T^{*}(X)\right] \right) \left( S_{n}(\theta, X) – E\left[S_{n}(\theta, X) \right] \right) \right]\\&=Cov(T^{*}(X), S_{n}(\theta, X))\qquad (7)\end{split}\end{equation*}と表すことができます。ここで相関係数の絶対値は1を超えないため(3)式、(6)式、(7)式より\begin{equation*}\begin{split}1&=Cov(T^{*}(X), S_{n}(\theta, X))^2 \\&\leq V[T^{*}(X)]V[S_{n}(\theta, X)] \\&=V[T^{*}(X)]I_{n}(\theta)\end{split}\end{equation*}となり、この両辺を\( I_{n}(\theta) \)で割ることによりクラメール・ラオの不等式が得られます。クラメール・ラオの不等式より、不偏推定量\( T^{*}(X) \)がすべての\( \theta \)に対して\( V[T^{*}(X)]=1/I_{n}(\theta) \)であるとき\( T^{*}(X) \)はUMVUEであるといえます。最後にUMVUEの具体例として、正規分布の母平均\( \mu \)の推定に関して標本平均\( \bar{X} \)がUMVUEであることを示します。そのためにまず\( X_1,\ldots,X_n \)が独立同一分布に従うとき\begin{equation*}I_{n}(\theta)=nI_{1}(\theta) \qquad (8)\end{equation*}が成り立つことを示します。\( X_1,\ldots,X_n \)が互いに独立に同一分布に従うため同時確率密度関数は\begin{equation*}\begin{split}f_n(x \mid \theta)&= \prod_{i=1}^{n}f_{1}(x_i \mid \theta)\qquad (9)\end{split}\end{equation*}と表されます。(9)式の両辺の対数をとり\( \theta \)で微分すれば\begin{equation*}\begin{split}S_n(\theta, X)&= \sum_{i=1}^{n}S_{1}(\theta, X_{i})\qquad (10)\end{split}\end{equation*}を得ることができます。ここで(6)式に(10)式を代入すれば\begin{equation*}\begin{split}I_{n}(\theta)&= V\left[ \sum_{i=1}^{n}S_{1}(\theta, X_{i}) \right]\end{split}\end{equation*}となります。ここで\( S_{1}(\theta, X_{i}), i=1,\cdots, n \)も独立同一分布に従うため、\( I_{n}(\theta)=nV[S_1(\theta, X_1)]=nI_{1}(\theta) \)となり(8)式が成り立つことがわかります。ここから実際に正規分布の母平均\( \mu \)の推定に関して標本平均\( \bar{X} \)がUMVUEであることを示していきます。まず\( \mu \)に関するスコア関数は\begin{equation*}\begin{split}S_{1}(\mu, x)&=\frac{\partial}{\partial\mu} {\rm log}f_{1}(x \mid \mu) \\&=\frac{\partial}{\partial \mu}{\rm log}\left[ \frac{1}{\sqrt{2 \pi \sigma^2}} {\rm exp}\left\{ -\frac{(x-\mu)^2}{2\sigma^2} \right\} \right] \\&=\frac{\partial}{\partial \mu} \left\{ -\frac{(x-\mu)^2}{2\sigma^2}-\frac{1}{2}{\rm log}(2 \pi \sigma^2) \right\} \\&=\frac{(x-\mu)}{\sigma^2}\end{split}\end{equation*}と表されます。従ってフィッシャー情報量は\begin{equation*}\begin{split}I_{1}(\mu)&= E\left[\left\{ \frac{(X-\mu)}{\sigma^2} \right\}^2 \right] \\&= \frac{E[(X-\mu)^2]}{\sigma^4} = \frac{1}{\sigma^2}\end{split}\end{equation*}となります。よって(8)式より\begin{equation*}\begin{split}\frac{1}{I_{n}(\mu)} = \frac{1}{nI_{1}(\mu)} = \frac{\sigma^2}{n} = V[\bar{X}]\end{split}\end{equation*}となり、\( \bar{X} \)がUMVUEであることが示されました。おわりに今回は一様最小分散不偏推定量(UMVUE)の理論について説明しました。標本平均の例からも分かるようにUMVUEの理論は、直観的に”良い”と思われる推定量の合理性を確かめられる有効な理論です。しかしながら常にUMVUEが存在するとは限らず、その場合は最尤推定などによって他の推定量を構成することを視野に入れる必要があります。この記事を読んで少しでも数理統計の奥深さを感じていただけると幸いです。最後まで読んでいただき、ありがとうございました。参考文献竹村彰通(1991). 『現代数理統計学』. 創文社.久保川達也(2017). 『現代数理統計学の基礎』. 共立出版.野田一雄・宮岡悦良(1992). 『数理統計学の基礎』. 共立出版.※特に今回の記事の論理構成の大部分は『現代数理統計学』(創文社)を参考にさせていただいております。注釈*1：数理統計において推定は点推定と区間推定に大別されます。*2：\( X_1,\ldots,X_n \)が確率変数なので確率変数の関数である統計量\( T(X) \)も確率変数です。*3：このことが不偏推定量を考える一つのモチベーションになっています。しかし\( \theta \)が未知である以上、\( b(T(X)) \)の修正は必ずしも容易ではありません。*4：他にも完備十分統計量の理論を用いる方法もあります。*5：以降の議論は\( f_n(X \mid \theta) \)が同時確率関数であっても成り立ちます。*6：証明中に出てきた「微分と積分の交換が保証されているという仮定」が正則条件です。"
https://www.ariseanalytics.com/activities/report/20220809/,国際会議 ACM AsiaCCS 2022 参加報告,"ARISE Tech Blog2022.08.09Tweet国際会議 ACM AsiaCCS 2022 参加報告ARISE analyticsの近藤です。弊社では、KDDIグループを支えるためのAI技術開発を行っており、通信セキュリティ技術へのAI導入もミッションの一つです。今回、セキュリティ技術の動向把握を目的とし、弊社メンバー2名とともに2022年5月30日～6月2日に開催されたACM AsiaCCS 2022に参加してきました。本記事では、AsiaCCSの概要およびいくつか研究をピックアップして紹介いたします。ACM AsiaCCS 2022の概要ACM AsiaCCSは、ACM SIGSAC（ASIA Conference on Computer and Communications Security）主催の年次カンファレンスのひとつです。コンピュータ・通信セキュリティに加え、暗号・情報セキュリティの話題を扱っています。ACM ASIACCS 2022のキービジュアル（引用：ACM ASIACCS 2022 – May 30th to June 2nd, 2022, Nagasaki, Japan）AsiaCCSの名前の通り、アジアの各都市で順番にで開催されています。2020年は台湾、2021年は香港開催となり、2022年度は、日本の長崎とバーチャルのハイブリッドで開催されました。日本での開催は2014年の京都開催以来です。参加者は全体で276名、うち現地参加者は85名でした。また、参加国は29か国、現地参加者は13か国でした。今回の会場に用いられた出島メッセ長崎は2021年の11月に開館したばかりの建物で、この建物で国際会議が開催されるのは今回が初めてです。そのため、現地メディアの取材も入り大変盛り上がっていたようです（筆者はバーチャル参加だったため、現地の雰囲気を味わえず残念でした…）。論文については463件投稿され、うち85件が採択されました（Acceptance Rate= 18.4％）。このうち、機械学習（Machine Learning）に関わるものは18件でした。採択論文の分野内訳（発表論文一覧をもとに筆者作成）今回のプログラムは、採択論文から構成されたオーラルセッション・ポスターセッションに加え、3件の基調講演と６件のWorkshopsから成り立っていました。基調講演では、差分プライバシーのためのデータ合成技術やセルラーネットワークのためのセキュリティ技術といったテーマが扱われていました。資料とレコーディングが公開されているため、気になった方はぜひご覧ください。今回併設されたWorkshopsは、以下のトピックが扱われていました。うち半数は今回が初めての開催でした。いずれも重要な領域であるとともに、本カンファレンスが扱うテーマの幅広さを感じるものでした。APKC – The 9th ACM ASIA Public-Key Cryptography Workshop（公開鍵暗号）BSCI – The Fourth ACM International Symposium on Blockchain and Secure Critical Infrastructure（ブロックチェーンとインフラ）CPSS – The 8th ACM Cyber-Physical System Security Workshop（サイバーフィジカルセキュリティシステム）CySSS – The 1st Workshop on Cybersecurity and Social Sciences（サイバーセキュリティと社会科学）WDC – The 1st Workshop on Security Implications of Deepfakes and Cheapfakes（ディープフェイク）WoRMA – The 1st Workshop on Robust Malware Analysis（マルウェア解析）オーラルセッション・ポスターセッションでは、バーチャル参加者はzoomミーティングにアクセスし、発表を聴講する形式でした。特設サイトも用意されており、発表後に資料やレコーディングが格納されるため、気になった発表を後からじっくり視聴することも可能になっていました。ピックアップ研究紹介本節では、AI領域の技術を用いて通信セキュリティの課題に取り組んでいる研究からいくつかピックアップして紹介します。なお、ASIACCSのProceedingsはOpen Accessとなっており、こちらからすべて閲覧可能です。本節で紹介した研究事例以外にも素晴らしい発表がたくさんありますので、ぜひご覧ください。GraphTrack: A Graph-based Cross-Device Tracking Framework著者：Binghui Wang , Tianchen Zhou , Song Li , Yinzhi Cao , Neil Gong概要グラフ構造を対象としたユーザトラッキング技術の一つです。ひとりのユーザが複数台のコンピュータ、スマートフォンを持つのが当たり前になっている中で、クロスデバイストラッキング（同一ユーザが保有しているデバイスを特定しデバイスを超えたユーザトラッキングを行う）を実現することが目的です。クロスデバイストラッキングの課題自体はだいぶ前から存在するものの、検索履歴を用いたアプローチが主流でした。しかし、IPとドメインの相関関係を捉えきれないことや閲覧履歴とデバイスをうまく紐づけできない点が課題になっていました。手法本研究が提案するGraphTrackではIP、ドメイン、デバイス間の相互関係をグラフ化することで複雑な相関関係を捉えられるようにしました。IPアドレスとデバイスが対応付けられたIP-Device Graph、IPアドレスとアクセス先ドメインが対応付けられたDomain-Device Graphを構築し、ランダムウォーク（グラフをサンプリングする手法の一種。ランダムに選んだノードを起点とし、ランダムに隣接ノードを選択して移動することを繰り返し、サンプリングを行う）を用いることで特徴空間へデバイスに対応するノードの埋め込みを行います。つぎに、特徴空間内でデバイスノード間の距離計算を行い類似度判定を行うことで、同一ユーザが保有するデバイスか否かを判定します。IP-Device Graph/Domain-Device Graphの概要（GraphTrack: A Graph-based Cross-Device Tracking Frameworkより引用）実験結果実世界データセットで評価した結果、提案手法であるGraphTrack-IP/GraphTrack-IP-SUは既存手法に比べ高い性能でのクロスデバイストラッキングを実現できていることが確認できました。実験結果（GraphTrack: A Graph-based Cross-Device Tracking Frameworkより引用）感想本手法はグラフ構造として扱うことで課題解決につなげるものであり、グラフ構造の懐の広さに感心しました。クロスデバイストラッキング自体はユーザデータの分析に用いられるものですが、セキュリティの観点でも複数デバイスの相関関係を算出し保有ユーザの関連性を見出すことはDDoS攻撃などのサイバー攻撃の防止・攻撃者特定に役立ちそうです。Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation著者：Bang Wu,Xiangwen Yang,Shirui Pan,Xingliang Yuan概要GNNモデル抽出攻撃方法の分類及び実現方法の提案です。近年増加しているモデル抽出攻撃では、APIサービスの入出力をもとにモデルの抽出（複製）を行います。モデルはAPIサービスのコアとなる部分であり、貴重な知的財産であることからモデル抽出攻撃の防御の必要性が高まっています。既存のモデル抽出攻撃は画像やテキストを対象としたものであり、グラフを対象としたものはありませんでした。そこで、本研究ではグラフを対象としたGNNモデル抽出攻撃方法を整理・実証し、防御方法の検討の足掛かりとすることを目的としています。GNNモデル抽出攻撃の概要（引用：Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation）手法今回の攻撃シナリオとしてFacebookやTwitterなどのSNSに対するGNNモデル抽出攻撃を想定します。このGNNモデルは、年齢や性別などの異なる属性を持つプロファイル情報から、ネットワーク内のユーザ（ノード）を予測できます。攻撃方法として、複数のユーザをハッキング・勧誘することで攻撃対象モデルへのクエリを実行できること、またSNSを通して追加情報を容易に収集できること、を考慮する必要があります。このシナリオに基づき、GNNモデル抽出攻撃における攻撃対象をNode,Graph,Datasetに仮定することで、組み合わせによる攻撃パターンを整理しました。結果、7つの攻撃パターンに分類されることを確認しました。さらに、それぞれの攻撃パターンについて具体的なアルゴリズムの提案を行いました。GNNモデル抽出攻撃パターンの分類。Xはノード情報、Aはエッジ構造、Gはグラフを示す。●/◐/○は攻撃者が保有している情報が完全/部分/無のどれかを示す。（引用：Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation）実験結果Cora,Citeseer,Pubmedデータセットを対象に各攻撃パターンによる攻撃を行い、GNNモデル抽出を行いました。この結果、単純なMLPモデルよりもTarget  Modelに近い性能が得られることが確認できました。実験結果。本タスクはモデル抽出攻撃のため、モデルより良い性能を得ることが目的ではなく、モデルに近い性能を得ることが目的であることに留意が必要。（引用：Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation）感想深層学習は画像や自然言語を中心に普及した背景もあり、画像や自然言語モデルを対象としたモデル抽出攻撃は非常に有名です。そのため、これらのドメインを対象としたサービスを開発するときは脆弱性を作りこまないような留意が必要です。グラフ構造を対象としたサービスはまだまだ普及途上ですが、交通インフラやECコマース、SNSなどを代表とするように、社会の様々なつながりはグラフ構造で表現可能です。今後グラフを対象としたAPIサービスが増えていくと思われることもあり、非常に重要な分析である印象を抱きました。なお、本研究の実装はgithubで公開されており追試可能です。https://github.com/TrustworthyGNN/MEA-GNNEVOLIoT: A Self-Supervised Contrastive Learning Framework for Detecting and Characterizing Evolving IoT Malware Variants著者：Mirabelle Dib,Sadegh Torabi,Elias Bou-Harb,Nizar Bouguila,Chadi Assi概要マルウェア検知のタスクでは、未知の新種マルウェアや亜種マルウェアもカバーできる必要があるため、ラベルあるの教師あり学習ではカバーできません。そこで、本手法では自己教師あり学習によるラベルレスでのマルウェアの埋め込み表現を学習する手法としてEVOLIoTを提案しています。手法マルウェアが出力する命令（アセンブリコード）に対しBERTで意味ベクトルを抽出し、Contrastive Learningによる自己教師あり学習によりマルウェアの埋め込み表現を学習します。Contrastive LearningにおいてはPositive Pairの生成がポイントとなりますが、ここでは同一のアセンブリコードに対しBERTのDropout Maskだけ変えることでPositive Pairを取得しています。EVOLIoTのアーキテクチャ（引用：EVOLIoT: A Self-Supervised Contrastive Learning Framework for Detecting and Characterizing Evolving IoT Malware Variants）実験結果亜種マルウェアを含むデータセットに対し、埋め込み表現を獲得したうえでt-SNEによる定性評価およびMean Average Precision (MAP)による定性評価を行いました。定性評価では既存手法に比べてクラスタごとのまとまりが形成され、定量評価では既存手法に対し良好な性能が得られることが確認できました。実験データセットの内訳（引用：EVOLIoT: A Self-Supervised Contrastive Learning Framework for Detecting and Characterizing Evolving IoT Malware Variants）t-SNEによる埋め込み表現のマッピング結果。右が提案手法（引用：EVOLIoT: A Self-Supervised Contrastive Learning Framework for Detecting and Characterizing Evolving IoT Malware Variants）定量評価結果（引用：EVOLIoT: A Self-Supervised Contrastive Learning Framework for Detecting and Characterizing Evolving IoT Malware Variants）感想今回のように未知のデータを扱わなければならないタスクにおいて、Contrasteive Learningを用いた自己教師あり学習は非常に有効なアプローチです。Positive Pairをいかにして生成するかがポイントですが、マルウェアのアセンブリコードをBERTに入力することで生成するアプローチには驚きでした。自然言語処理では広い領域で有効性が確認されているBERTですが、今回のようにマルウェアのアセンブリコードにも適用できることは大きな知見となりました。おわりに筆者はこれまで画像や動画、情報検索など機械学習と相性の良い領域の研究開発に取り組んでいました。今回初めてセキュリティ領域の国際会議に参加しましたが、セキュリティ分野が扱うテーマは非常に幅広く、これまで意識しなかったようなテーマが多くあることに驚きました。今後もセキュリティ領域を定期的にウォッチし、機械学習と相性の良い分野を見極めたうえで社会に役立つ技術開発を行っていきたいです。最後になりましたが、ARISE analyticsではKDDIの通信データを対象とした技術開発に取り組んでいます。今回紹介した内容や、その他記事で触れている弊社の取り組みに興味がございましたらぜひお声がけください。https://www.ariseanalytics.com/recruit/"
https://www.ariseanalytics.com/activities/report/20221005/,【Go言語入門】goroutineとは？ 実際に手を動かしながら goroutineの基礎を理解しよう！,"ARISE Tech Blog2022.10.05Tweet【Go言語入門】goroutineとは？ 実際に手を動かしながら goroutineの基礎を理解しよう！はじめまして。2022年4月に中途入社し、現在Analytics Delivery Divisionでバックエンド開発をしているエンジニアのナムです。ARISE analyticsに入社してから開発言語としてGoを使うことになりました。Goの特徴・メリットはいろいろありますが、今回はその中でもgoroutineについて簡単に話したいと思います。goroutineとはgoroutineは「Goでプログラムの同時性を簡単に具現し、既存の単純スレッド基盤に比べて効率的な動作を遂行するために作った作業単位」です。全てのGoプログラムは必ず1個以上のgoroutineを持ち、常にバックグラウンドで動作します。それぞれのgoroutineは独立的に実行されます。goroutineの特徴は非常に軽量なスレッドということです。これについては下に大きく3つに分けて説明します。メモリ消費goroutineはカーネルスレッド（以下、スレッド）に比べてより少ないメモリのみ必要です。スレッドはスレッド間のメモリ保護の役割をするGuard pageスペースを含めて1MB程度のスタックを必要とします。一方、goroutineは2KBのスタックだけが必要です。そして、メモリが足りない場合はヒープを使用します。生成・破壊コストスレッドはOSからリソースを要請し、作業が終了したらリソースを戻すなどやり取りの時間がかかります。しかし、goroutineはGo Runtimeから生成と破壊が行われるためコストが低いです。コンテキストスイッチgoroutineはスレッドモデルとしてM:Nモデルを採用しています。そのため、マルチタスキングによるコンテキストスイッチが抑えられ、マルチコアが活用できるというメリットがあります。スケジューリングが複雑になるというデメリットがありますが、GO Runtime Schedulerが解消してくれます。ちなみに、Goはgoroutineの受動管理をサポートしていないです。Go Runtime Scheduler？Go Runtime Schedulerは、Goプログラムが実行される時点で一緒に実行されてgoroutineを効率的にスレッドにスケジューリングさせる役割を遂行します。下のような原則を持って動作します。カーネルスレッドはコストが高いため、できる限り少なく使用する。多くのgoroutineを実行し、高いコンカレンシーを維持する。Nコアマシンで、N個のgoroutineをパラレルに動作させる。※一番下の原則からgoroutineはマルチCPUコアを持ったハードウェアで最も性能を出せるということが分かります。ここまでがgoroutineについての簡単な説明でした。もっと深堀りをするとGMP模型、Schedulerの動作など長い話になってしまうので、とりあえず実際にgoroutineを実装してみましょう。goroutine実装goroutineの作り方はすごく簡単です。関数の前にgoというキーワードを付けて、その関数を実行するだけでgoroutineが生成されます。gofunction()無名関数を利用することも可能です。gofunc(){...}()下はgoroutineを実装したコードです。どのような結果になるか考えてみてください。packagemainimport(""fmt""//標準I/Oのためにfmtパッケージをimport)funcSay(sstring){fmt.Println(s)}funcmain(){goSay(""hello"")goSay(""world"")}上記の「全てのGoプログラムは必ず1個以上のgoroutineを持つ」という内容がヒントになります。これは、main関数も1つのgoroutineだということです。つまり、このmain goroutineの上で動くgoroutineは自分を呼び出した関数が無くなったことで消滅され、何も出力されない結果になります。理由が分かったので、結果が見えるようにしてみましょう。単純にmain関数の終了を遅延させれば結果が見えるはずです。...// 同じコードfuncmain(){goSay(""hello"")goSay(""world"")fmt.Scanln()}方法はいろいろありますが、ここではScanln()を呼び出し、ユーザーの入力を待つように実装することでmain関数を終了しないようにしました。実行すると、-- A --helloworld-- B --worldhello出力結果はAかBのどちらかになります。実行を繰り返してみると結果が変わることが分かります。goroutineはgoroutine同士が独立しているため、実行の順序性は担保されていません。では、どうやって実行の順序性（実行フロー）をgoroutineは制御できるのでしょうか。この時使うのがchannelです。channelchannelとはchannelはgoroutine同士の値をやり取りする通路の役割と実行フローを制御する役割を担うデータ構造です。 channel自体はvalueではなく、Referenceタイプです。そして、全てのタイプをchannelとして使用ができ、channelに値を渡して抽出する形で使用します。Goはchannelを基本データ型で提供するので、他のパッケージやライブラリなしですぐ使用することができます。channelを図で表現すると下のようになります。channel実装channelはmake()で生成できます。ch:=make(chanstring)値は<-でやり取りができます。ch<-data// channelにdata変数の値を送る（送信）data:=<-ch// channelから値を抽出し、その値をdata変数に入れる（受信）では、channelを使用してみましょう。packagemainimport(""fmt"")funcSay(cchanstring){data:=<-c// Afmt.Println(data)data=""Let's Go""c<-data// B}funcmain(){ch:=make(chanstring)gofunc(cchanstring){data:=""hello world""c<-data// Cdata=<-c// Dfmt.Println(data)}(ch)goSay(ch)fmt.Scanln()}その結果はhello worldLet's Go先に「hello world」が出力された後「Let’s Go」が出力されます。この結果は何回を実行しても変わりません。何故なのかは実行順序を見ながら説明したいと思います。実行順序1.C（※AとCの実行順位が同じ時点）・最初、2個のgorountineの中でどちらが先に実行されるかは分かりません。・しかし、どちらが先に実行されるとしても「A」は受信なので、channelに値が送信されることを待ちます。・そのため、いずれの時も「C」が先に実行されます。2.A・「C」がchannelに値を送信した後は次の行に進まず、channelの値が受信されることを待ちます。・待っていた「A」はchannelの値を受信します。3.B（※BとDの実行順位が同じ時点）・「A」がchannelの値を受信した後、そのまま進んで「B」まで実行されます。・「A」が受信する時点で「C」の待ち状態が解除されるため「D」も実行されます。・つまり、この時点では「B」と「D」の中でどちらが先に実行されるかは分かりません。・しかし、どちらが先に実行されるとしても「D」は受信なので、channelに値が送信されることを待ちます。・そのため、いずれの時も「B」が先に実行されます。4.D・「B」がchannelに値を送信した後は次の行に進まず、channelの値が受信されることを待ちます。・待っていた「D」がchannelの値を受信します。上の流れからchannelを利用することでgoroutine同士に値のやり取りができること、goroutineの実行順序を制御できることが分かります。これでchannelについて理解できたと思います。次は、Goでchannelを操作する方法を軽く見てみましょう。channelの方向基本的にchannelは双方向ですが、単方向channelも作ることができます。単方向チャンネルを作る時はmake()と矢印を使用します。// 受信用channelc1:=make(<-chanType)// 送信用channelc2:=make(chan<-Type)buffered channelbuffered channelは、受信者が受け取る準備ができていなくても、指定されたバッファだけ値を送信し、継続して他のタスクを遂行することができます。buffered channelで送信側はバッファがいっぱいになった場合にのみ遮断され、受信側ではバッファが空いている場合にのみ遮断されます。bufferd channelは、make(chanType、N)を使用して作成することができます。Nには使用するバッファの個数を書きます。myChannel:=make(chanType,N)closeclose()を使用してchannelを閉めることができます。channelを閉めたら、該当channelには二度と送信することはできません。しかし、channelに値が存在する限り受信は可能です。close(myChannel)下のコードを使用してchannelが閉じているかどうか確認することができます。閉じていたらcheckがfalseになり、開いていたらcheckがtrueになります。data,check:=<-myChannelfor rangefor rangeを使用してchannelが閉じる時まで値を受信することができます。channelが開いていたらrangeはchannelに値が入るまで待機します。channelが閉じられたらループは終了になります。fordata:=rangemyChannel{...}selectswitchと似ていますが、selectでcaseはchannelで送信または受信作業を意味します。selectはcaseのいずれかが実行されるまで待機します。 もし多数のcaseが用意される場合には、selectがランダムで一つを選択します。selectにdefaultがあれば、caseが用意されていなくても待機せずにdefaultを実行します。select{case<-ch1:// ch1に値が入った時に実行case<-ch2:// ch2に値が入った時に実行default:// 全てのchannelに値が入らなかった時に実行}同期化オブジェクトGoではchannel以外にもgoroutineの実行フローを制御する同期化オブジェクトを提供します。オブジェクトは複数ありますが、本記事ではMutexとWaitGroupについて話します。MutexMutexは複数のgoroutineが共有する値を保護する時に使用します。syncパッケージで提供するMutexの構造体と関数は次のようです。sync.Mutexfunc (m *Mutex) Lock()func (m *Mutex) Unlock()下はMutexを使用せず、2個のgoroutineがそれぞれ1万回ずつスライスに値を追加するコードです。packagemainimport(""fmt""""time""// Sleepを利用してプログラムを待機させるためにtimeパッケージをimport)funcmain(){vardata=[]int{}gofunc(){fori:=0;i<10000;i++{data=append(data,1)}}()gofunc(){fori:=0;i<10000;i++{data=append(data,1)}}()time.Sleep(2*time.Second)// 2秒待機fmt.Println(len(data))// スライスの長さを出力}結果として20000を期待しますが、複数回実行してみると20000、10000、9432、13425…など結果が20000だと担保されていないことが分かります。これは二つのgoroutineが競合し、同時に値にアクセスしたのでappend()が正確に処理されていない時があるためです。下は上のコードでスライスをMutexで保護したコードです。packagemainimport(""fmt""""sync""// Mutexオブジェクトを使用するためにsyncパッケージをimport""time""// Sleepを利用してプログラムを待機させるためにtimeパッケージをimport)funcmain(){vardata=[]int{}varmutex=new(sync.Mutex)gofunc(){fori:=0;i<10000;i++{mutex.Lock()// スライスを保護data=append(data,1)mutex.Unlock()// スライスを保護解除}}()gofunc(){fori:=0;i<10000;i++{mutex.Lock()// スライスを保護data=append(data,1)mutex.Unlock()// スライスを保護解除}}()time.Sleep(2*time.Second)// 2秒待機fmt.Println(len(data))// スライスの長さを出力}複数回実行してみるといつも20000という結果が期待通りに出力されます。これでMutexによってスライスが保護され、append()が正確に処理されたことが分かります。ただし、Lock()とUnlock()は必ずペアを合わせなければならず、ペアが合わない場合はデッドロック(deadlock)が発生するので注意しましょう。WaitGroupWaitGroupは、goroutineがすべて終わるまで待つ時に使用します。 前ではtime.Sleep()やfmt.Scanln()を使用してgoroutineが終わるまで臨時待機しました。 今回はWaitGroupを使用し、goroutineが終わるまで待ってみます。syncパッケージで提供するWaitGroupの構造体と関数は次のようです。sync.WaitGroupfunc (wg *WaitGroup) Add(delta int)func (wg *WaitGroup) Done()func (wg *WaitGroup) Wait()下はWaitGroupを使用して、すべてのgoroutineが終った後mainを終了するコードです。packagemainimport(""fmt""""sync"")funcmain(){wg:=new(sync.WaitGroup)fori:=0;i<5;i++{wg.Add(1)// 繰り返す度にAdd関数で1ずつ追加gofunc(nint){fmt.Println(n)wg.Done()// goroutineが終わったこと知らせる}(i)}wg.Wait()// 全てのgoroutineが終わるまで待機fmt.Println(""プログラム終了"")}Add()でgoroutineの数を追加し、goroutineの中でDone()を使用してgoroutineが終了したことを知らせます。そして、Wait()ですべてのgoroutineが終わるまで待ちます。なので、実行してみると5個のgoroutineが終った後mainが終了されることが分かります。ただし、Add()に設定した値とDone()が呼び出される回数は同じである必要があります。 この回数が合わないとpanicになるので注意しましょう。goroutineを使用する時の注意点下はGoコミッターであるDave氏が話したことです。Never start a goroutine without knowing how it will stop翻訳すると、「どのように停止するかを知らずにゴルーチンを開始しないでください」ということです。本記事で使用したコードは単純なのでどのタイミングでどのようにgoroutineが終るかを分かりやすいでが、業務で使用することになったら複雑でgoroutineのライフサイクルが分かりにくくなると思います。この時はGoのcontext（WithCancel、WithDeadlineなど）を使用することでgoroutineのライフサイクルを制御することができます。興味がある方はぜひ調べてみてください。まとめgoroutineとは何かから始め、値のやり取りと実行フローを制御するためのchannel、そして、channel以外にもgoroutineの制御ができる同期オブジェクトとgoroutineを使用する時の注意点について話しました。いままでの内容を組み合わせてみると個人的にはgoroutineをchannelで値のやり取りしながら同期オブジェクトで非同期の問題点（race conditionなど）を制御し、contextでライフサイクルを管理するのがbest practiceかと思います。本記事を読まれ、goroutineの基礎と使い方について理解できたら良いと思います。最後に、ARISE analyticsでは様々なプロジェクトで先端技術を使って開発しており、一緒に開発する仲間を募集しています。ご興味のある方はこちらのページからご連絡をお待ちしています！"
https://www.ariseanalytics.com/activities/report/20221028-2/,ARISE Kaggle部活動記 #4,"OTHERS2022.10.28TweetARISE Kaggle部活動記 #4Kaggle部ARISE analytics 渡邉です。ARISE Kaggle部の活動記へようこそ。今回、Google Smartphone Decimeter Challenge 2022に参加し20位銀メダルを獲得できました。コンペのポイントと解法の紹介を行いたいと思います。コンペ概要目的Andoridスマートフォンで受信したGNSSのデータから、その受信位置を予測することが目的。データセットスマートフォンを車内に設置し、ドライブする。数十分のドライブの間中、スマートフォンと人工衛星は通信を行っている。このとき、いくつの人工衛星と通信できるかはドライブの経路に依存するため、データ量は経路ごとに増減することとなる。私の解法3つの手法を試した。(どれも私が位置から考えたわけではなく、他のkaggle参加者が公開してくださった手法を基にしています。添付する実装例は、その公開されたコードをリンクとしています。)衛星通信データWLS衛星データから各時点でスマートフォンの位置を推定する。カルマンフィルタ平滑化各時点の位置を、時系列全体の位置の情報を使って尤もらしくする。加加速度（躍度）最小化ドライブであるから、急加速減速はしないはず。それをカルマン平滑化の結果とバランスを取りながら修正する。解法の説明衛星通信データWLS【この手法の目的】衛星データから受信デバイスの位置と速度を特定する【問題のモデル化】変数を次のように与える。受信デバイスの位置\( x,y,z \)、受信デバイスの時刻\(c \) 、  \(i\)番目の衛星の位置\(x^i_s, y^i_s, z^i_s\)、 \(i\)番目の衛星の時刻 \(t^i_s\)、受信デバイスと\(i\)番目の衛星の距離 \(\rho^i\)ここで、次の関係が成り立つ。\(\rho^i=\sqrt{(x^i_s-x)^2+(y^i_s-y)^2+(z^i_s-z)^2}\)　・・・(1)\(\rho^i\)は既知であるように見える。なぜなら、例えば12:00:00に発信された信号を12:01:00に受信したならば、60x[電波の秒速]で求められるからである。しかし、ここで問題が発生する。発信された時刻は、衛星が記録するため正確だが、受信した時刻は受信デバイスによる記録のため、不正確である。この時刻のずれを\(d\)とおく。ここまでの議論を次式で表す。\(\rho^i=c(t+d-t_s)\)\(=c(t-t_s)+cd\)\(=P+cd\)・・・(2)\(P\)を疑似距離とよぶ。それぞれの時刻から計算できるが、\(cd\)分ズレているため、こう呼ばれているのだろう。(1),(2)を合わせて考えると、\(P+cd = \sqrt{(x^i_s-x)^2+(y^i_s-y)^2+(z^i_s-z)^2}\)　・・・(3)となる。既知の変数と未知の変数に分けて考えると、◆既知・\(P\)　（発信した時刻と受信した時刻は分かるから）・\(c\)　（物理定数）・\(x^i_s, y^i_s, z^i_s\)　（受信した信号に記録されている）◆未知・\(x,y,z,d\)よって、最低4機の衛星と通信できれば受信デバイスの位置が求められることが分かった！しかし、4機以上の衛星と通信できることの方が多い。このとき、ノイズを多分に含むため、すべての方程式を満たす解は見つからない。このようなとき、できるだけそれぞれの方程式を満たさない量（正確な呼び方を筆者は知らないが、誤差と呼ぶことにする）を小さくする点を解とすることが考えられる。誤差を小さくするよう逐次的に計算する方法を逐次最小二乗法とよぶ。さらに工夫すると、衛星ごとにデータの信頼性は異なると考えられるため、衛星ごとに誤差に重みをつけることもできる。これはWLS（Weighted Least Squer）とよばれる。また、各時点の衛星の速度も得られるため、同様に各時点の受信デバイスの速度も求めることができる。【このコンペにおける工夫点】実装はこちら位置と速度それぞれをscipy.optimize.least_squaresによって行った。工夫点は次の二つ。1.信号の信頼性がデータに含まれていたため、それで重みづけした。・RawPseudorangeUncertaintyMeters・PseudorangeRateUncertaintyMetersPerSecond2.WLSの損失関数にはコーシーロスを採用した。(\(loss(z) = ln(1+z)\))・デフォルトの線形ロス(\(loss(z) = z\))よりもスコアが向上した。・外れ値の影響を抑えられたからと考えている。外れ値により異常に大きな誤差が出てしまった場合でも、比較的小さな誤差をみなす。カルマンフィルタ平滑化【この手法の目的】ノイズが載った時系列位置データから、ノイズを取り除く【問題のモデル化】WLSにより各時刻における位置データが得られたが、これらのデータは真の位置にノイズが載ったものと考える。また、ある時刻と次の時刻の間、車は等速直線運動に従うとする。これを状態空間モデルを用いて表現する。\(\begin{bmatrix} x^{(t+1)} \\ y^{(t+1)} \\ z^{(t+1)} \end{bmatrix}\)=\(\begin{bmatrix} 1 &0 &0 \\ 0 &1 &0 \\ 0 &0 &1 \end{bmatrix}\)\(\begin{bmatrix} x^{(t)} \\ y^{(t)} \\ z^{(t)} \end{bmatrix}\)+\(\begin{bmatrix} w_x^{(t)} \\ w_y^{(t)} \\ w_z^{(t)} \end{bmatrix}\)\(\begin{bmatrix} o_x^{(t+1)} \\ o_y^{(t+1)} \\ o_z^{(t+1)} \end{bmatrix}\)= \(\begin{bmatrix} 1 &0 &0 \\ 0 &1 &0 \\ 0 &0 &1 \end{bmatrix}\) \(\begin{bmatrix} x^{(t+1)} \\ y^{(t+1)} \\ z^{(t+1)} \end{bmatrix}\)+\(\begin{bmatrix} \mu_x^{(t+1)} \\ \mu_y^{(t+1)} \\ \mu_z^{(t+1)} \end{bmatrix}\)\(W^{(t)} = \begin{bmatrix} w_x^{(t)} \\ w_y^{(t)} \\ w_x^{(t)} \end{bmatrix}\),\(W^{(t)} \sim N(V^{(t)}, \sigma_v^{(t)})\),\(V^{(t)}=\begin{bmatrix} v_x^{(t)} \\ v_y^{(t)} \\ v_z^{(t)} \end{bmatrix}\)\(M^{(t)}=\begin{bmatrix} \mu_x^{t} \\ \mu_y^{t} \\ \mu_z^{t} \end{bmatrix}\),\(M^{(t)}\sim N(0,\sigma_x^{(t)})\)状態空間モデルが決定すれば、あとはカルマンフィルタ平滑化をかけるだけである。詳細はこの本が詳しい。【このコンペにおける工夫点】実装はこちら1.各時点のシステムノイズの共分散を推定した・共分散はハイパーパラメータとして扱いカルマン平滑化を行う。今回はWLSより共分散を推定することができた。http://ceres-solver.org/nnls_covariance.html2.Xiaomiのシステムノイズの共分散を大きくした・このコンペは各時点の速度も正解データとして与えられていた。その結果、Xiaomiのスマホだけ速度の推定精度が悪いことがわかっていた。これをカルマンフィルタ平滑化に反映するため、Xiaomiスマホの推定時だけ \(\sigma_v\)を10倍した。加加速度（躍度）最小化【この手法の目的】予測誤差を最小化しつつ、加加速度を最小化する【問題のモデル化】ある時刻と次の時刻の間、車は緯度\(\phi(t)\)・経度\(\psi\)それぞれにおいて、等加加速度運動に従うとする。公式を参考に、緯度について書くと、\(\phi(t+\delta t) = \phi(t) + \delta t \dot{\phi(t)} + (1/2)\delta t^2 \ddot{\phi}(t) + (1/6)\delta t^3 \dddot{\phi}(t) \)となり、位置、速度、加速度を状態としたシステム方程式を考えると、次式のように表現できる。\(\begin{bmatrix} \phi(t+\delta t) \\ \dot\phi(t+\delta t) \\ \ddot\phi(t+\delta t) \\ \psi(t+\delta t) \\ \dot\psi(t+\delta t) \\ \ddot\psi(t+\delta t) \end{bmatrix}\)=\(\begin{bmatrix} 1 & \delta t &(1/2)\delta t^2 & 0 & 0 & 0\\ 0 & 1 & \delta t & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & \delta t & (1/2)\delta t^2 \\ 0 & 0 & 0 & 0 & 1 & \delta t \\ 0 & 0 & 0 & 0 & 0 & 1 \end{bmatrix}\)\(\begin{bmatrix} \phi(t) \\ \dot\phi(t) \\ \ddot\phi(t) \\ \psi(t) \\ \dot\psi(t) \\ \ddot\psi(t) \end{bmatrix}\)+\(\begin{bmatrix} (1/6)\delta t^3 & 0 \\ (1/2)\delta t^2 & 0 \\ \delta t & 0 \\ 0 & (1/6)\delta t^3 \\ 0 & (1/2)\delta t^2 \\ 0 & \delta t \end{bmatrix}\)\(\begin{bmatrix} \dddot\phi(t) \\ \dddot\psi(t) \end{bmatrix}\)上記は連続時間についての式であるため、データに即して離散時間で表す。ただし、サンプリング周期は十分小さいとする。\(\begin{bmatrix} \phi[n+1] \\ \dot\phi[n+1] \\ \ddot\phi[n+1] \\ \psi[n+1] \\ \dot\psi[n+1] \\ \ddot\psi[n+1] \end{bmatrix}\)=\(\begin{bmatrix} 1 & \delta t &(1/2)\delta t^2 & 0 & 0 & 0\\ 0 & 1 & \delta t & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & \delta t & (1/2)\delta t^2 \\ 0 & 0 & 0 & 0 & 1 & \delta t \\ 0 & 0 & 0 & 0 & 0 & 1 \end{bmatrix}\)\(\begin{bmatrix} \phi[n] \\ \dot\phi[n] \\ \ddot\phi[n] \\ \psi[n] \\ \dot\psi[n] \\ \ddot\psi[n] \end{bmatrix}\)+\(\begin{bmatrix} (1/6)\delta t^3 & 0 \\ (1/2)\delta t^2 & 0 \\ \delta t & 0 \\ 0 & (1/6)\delta t^3 \\ 0 & (1/2)\delta t^2 \\ 0 & \delta t \end{bmatrix}\)\(\begin{bmatrix} \dddot\phi[n] \\ \dddot\psi[n] \end{bmatrix}\)簡単のため、次のように置きなおす。\(\Phi[n]\)=\(\begin{bmatrix} \phi[n] \\ \dot\phi[n] \\ \ddot\phi[n] \\ \psi[n] \\ \dot\psi[n] \\ \ddot\psi[n] \end{bmatrix}\), \(A=\begin{bmatrix} 1 & \delta t &(1/2)\delta t^2 & 0 & 0 & 0\\ 0 & 1 & \delta t & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & \delta t & (1/2)\delta t^2 \\ 0 & 0 & 0 & 0 & 1 & \delta t \\ 0 & 0 & 0 & 0 & 0 & 1 \end{bmatrix}\),\(B=\begin{bmatrix} (1/6)\delta t^3 & 0 \\ (1/2)\delta t^2 & 0 \\ \delta t & 0 \\ 0 & (1/6)\delta t^3 \\ 0 & (1/2)\delta t^2 \\ 0 & \delta t \end{bmatrix}\),\(U[n]=\begin{bmatrix} \dddot\phi[n] \\ \dddot\psi[n] \end{bmatrix}\)方程式を整理すると次になる。\(0=\begin{bmatrix} A & -I \end{bmatrix}\)\(\begin{bmatrix} \Phi[n] \\ \Phi[n+1] \end{bmatrix}\)+\(BU[n]\)全ての時点についてまとめて表す。\(0\)=\(\begin{bmatrix} A & -I & 0 & \cdots & & 0 \\ \vdots & & & & & \\ 0 & \cdots & A & -I & \cdots & 0 \\ \vdots & & & & & \\ 0 & \cdots & 0 & 0 & A & -I \end{bmatrix}\)\(\begin{bmatrix} \Phi[1] \\ \vdots \\ \Phi[n] \\ \vdots \\ \Phi[N] \\ \end{bmatrix}\)+\(\begin{bmatrix} B & 0 & \cdots & & 0 \\ 0 & B & & & \\ \vdots & & \ddots & & \\ & & & & \\ 0 & & & & B \end{bmatrix}\)\(\begin{bmatrix} U[1] \\ \vdots \\ U[n] \\ \vdots \\ U[N] \end{bmatrix}\)これが制約条件となる。簡単のために文字を置きなおす。\(0 = DX+EJ\)また、予測誤差を定義する。\(\begin{bmatrix} \hat\Phi[1] \\ \vdots \\ \hat\Phi[n] \\ \vdots \\ \hat\Phi[N] \\ \end{bmatrix}\)-\(\begin{bmatrix} 1 & \cdots & 1 & 0 & \cdots & 0 & 0 & \cdots & & \\ 0 & \cdots & 0 & 1 & \cdots &1 & 0 & \cdots & & \\ & & & & & & \ddots & & & \\ 0 & \cdots & 0 & & & & & 1 & \cdots & 1 \end{bmatrix}\)\(\begin{bmatrix} \Phi[1] \\ \vdots \\ \Phi[n] \\ \vdots \\ \Phi[N] \\ \end{bmatrix}\)これも簡単のために置きなおしておく。\(Y\)-\(C\)\(X\)ここまでで、最適化は次式のように定義できる。◆決定変数\(X=\begin{bmatrix} \Phi[1] \\ \vdots \\ \Phi[n] \\ \vdots \\ \Phi[N] \\ \end{bmatrix}\),\(ｍJ=\begin{bmatrix} U[1] \\ \vdots \\ U[n] \\ \vdots \\ U[N] \end{bmatrix}\)◆目的変数\(U^TRU+(Y-CX)^TL(Y-CX)\)\(R\)と\(L\)は、加加速度最小化か予測誤差最小化どちらを重視するか決定する適当な重み◆制約\(0 = DX+EJ\)【このコンペにおける工夫点】実装はこちらコースごとの\(R\)と\(L\)の調整・カルマン平滑化により予測できていないことが推察された場合、Lを小さくし、予測誤差の重みを小さくした。・例：両側に木々が生い茂っている見通しが悪いコース終わりにGNSSという歴史ある技術ではあるため、キャッチアップの量が膨大でしたが、ドメイン知識をつける楽しさが味わえたコンペでした。また、時系列データ平滑化や最適化についても新しい手法に触れられました。ARISE analyticsでは、kaggle等の分析コンペティションで上位成績を残すと、インセンティブとして報奨がもらえるARISE Tech Master制度があるので引き続き金メダルを目指し、努力していきたいです。分析はチームワークが大事です。役割を決めお互いの強みを持ち合うことで、一人で行うより何倍も効率的に分析課題に取り組めます。我々と一緒にKaggle部を盛り上げてくださる方はこちらのページからご連絡お待ちしております!!!!"
https://www.ariseanalytics.com/activities/report/20221031/,【論文読み会】International Conference on Learning Represantation （ICLR）2022,"ARISE Tech Blog2022.10.31Tweet【論文読み会】International Conference on Learning Represantation （ICLR）2022こんにちは。Marketing Solution Division, Marketing Transformation Unitの田中です。今回は前回のNeurIPS 2021 論文読み会に引き続き、第2回の論文読み会として社内で行われた、InternationalConferenceon Learning Represantation (ICLR) 2022論文読み会のまとめをお届けいたします。この論文読み会を開催するにあたって、前回と同様に社内で論文読み会に参加したい人を募り、各々が気になる論文を読んで共有する形をとりました。本記事では、発表者5名が紹介した論文について執筆させていただきます。BEiT: BERT Pre-Training of Image Transformers紹介者： 田中卓磨■概要BEiTはViTとBERTを組み合わせた、画像処理における自己教師あり表現学習モデルです。具体的には、まず画像を離散的なVisual Tokenに変換します。加えて、ViTと同様に画像をパッチに分解した後、BERTのMasked Language Modelのように画像の一部をランダムにマスクし、マスクされた画像パッチに対応するtokenを他のtokenから予測するタスクを学習することで画像の表現を獲得していきます。実験では、画像分類とセマンティックセグメンテーションにおいて、他手法と比較してBEiTが優位な結果を示しました。【論文読み会】BEiT_BERT Pre-Training of Image Transformers.pptxfromARISE analytics(URL)BEiT: BERT Pre-Training of Image Transformers | OpenReviewPiCO: Contrastive Label Disambiguation for Partial Label Learning紹介者：近藤真暉■概要Partial Label Learning（PLL：部分ラベル学習）を実現するための手法です。対照学習の代表的な手法であるMoCo (CVPR2020) をベースに、スタックされたMomentum Embeddingを流用するPositive Sample Selectionと移動平均法を用いてクラスの代表ベクトルを更新するPrototype-based Label Disambiguationを導入することでPLLの高精度化を実現しました。PLLでは、画像キャプションや動画の字幕といった情報を教師情報として扱うことが可能です。今回のPiCOの貢献により、他タスクのアノテーション流用によるモデルの高精度化が期待できます。【論文読み会】PiCO_Contrastive Label Disambiguation for Partial Label Learning.pptxfromARISE analytics(URL)PiCO: Contrastive Label Disambiguation for Partial Label Learning | OpenReviewAnalytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models紹介者：伊藤光祐■概要ノイズを逐次的に加工していくことでデータを生成することができるDiffusion Probablistic Model (DPM)へ簡単に組み込むことが可能な手法を提案しています。既存手法の性能を向上させつつ、20~80倍の高速化を達成することができるという結果を示した論文です。上記の結果は以下の2つの工夫により達成されています。・これまで固定パラメータとすることが一般的であったDPMの各ステップで推定される分布の分散を、モデルの出力から解析的に最適な数値を算出するようにした。・学習したステップから重要な部分だけを、主に手作業で抽出して、ステップ数を減らすことが一般的であったのを、動的計画法を用いたアルゴリズムで性能劣化を最小限に最適なステップを抽出することで高速化した。今回の紹介スライドではこれらの工夫について解説を行っています。【論文読み会】Analytic-DPM_an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models.pptxfromARISE analytics(URL)Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models | OpenReviewPyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting紹介者： 秋元祐介■概要本論文で提案されているPyraformerは、時系列解析において時間に依存するデータの特徴を正確にとらえるために、入力データから複数の異なる解像度の時系列データを再構築して予測を行うモデルです。再構築された解像度の異なる時系列データは解像度間で木構造をもち、Pyraformerではこの木構造に対してAttention機構を適用することによって長期的な時間依存性を捉えることに成功しました。また、既存手法と比較して計算量を大きく削減することに成功し、推論速度が向上しています。実験結果では、ReformerやLongformerなど既存手法の精度を軒並み上回る性能を示しました。【論文読み会】Pyraformer_Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting.pptxfromARISE analytics(URL)Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time…Autoregressive Diffusion Models紹介者：奥井恒■概要Autoregressive DiffusionModels (ARDM) は、自己回帰モデルと拡散モデルを一般化したモデルです。本手法では拡散モデルのために開発された動的計画法を用いて、性能を大幅に低下させることなく、同時に複数のトークンを生成するために並列化することができます。本論文では、ARDMが離散拡散モデルと同等かそれ以上の性能を持ち、かつモデリングステップの効率性が高いという結果が示されています。【論文読み会】Autoregressive Diffusion Models.pptxfromARISE analytics(URL)Autoregressive Diffusion Models | OpenReviewおわりに今回、Diffusion Model、Transformerの時系列解析・画像処理、PLLと幅広く様々な分野の論文が紹介され、会社全体として多くの知識を蓄積することができました。また、発表後にも聴講者から多くの質問をいただき、互いの論文への理解を深めることができました。個人的にDiffusion Model はお絵描きAIで注目を集めていたため、今回紹介された新しい手法とともに基礎から学び直せる良い機会だと感じました。また、画像処理や時系列解析といった業務に応用しやすい分野の紹介が多かったため、案件に応じて実務で実装していきたいと思います。今後とも第3回、第4回と国際学会の論文読み会を継続していき、最先端の技術をキャッチアップして業務に還元できるよう活動を続けていきたいと思います。"
https://www.ariseanalytics.com/activities/report/20221202/,社員の技術力やスキル研鑽を支援する！社内図書館「ARISE library」とおすすめ書籍紹介,"ARISE Tech Blog2022.12.02Tweet社員の技術力やスキル研鑽を支援する！社内図書館「ARISE library」とおすすめ書籍紹介始めにこんにちは。2020年4月に新卒として入社したMarketing Solution Division所属兼ARISE図書委員の小林と申します。今回の記事は、ARISE library＆書籍購入補助制度について概要を説明し、それらを利用して私が読んだ本について紹介いたします。ARISE analyticsにはデータサイエンティストの技術力やスキルアップのための制度や環境があることを知っていただけると幸いです。ARISE library＆書籍購入補助制度とは～ARISE libraryについて～ARISE libraryとはオフィスのカフェスペース内に本棚が置いてある場所と、置いている本の貸出制度や書籍貸出管理システムのことを指します。きれいな本棚と観葉植物があり、ヒカリエ31Fからの景色を眺めることができる空間なので個人的に好きです！(出社時は気分転換に、カフェスペースに移動してオンラインMTGに参加することもあります)ARISE analyticsで働く従業員は貸出に必要な手続きを踏めば自由に本を借りることができます。機械学習、マーケティング、コンサルティング、ビジネストレンド、人事など、様々な本がカテゴリー分けして配置されており 、自分が必要な本を探しやすい設計になっています。様々なカテゴリの本が置いてあり、幅広いテーマの本に触れることができます。読みたいけど本棚にない本は書籍購入補助制度を使って、本を購入することができます。また読まなくなった本を寄贈することもできます。～書籍購入補助制度について～続いて書籍購入補助制度の詳しい内容について紹介します。書籍の費用を会社が負担し、購入した本の感想やレビューを専用のslackチャンネルに投稿後、納本するルールとなっています。実質無料で本を読むことができ、その本を全社員へ共有できる制度は素晴らしいなと思っています。普段他の人がどんな本を読んで自己研鑽をしているかを知るきっかけにもなりますし、本を買うお金も節約できます！ 笑ARISE図書委員が開発を手掛ける「書籍貸出管理システム」ちなみに私が所属するARISE図書委員には、データサイエンティストやエンジニアなどが所属しており、ARISE library運営に欠かせない「書籍貸出管理システム」を業務の合間を縫って自分たちで開発しています。開発の裏話等は以前こちらの記事にまとめてあるので、宜しければご覧ください！（https://www.ariseanalytics.com/activities/report/20200908/）最近は本システムをさらにアップデートし、過去に自分が借りた書籍の一覧が表示されたり、従業員それぞれの行動ログをもとにしたおすすめ書籍が表示されるレコメンドエンジンが組み込まれたり、様々な機能を追加で実装しています。次章では、レコメンドエンジンの導入に関して、担当した野尻さんに開発の裏話を紹介していただきます。書籍貸出管理システム開発担当による 機能アップデートの裏話…こんにちは。ARISE図書委員で書籍貸出管理システム開発を担当している野尻です。書籍貸出管理システムのレコメンドエンジンは、リストを計算する部分と、リストの配信を行う部分に分かれています。全体構成は以下の図になります。まず、リスト配信部分ですが、「API Gateway」、「Lambda」、「Dynamo DB」を用いて、レコメンドの配信を行っています。この部分の責務は２つあり、1つ目が「ユーザが書籍をクリックしたログを取得する」ことで2つ目が「事前に計算したレコメンドリストをフロントエンドに連携する」ことになります。1つ目の「ユーザが書籍をクリックしたログを取得する」については、ユーザが書籍貸出管理システムにおいて書籍をクリックした瞬間に、クリックした書籍のID・ユーザID・日時がAPI Gatewayにパラメータとして投げられ、それをトリガーとしてLambdaが実行され、最終的にDynamo DBにログとして登録されます。こちらのログは、後述するレコメンド計算部分に用いられます。2つ目の「事前に計算したレコメンドリストをフロントエンドに連携する」については、ユーザがログインして書籍貸出管理システムを開いた際に、ユーザIDがAPI Gatewayにパラメータとして投げられ、それをトリガーとしてLambdaが実行されます。Lambdaの中では、事前にレコメンドリストが保管されているDynamo DBにユーザIDをキーとしてクエリを投げ、レコメンドリストを取得し、そのリストをAPI Gatewayに返却する、という動作を行っています。その後API Gatewayからフロントエンドに連携され、画面に表示する、というような流れになっています。次にレコメンド計算部分についてですが、計算を行うための基盤については「AWS lambda」を使用しており、その上でPythonを動作させて計算しています。lambda上でプログラムを動かすためには、直でlambdaにコードを書くか、コンテナイメージをデプロイしてコンテナ上で動作させる、というような選択肢があるのですが、今回はソースコードの管理のしやすさと新しいコードをデプロイするときの利便性を考え、ECRを用いてコンテナイメージを管理して、そこからlambdaにデプロイする、という構成にしました。そのように構成されたlambdaをEvent Bridgeを利用して、日次でバッチ実行させレコメンドリストを更新しています。そして更新した中身のレコメンドロジックについては、とりあえず初期段階ということで、単純なレコメンドのアルゴリズムである「協調フィルタリング」を用いています。簡単なので、興味がある方は調べてみてください。今回書籍貸出管理システムにレコメンドを導入した理由は、レコメンドがあったらどんな書籍が出るんだろう？という好奇心と、実は私は本業務でもECサイトにおけるレコメンドエンジン開発に取り組んでおり、そこで配信基盤を作りたいけど、前例が無かったので、とりあえず社内サイトで作ってみるのがいいのではないか、という意見があったためです。今後は時間が足りず作れなかった機能（CT,CVの計測など）に取り組んでいけたらと思っています。データサイエンティストとしてのスキルアップに繋がった書籍をご紹介！ここまでは、様々なジャンルの書籍を社員自身が会社の書籍購入補助制度によって購入でき、それを管理するシステムを社員が開発している・・・など、ARISE libraryの魅力について触れました。ここからは私が書籍購入補助制度を利用して、実際に投稿した書籍レビューをご紹介します。どちらもデータサイエンティストとしてスキルアップするのに、おすすめの書籍となります！①：機械学習を解釈する技術〜予測力と説明力を両立する実践テクニック（技術評論社）業務で扱う技術で詳細に理解していない内容に関する本を読もうと思い、ARISE libraryの本棚を眺めていたところ、この本がSHAP値に関連した本であり、SHAP値に関して深く理解していなかったので思わず手に取りました。SHAP値は目的変数に対して、各特徴量がどのような影響を与えているかを定量化・可視化するために使う指標(機械学習を解釈する指標)というようなざっくりとした理解でしたが、この本を読んで数式レベルで理解することができました。SHAP値は複雑なロジックで導出しているのですが、そのロジックの背景となる他の指標から順を追って説明し、実際にコード実装しながら分かりやすく書いてあります。またこの本を読んだおかげで、機械学習を解釈する別の指標があるということを知ることができました。②：A/Bテスト実践ガイド 真のデータドリブンへ至る信用できる実験とは（ KADOKAWA ）この本は分析業務の施策・効果検証で使用される手法の「A/Bテスト」をテーマとした本です。自分がこの本を選んだ理由としては、マーケティング施策効果を正確に検証設計する考え方を基本からきちんと理解したいと思ったためです。本の内容は、AmazonやMicrosoft、Googleなどの大手外資企業が実際に実施してきたA/Bテストの実施例をもとに検討観点(実施する上での適切な設計方法やデータ抽出方法や結果の解釈方法など)を説明するというものになっています。実例で整理されているため、内容をイメージしやすくなっているなと感じました。終わりに今回は、ARISE libraryと書籍購入補助制度を紹介しました。私は今後も継続的にこれらを利用して自己研鑽に励みつつ、社内でのARISE library活性化にも取り組んでいこうと思います。今回もお読みいただきありがとうございました！"
https://www.ariseanalytics.com/activities/report/20221214/,ワクチン接種後の体温変化をガウス過程回帰してみた,"ARISE Tech Blog2022.12.14Tweetワクチン接種後の体温変化をガウス過程回帰してみた始めにはじめまして、2022年4月に新卒として入社した Customer Analytics Division / DX Technology Unit の日比です。弊社では、より高性能なAIを開発するためのスキル研鑽として社内ブログでのアウトプットが盛んに行われています。本日はその中で私が執筆した「ワクチン接種後の体温変化をガウス過程回帰してみた」という記事について共有させていただきます。0. 概要今回はタイトルにもあるようにガウス過程回帰について自分の理解を書こうと思います。「ガウス過程ってなに？」って人も多いかと思いますが、「ベイズ最適化は知っているよ」って人は多いのではないでしょうか？ ベイズ最適化はガウス過程回帰の結果を用いてパラメーターの最適値を探すチューニング手法になっており、実は知らない間にガウス過程回帰にお世話になっているのです。少しガウス過程回帰について興味が出てきましたか？この記事では、始めに前提知識として必要な多変量ガウス分布およびリッジ回帰について説明した後、本題であるガウス過程回帰の話に移ります(リッジ回帰は重み\(\boldsymbol{w}\)がガウス分布からサンプリングされるってどういう事かをイメージ付きやすくするために執筆しているので、飛ばしてもらってもかまいません)。そして、ガウス過程回帰をもちいて実際にワクチン接種後の体温変化を回帰してみたので、その結果を共有したいと思います。1. 多変量ガウス分布1-1 概要始めに多変量ガウス分布について確認しておきます。皆さんご存知１次元のガウス分布は以下の通りです。\(\cal{N}(x | \mu,\sigma) =\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2}) \tag{1.1}\)これを\(D\)次元の多変量ガウス分布に変換すると以下です。\(\mathcal{N}( \mathbf{x} |  \boldsymbol{\mu},\Sigma) =\frac{1}{(\sqrt{2\pi})^D\sqrt|\Sigma|}\exp(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})) \tag{1.2}\)この時 \(\mathbf{x},\boldsymbol{\mu}\)は\(D\)次元ベクトルで \(\Sigma\) は \(D\times D\) の共分散行列です。共分散行列は期待値\(E[X]\) を用いて、$$\Sigma = E[\mathbf{x}\mathbf{x}^T] – E[\mathbf{x}]E[\mathbf{x}]^T \tag{1.3}$$と表す事が出来ます。名前からも察しがつくように \(\Sigma\)によって、下図のようにガウス分布の相関が変わっていきます。多変量正規分布 – 機械学習に詳しくなりたいブログより引用左から,$$\Sigma = \left( \begin{array}{c}1 & 0 \\0 & 1\end{array} \right), \left( \begin{array}{c}1 & 0.7 \\0.7 & 1\end{array} \right), \left( \begin{array}{c}1 & -0.7 \\-0.7 & 1\end{array} \right)$$としたときの２次元ガウス分布を示しています。1-2 条件付き分布さて、多変量ガウス分布のイメージ着いたところで、次に条件付き分布について考えていきます。ここでいう条件付きとは、多変量ガウス分布からサンプリングされる ベクトル \(\mathbf{x}\) のうち、いくつかの値が固定されることを意味しています。例えば、$$\mathbf{x} = \left( \begin{array}{c}\mathbf{x}_{1} \\\mathbf{x}_{2}\end{array} \right)\sim p(\mathbf{x}_{1},\mathbf{x}_{2}) = \mathcal{N}\left(\begin{array}{l}\left(\begin{array}{l}\boldsymbol{\mu_{1}} \\\boldsymbol{\mu_{2}}\end{array}\right),\left(\begin{array}{l}\Sigma_{11} & \Sigma_{12} \\\Sigma_{21} & \Sigma_{22}\end{array}\right)\end{array} \right) \tag{1-2.1}$$と \(\mathbf{x}\) を \(\mathbf{x_{1}}\) と \(\mathbf{x_{2}}\) に分割して考えたときに、\(\mathbf{x_{1}}\) の値が固定された時の \(\mathbf{x_{2}}\) が従う確率分布$$\mathbf{x}_{2}\simp(\mathbf{x}_{2}|\mathbf{x}_{1}) = \mathcal{N}( ? , ? ) \tag{1-2.2}$$で?に当てはまる平均値や共分散行列について考えてみます。イメージとしては以下の図のようになっています。 2次元ガウス分布に対して \(x_{b}\) が固定されていないときの \(x_{a}\) の確率分布 \(p(x_{a})\) と \(x_{b}\) がある値に固定された時の \(x_{a}\) の確率分布 \(p(x_{a}|x_{b})\) を比較してみると平均の位置と分散が変化していることがイメージ出来るでしょうか。条件付き正規分布(Conditional Gaussian distributions)｜改めて理解する多次元正規分布 #2 – Liberal Art’s diaryより引用これを具体的に頑張って計算してみると、結果として以下を得ます。$$p(\mathbf{x}_{2}|\mathbf{x}_{1}) = \mathcal{N}( \boldsymbol{\mu}_{2} + \Sigma_{21}\Sigma_{11}^{-1}(\mathbf{x}_{1}-\boldsymbol{\mu}_{1}) , \Sigma_{22} – \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12} )\tag{1-2.3}$$具体的な証明は示しませんが、\(\mathbf{x}_{1}\) が決まることで平均の位置が \(\Sigma_{21}\Sigma_{11}^{-1}(\mathbf{x}_{1}-\boldsymbol{\mu}_{1})\) だけずれ、 分散が \(\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}\) だけ小さくなる事が分かります。2. 線形回帰とリッジ回帰次に、リッジ回帰について知っておいた方がガウス過程回帰のイメージが付きやすいと思うのでリッジ回帰について説明します。また、リッジ回帰は線形回帰が解けない(逆行列が存在しない)場合などに有用な手法なので、線形回帰についても簡単に復習しておきます。2-1 線形回帰線形回帰とは、例えば$$\begin{array}{l}\hat{y} = w_0 + w_1x + w_2x^2 \\\hat{y} = w_0 + w_1x + w_2x^2 + w_3\sin{x}\end{array}$$のように係数 \(\boldsymbol{w} = (w_0,w_1,w_2 ..)^T\) に対して線形になるような式を考えるモデルでした。より一般的には、$$\hat{y} = w_0 + w_1\phi_1(x) + w_2\phi_2(x) + w_3\phi_3(x) + … + w_H\phi_H(x)\tag{2-1.1}$$のように任意の関数\(\phi_n(x)\)で書くことが出来ます。ここで\(\phi_n(x)\)を基底関数と呼ぶことにします。これをN個のデータかつ \(x\) がベクトルである時を考えると、$$\left( \begin{array}{c}\hat{y}_1 \\\hat{y}_2 \\\vdots \\\hat{y}_N\end{array} \right) = \left( \begin{array}{c}\phi_0(\mathbf{x}_1) \quad \phi_1(\mathbf{x}_1) \quad \phi_2(\mathbf{x}_1) \quad \cdots \quad \phi_H(\mathbf{x}_1) \\\phi_0(\mathbf{x}_2) \quad \phi_1(\mathbf{x}_2) \quad \phi_2(\mathbf{x}_2) \quad \cdots \quad \phi_H(\mathbf{x}_2) \\\vdots \\\phi_0(\mathbf{x}_N) \quad \phi_1(\mathbf{x}_N) \quad \phi_2(\mathbf{x}_N) \quad \cdots \quad \phi_H(\mathbf{x}_N)\end{array} \right)\left( \begin{array}{c}w_0 \\w_1 \\\vdots \\\vdots \\w_H\end{array} \right)\tag{2-1.2}$$ここで、\(\phi_0(\mathbf{x}) \equiv 1\) と定義しています。また、式のベクトルおよび行列を左から \(hat{\mathbf{y}}, \Phi, \boldsymbol{w}\) と表記することとします。線形回帰では、あるデータセット \((\mathbf{Y},\mathbf{X})\) に対して \(\boldsymbol{w}\) は以下の式で与えられます。$$\boldsymbol{w} = (\Phi^T\Phi)^{-1}\Phi^T\mathbf{Y}\tag{2-1.3}$$つまり、\((\Phi^T\Phi)\) の逆行列があればこの問題は解けるということです。2-2 正則化としてのリッジ回帰線形回帰では\((\Phi^T\Phi)\) の逆行列があればうまく回帰をすることが出来るとわかりました。ただ、必ずしも逆行列があるとは限りません。例えば基底関数の数 \(H\) が データ数 \(N\) よりも少ない場合は \((\Phi^T\Phi)\) のランクが \(H\) より小さくなるため逆行列が存在しません。リッジ回帰ではいわゆるL2ノルムを考えることでこの問題を回避します。回帰ではよく二乗誤差の最小化を考えて \(\boldsymbol{w}\) を求めます。この際、二乗誤差に加えて\(\boldsymbol{w}\) の大きさを加える正則化を考えます。すなわち、$$Err = (\boldsymbol{Y} – \boldsymbol{\Phi}\boldsymbol{w})^2 + \alpha\boldsymbol{w}^T\boldsymbol{w} \tag{2-2.1}$$を考えます。これを\(\boldsymbol{w}\)で微分し、その値が０となる\(\boldsymbol{w}\)について解くと以下が得られます。$$\boldsymbol{w} = (\Phi^T\Phi + \alpha\mathbf{I})^{-1}\Phi^T\mathbf{Y}\tag{2-2.2}$$結果的には、線形回帰の解の時と比べて \((\Phi^T\Phi)\) に \(\alpha\mathbf{I}\) が足されたことが分かります。ここで \(\alpha\) はハイパーパラメータで、\(\mathbf{I}\) は単位行列です。この \((\Phi^T\Phi + \alpha\mathbf{I})\) には必ず逆行列があるのでうまく解を求めることが出来ます。2-3 ベイズ的観点で見るリッジ回帰正則化として導入した \(\alpha\boldsymbol{w}^T\boldsymbol{w}\) ですが、実は確率モデルから自然に導くことが可能です。ある\(N\)点のデータセット \( (Y,\Phi(X)) \) が得られたときに、係数が \(\boldsymbol{w}\)であった確率を示す確率密度関数を \(p(\boldsymbol{w}|Y,\Phi(X))\) とします。回帰は \(p(\boldsymbol{w}|Y,\Phi(X))\)が最も大きな値を取る \(\boldsymbol{w}\) を計算することと同じになるので、 \(p(\boldsymbol{w}|Y,\Phi(X))\)  が最大になる条件を考えてみましょう。\(\boldsymbol{w}\) の条件付き確率密度関数はベイズの定理より以下のように変換できます。$$p(\boldsymbol{w}|Y,\Phi(X)) = p(Y|\boldsymbol{w}, \Phi(X)) p(\boldsymbol{w})\tag{2-3.1}$$なお、分母成分は定数であり、今回の問題には影響しないので省略しました。また、\(p(\boldsymbol{w}|\Phi(X)) = p(\boldsymbol{w})\)  を用いています。\(Y\)  の条件付き分布については、回帰でえられる \(\Phi(X)\boldsymbol{w}\) にガウス分布するノイズ \(\epsilon\)  が乗っかった値が \(Y\)  になると考えると,$$p(Y|\boldsymbol{w}, \Phi(X)) = \mathcal{N}(Y|\Phi(X)\boldsymbol{w}, \sigma^2\boldsymbol{I}_N)\tag{2-3.2}$$と書くことが出来ます。\(p(\boldsymbol{w})\)  に関してもガウス分布を仮定しましょう。すなわち、$$p(\boldsymbol{w}) = \mathcal{N}(\boldsymbol{w}|0, \lambda^2\boldsymbol{I}_{H})\tag{2-3.3}$$です。なお、\(\boldsymbol{w}\) の各成分は完全に無相関なガウス分布からサンプリングされるとしています。それでは、いつものように log を取って具体的に最大化すべき式を導出します。$$\log{p(\boldsymbol{w}|Y,\Phi)} = -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(y_{n}-\boldsymbol{\phi}(\mathbf{x}_{n})\boldsymbol{w})^2 – \frac{1}{2\lambda^2}\sum_{i=1}^{H}w_{i} + C \\\propto – \{ (\boldsymbol{Y}-\Phi\boldsymbol{w})^2 + \alpha\boldsymbol{w}^T\boldsymbol{w}\}\tag{2-3.4}$$得られた結果と、前節で導入したリッジ回帰の式(2-2.1)を比較すると完全に一致しています。すなわち、正則化として\(\boldsymbol{w}\) の大きさを加えることと、\(\boldsymbol{w}\) を多変量ガウス分布からサンプリングされる値として扱う事は同じ結果を導くのです。ここで重要な点として、\(\boldsymbol{w}\) が多変量ガウス分布からサンプリングされた値であるという事を覚えておいて、いよいよガウス過程回帰について見ていきましょう。3. ガウス過程回帰ようやく本題であるガウス過程回帰に移ります。3-1 ガウス過程ここでも、式(2-3.3) と同様に\(\boldsymbol{w}\) はガウス分布からサンプリングされるものとします。あるデータセット\((\boldsymbol{Y},\boldsymbol{X})\) が与えられると、\(\Phi(\boldsymbol{X})\)  は定数と考えられるので、\(\boldsymbol{Y}\) はガウス分布にしたがう\(\boldsymbol{w}\) を \(\Phi\) で線形写像したものと考えられます。したがって\(\boldsymbol{Y}\)  もガウス分布に従います。この時の期待値と共分散行列(式(1.3))を計算すると以下のようになります。$$E[\boldsymbol{Y}] = E[\Phi\boldsymbol{w}] = \Phi E[\boldsymbol{w}] = 0 \tag{3-1.1}\$$$$\Sigma = \lambda^2\Phi\Phi^T \tag{3-1.2}\$$すなわち,\(\boldsymbol{Y}\) が従うガウス分布は以下のように表されます。$$\boldsymbol{Y} \sim \mathcal{N}(0,\lambda^2\boldsymbol{\Phi\Phi}^T) \tag{3-1.3}$$ここで注目すべき点は、\(\boldsymbol{w}\)  は期待値が取られて消去されていることです。これによって、例え基底関数の数が高次、あるいは無限次元であったとしても、具体的に\(\boldsymbol{w}\) を求める必要はなく、\(\lambda^2\boldsymbol{\Phi\Phi}^T\) によってのみ\(\boldsymbol{Y}\) が従う分布が表現されます。このように、どのような\(N\)個の入力\(\boldsymbol{X}=(\mathbf{x_{1}}, …, \mathbf{x_{N}})\) についても、対応する\(\boldsymbol{Y} = (y_1, … y_N)\)  が多変量ガウス分布に従う時、\(\boldsymbol{X}と\boldsymbol{Y}\) はガウス過程に従う、と言います。なお、 \(\boldsymbol{Y}\) の平均値が0になってしまっていますが、あらかじめ \(\boldsymbol{Y}\) から \(\boldsymbol{Y}\) の平均値を引いておけば必ず0になるので、そのような処理を行っているものとして今後の議論を進めます。3-2 カーネルトリック次に\(\boldsymbol{\Phi\Phi}^T\) の計算について考えます。単純な方法として、任意の基底関数を要素に持つベクトル \(\phi(\mathbf{x}) = (\phi_1(\mathbf{x}), … , \phi_H(\mathbf{x}))\)  を定めて計算するように思うかもしれませんが、その必要はありません。実際にほしい値は、基底関数ベクトルの内積である以下の式の値です。$$k_{nn’}(\mathbf{x}_{n}, \mathbf{x}_{n’}) = \phi(\mathbf{x}_{n})^T\phi(\mathbf{x}_{n’}) \tag{3-2.1}$$したがって、\(\phi(\mathbf{x})\) を明示的に与えるのではなく、この内積の値を与える関数を与えてやれば良いのです。その関数の事をカーネル関数と呼び、ガウスカーネル(RBFカーネル)、線形カーネル、指数カーネルなど様々な種類のカーネルが存在します。また、このように\(\phi\) を直接表現することを避けてカーネルだけで内積を計算する事をカーネルトリックと言います。一つ例としてガウスカーネルは以下の式で与えられます。$$k(\mathbf{x}, \mathbf{x’}) = \theta_1 \exp(-\frac{|\mathbf{x}-\mathbf{x’}|^2}{\theta_2}) \tag{3-2.2}$$実はこのカーネルの元になっている基底関数を考えると、平均の値が異なる無限個のガウス分布で回帰するモデルになっています。本来の線形回帰であれば無限個の基底関数で回帰することなどできませんが、カーネルを使うことによって非常に高い表現が得られたのです。まさにトリックですね！3-3 予測値の計算最後に予測値の計算ですが、1-2章で説明した式を用いれば、既にあるデータセットを用いた条件付き多変量ガウス分布を計算できますし、あたらしい\(x\) に対する \(y\) の予測値(正しくは新しい\(x\) における\(y\) 方向の分布)も計算することが出来ます。4. ワクチン接種後の体温変化をガウス過程回帰基本的なガウス過程回帰のプロセスが理解できたので、実際に使ってみました。今回は、2回目のワクチン接種をした際の体温変化を回帰してみました。使用したツールはpythonでbayse_optというモジュールのBaysianOptimaizationという関数を使用しています。 青い点が測定点、オレンジのバンドが\(1\sigma\) を表しています。赤い線はロキソニンを服用した時の時間です(面白い形にしたくて、体温上がるまで我慢したのはここだけの話)。非常に複雑な形をしていますが、それっぽく回帰出来ている事が分かります。スゴイですね!!5. 終わりにガウス過程回帰のイメージが理解できたでしょうか？ 結局のところ、ガウス過程回帰では各データ点のyの値はある相関を持った多変量ガウス分布からサンプリングされた値であり、その相関はx方向の値とカーネル関数を用いて定義されているのでした。色々省略して説明したところもあるので、少しわかりづらかったかもしれませんが、これで興味を持った人は是非勉強してみてください。基本的にこの記事は参考文献に記載した本を参考に執筆しているので、詳細を知りたい人はこちらをご購入下さい。また、ベイズ最適化はガウス過程が分かってしまえばイメージは割とすぐに理解できるかなと思います。以上ガウス過程回帰を使ってみた話でした。次回ワクチンを打つ機会があれば是非ガウス過程回帰を試してみてください。寝ている間に体温がどこまで上がったのかわかるかもしれません。6. 参考文献ガウス過程と機械学習 (機械学習プロフェッショナルシリーズ) | 持橋 大地, 大羽 成征 |"
https://www.ariseanalytics.com/activities/report/20230221/,暗号と情報セキュリティシンポジウム（SCIS2023) 発表報告,"ARISE Tech Blog2023.02.21Tweet暗号と情報セキュリティシンポジウム（SCIS2023) 発表報告はじめにARISE analyticsの近藤です。弊社では、KDDIグループを支えるためのAI技術開発を行っており、通信セキュリティ技術へのAI導入もミッションの一つです。今回、技術開発成果の共有を目的とし、2023年1月24日～1月27日に開催された暗号と情報セキュリティシンポジウム（SCIS2023）で発表してきました（発表に関する情報はこちらから確認できます）。本記事では、SCISの概要とともに、弊社から発表した内容を簡単に紹介いたします。SCIS2023の概要SCIS2023は、電子情報通信学会 情報セキュリティ研究専門委員会 （ISEC研）が主催のシンポジウムです。セキュリティ関連のシンポジウムとしては、コンピュータセキュリティシンポジウムに並んで国内最大級のシンポジウムになります。もともと暗号を中心に扱っていたシンポジウムですが、徐々に領域を広げていきセキュリティ領域も扱うようになりました。近年はAIセキュリティ・心理学・ネットワークセキュリティなどの新規テーマも扱われております。バナー（引用：SCIS2023 暗号と情報セキュリティシンポジウム）今回はリーガロイヤルホテル小倉＠北九州市で開催されました。余談ですが、最近は九州地方でセキュリティ系のシンポジウムが頻繁に開催されています。前回報告したAsiaCCS2022＠長崎市に加え、今後の開催ではCSS2023＠福岡市、SCIS2024@長崎市が予定されています。10年に1度の大寒波で、現地は吹雪でした小倉駅周辺には歓迎のメッセージがたくさん掲げられていました今回の発表内容今回、弊社からは以下のタイトルで発表しました。タイトル：教師なしGNNによるIoTデバイスの異常通信検知の検討論文著者：近藤 真暉（株式会社 ARISE analytics）、奥井 宣広（株式会社 KDDI総合研究所）本発表は、IoTデバイスの通信データを対象とした異常検知を目的に、教師なしグラフニューラルネットワークを提案したものになります。以下、概要を簡単に説明させていただきます。背景・関連技術本発表は、IoTデバイスの通信データを対象とした異常通信検知を目的としたものです。ここでいう異常通信とは、Miraiなどのマルウェア等によるサイバー攻撃によって発生する正常とは異なる通信を指します。これらを自動検知することで、セキュリティ対策を取りやすくすることが目的です。ここで、機械学習を用いた異常通信検知はラベル情報を用いる教師あり学習とラベル情報を用いない教師なし学習に分かれます。しかし、サイバー攻撃は常にアップデートされるため、未知の通信にも対応できる教師なし異常通信検知の重要性が高まっています。一方、近年グラフニューラルネットワーク（GNN）が注目を集めており、異常通信検知への適用で性能向上する事例も報告されはじめています（例：参考文献[1]）。しかし、GNNを用いた異常通信検知は教師ありの手法がベースであり未知の異常に対して十分な検知精度が得られない可能性がありました。そこで、本発表では、教師あり異常通信検知で性能が報告されているGNNを、教師なし異常通信検知に適用できるようにし、活用の幅を広げることを目指します。提案手法本手法は、GNNを用いた教師あり異常通信検知であり、IoTデータのオープンデータで高い性能が報告されているされているE-GraphSAGE[1]をベースに、Metric Learningを導入することで教師なしGNNを実現します。E-GraphSAGEはその名の通り、GraphSAGE[2]を拡張した手法です。ノードとエッジを入れ替える変換処理により、通信データをノードに格納できるようになるという利点があります（具体的な処理は手順①に記載いたします）。手順①通信データからのグラフ構築まず、通信データをグラフ構造に変換します。今回は、[1]と同様の手法を用います。まず、ホストとなる通信デバイスをノードに、ホスト間の通信をエッジとみなし、グラフ構造に変換します。これにより、グラフを用いた通信表現が可能になります。ただし、一般的なGNNはノードに特徴が含まれていることを前提としています。そのため、エッジに格納されている通信データを活用できません。そこで、右図のようにノードとエッジを入れ替えることで、通信データを特徴として格納できるようになります。これにより、通信量といった通信データをGNNで処理することが可能になります。手順②ノード埋め込みモデルの学習ここからはオリジナルの手法になります。本手順では、ノードを特徴ベクトルに変換するノード埋め込みモデルの学習を行います。ポイントとなるのは、Metric Learningの導入です。Metric Learningは、ラベル情報を用いない教師なし学習による特徴表現学習手法であり、アンカー・ポジティブ・ネガティブを用います。アンカーに対し、ポジティブは近くなるよう、ネガティブは遠くなるように損失関数を設計し、学習を行うことで特徴表現学習が実現できます。学習にあたり、ノードからk-hopサンプリングによりサブグラフを構築して特徴空間に埋め込むことを考え、アンカーノードから構築されたサブグラフとポジティブノードから構築されたサブグラフが似るように、ネガティブノードから構築されたサブグラフが似ないようにノード選択を行います。今回は、ランダムに選択されたアンカーノードに対し、隣接ノードをポジティブノード、遠くにあるノードをネガティブノードとして選択しました。手順③異常検知モデルの学習ノードの埋め込みにより得られた特徴を用い、異常検知モデルを学習します。組み合わせる異常検知モデルはどのようなものでも良いのですが、今回はMetric Learningと同様に距離計算ベースの手法であるkNNを採用しました。なお、埋め込みモデルの学習データと異常検知モデルの学習データは同一のものを用いています。実験オープンデータ（UNSW-NB15/The TON_IoT）を用い、提案手法の評価を行いました。ここでは正常通信のみで構成された訓練データを用い、ノード埋め込みモデルおよび異常検知モデルの学習を行っています。また、評価は異常通信を含むテストデータを利用しています。データセット異常通信の種類数正常通信の割合（％）既存特徴の次元※1訓練データの件数（正常通信のみ）検証データの件数（異常通信含む）テストデータの件数（異常通信含む）UNSW-NB15[3]996.8343485,0015,000210,000The TON_IoT[4]965.0739316,0435,000210,000表：データセットの内訳実験① 埋め込み特徴の可視化による特徴表現学習の定性評価ノード埋め込みモデルにより得られた特徴の可視化結果（左がUNSW-NB15、右側がTON_IOT）。 青色が正常通信、それ以外の色が異常通信を示す。ノード埋め込みにより得られた特徴を、t-SNEで可視化しました。青色が正常通信、それ以外の色が異常通信を示します。可視化結果を見ると、同じ色によるまとまりが確認できており、提案手法により正常通信と異常通信を分離できるような特徴表現が学習できていることがわかります。実験②オープンデータを用いた通信異常検知の定量評価表：実験結果2値分類による通信異常検知の結果です。データセットごとのROCAUCと、2つのデータセットの平均を取った単純平均、2つのデータセットの差分をそれぞれ示しています。今回の実験では、通信データの特徴表現として、①ノード埋め込み前の特徴（RAW）、②提案手法である埋め込み後の特徴（Uns. E-GraphSAGE）を用いました。また、特徴表現に対して組み合わせる異常検知アルゴリズムとして、提案手法であるkNNに加えAE（オートエンコーダ）、OCSVM（One-Class SVM）も検証します。データセットごとにROCAUCを確認すると、UNSW-NB15ではRAW（AE）が0.981、The TON_IoTではRAW（kNN）が0.999と最も数字が良いことがわかりました。しかし、平均を確認すると、RAW（AE）が0.879、RAW（kNN）が0.978でした。また、差分もRAW（AE）が0.204、RAW（kNN）が0.042でした。一方、提案手法であるUns. E-GraphSAGE（kNN）はUNSW-NB15が0.976、The TON_IoTが0.990であるものの、平均が0.983、差分が0.014であることが確認できました。これらの結果は、本実験の範囲において、ノード埋め込み前の特徴（RAW）と異常検知アルゴリズムを組み合わせた場合、データセットごとのばらつきが大きく汎用性が低下していることを示唆しています。一方、提案手法はばらつきが少なく汎用性が向上していることが確認できました。まとめ本発表では、IoT通信データを対象とした異常検知のために、教師なしのGNNを提案しました。本発表のcontributionを以下に整理します。Metric Learningを用いた教師なし学習によるグラフの埋め込み手法の提案異常通信検知向け公開データセットを用い、本手法により汎用的な性能が得られていることを確認今後は提案手法を洗練させ、より使いやすい方法にアップデートしていきます。おわりに本記事では、SCISの概要とともに、弊社から発表した内容を簡単に紹介しました。本発表の内容は弊社で取り組んでいる技術開発成果の一部になります。弊社では本テーマ以外にもKDDIの通信データを対象とした技術開発に取り組んでいます。今回紹介した内容や、その他記事で触れている弊社の取り組みに興味がございましたら、ぜひお声がけください。採用 | 株式会社ARISE analytics（アライズ アナリティクス）参考文献[1] Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Marcus Gallagher, and Marius Portmann. E-GraphSAGE: A Graph Neural Network based Intrusion Detection System for IoT. NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium, 2022.[2] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs. Advances in neural information processing systems, 30, 2017.[3] Nour Moustafa and Jill Slay. UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set). In 2015 Military Communications and Information Systems Conference (MilCIS), pages 1–6, 2015.[4] Abdullah Alsaedi, Nour Moustafa, Zahir Tari, Abdun Mahmood, and Adnan Anwar. TON_IoT Telemetry Dataset: A New Generation Dataset of IoT and IIoT for Data-Driven Intrusion Detection Systems. IEEE Access, 8:165130–165150, 2020.おまけ九州地方はおいしいごはんの宝庫ですが、小倉駅周辺にも沢山ありました。また社外発表でおいしいごはんを食べられるよう、技術開発に努めてまいります。カワハギの刺身北九州市名物の鉄鍋餃子"
https://www.ariseanalytics.com/activities/report/20230308-2/,傾向スコアの学習に入れてはいけない変数,"ARISE Tech Blog2023.03.08Tweet傾向スコアの学習に入れてはいけない変数はじめまして、 Customer Analytics Division / DX Technology Unit の渡邉です。普段は主に通信事業者の基地局設備のデータを扱う業務を行っています。今回は「傾向スコアの学習に入れてはいけない変数」について、ご紹介します。サマリランダム化比較試験 (RCT:randomized controlled trial)が行えない場合、因果効果を得るために、傾向スコアを用いる手法が用いられることがあります。しかし、Judea Pearl流の構造方程式を用いた因果推論によれば、傾向スコアの学習に含めるとバイアスを生む変数の条件が示されていますが、あまりこの辺を注視されていないような気がしています。そこで、文献を調査し、バイアスを生む条件を理論的に示しました。その後、サンプルデータにおいても実験し、バイアスが存在することを確かめました。理論編Judea Pearlの因果推論について基礎をまとめます。1.構造的因果モデル(SCM:Structural Causal Model)因果には方向が存在していると考えられるでしょう。例えば、ある薬の投与が、病気からの回復に効果があるかどうか調べているとします。性別が、この効果に影響を及ぼすとしましょう。このとき、薬の投与が性別に影響を与えたり、病気からの回復が性別に影響を与えることは無いでしょう。この場合は、次のようなSCMが考えられます。ここで、投薬の有無をX、回復をY、性別をZとして\(V=\{X, Y, Z\},U=\{U_X, U_Y,U_Z\}, F=\{f_1, f_2\}\)\(X=f_1(Z, U_X) ,Y=f_2(X, Z, U_Y), Z=U_Z\)と表すことができます。ここで、ＶはSCMに登場する変数集合、Uは各変数の誤差変数の集合とします。「投薬は、性別と観測できない影響によって決まる」「回復は、投薬と性別と観測できない影響によって決まる」ということを表しています。2.因果グラフ上述したSCMをグラフィカルに表現します。右辺から左辺に矢印を引くだけです。改めて、「投薬は、性別と観測できない影響によって決まる」「回復は、投薬と性別と観測できない影響によって決まる」ということがグラフィカルに理解できます。3.SCMに基づく介入の定義ランダム化試験の何がうれしかったのかをおもいだしてみると、未観測の変数が介入変数に与える影響をなくすことができる点でした。このことを今回のSCMで表すと、例えば投薬をする場合は\(V=\{X, Y, Z\},U=\{U_X, U_Y,U_Z\}, F=\{f_2\}\)\(X=1 ,Y=f_2(X, Z, U_Y), Z=U_Z\)と書き換えられることになります。投薬はもはや性別と観測できない変数の影響を受けていません。介入後のSCMを因果グラフで表すと次のようになります。4.平均因果効果の導入上で述べた介入の操作を代数的に表します。介入前のグラフに基づいた確率分布をPとして、\(P(Y=y|do(X=x))\)と表します。do演算子と呼びます。例えば、上記の例では「全員に薬を投与した場合、全員に薬を投与しなかった場合の回復する確率の差」が知りたい因果効果として妥当でしょう。これを平均因果効果(ACE : average causal effect)と呼ぶことにして、次の式で表します。\(ACE=P(Y=1|do(X=1))-P(Y=1|do(X=0))\)5.因果効果の推定問題は、do演算子が含まれていると、その確率をデータから推定することができないことです。これを解決するために、介入後のグラフに基づいた確率分布\(P_m\)とおいて、\(P\)と\(P_m\)との共通点を探ってみると、以下に気が付きます。\(P(Z=z)\)は介入前後で変化しない条件付き確率\(P(Y=y|Z=z, X=x)\)は介入前後で変化しないよって次の式変形が成り立ち、データから因果効果を推定できることがわかります。\(P(Y=y|do(X=x))\)\(=P_m(Y=y|X=x)\)　（定義による）\(=\sum_z P_m(Y=y|X=x, Z=z)P_m(Z=z|X=x)\)\(=\sum_z P_m(Y=y|X=x, Z=z)P_m(Z=z)\)　（今回のSCMによる）\(=\sum_z P(Y=y|X=x, Z=z)P(Z=z)\)(上記で挙げた\(P\)と\(P_m\)との共通点による)このとき、「変数Zで調整する」という言葉遣いをします。上記を一般化すると、介入変数のすべての親で調整を行うとACEが推定できることが知られています。次の定理としてまとめます。グラフ\(G\)において、\(X\)の親を\(PA\)とすると、\(X\)が\(Y\)に及ぼす因果効果は\(P(Y=y|do(X=x))=\sum_z P(Y=y|X=x, PA=z)P(PA=z)\)で与えられる。6.調整してはいけない変数上で述べた基準は、バイアスなくACEを推定するために必要な変数集合を与えてくれるものでした。必要な変数集合だけでなく、余計な変数も調整するとどうなるでしょうか。つまり、上記定理において、Xの親だけでなく他の変数も調整してしまったらどうなるでしょうか。実は、調整するとバイアスを生む変数のタイプが知られています。中間変数介入変数から目的変数に伸びる矢印の途中にある変数例：次の図のZコライダー介入変数からその変数に矢印が伸びていて、目的変数からその変数に矢印が伸びている例：次の図のZ7.共変量同値と傾向スコア傾向スコアで調整することは、それを推定するために学習した変数すべてで調整することと同値であることが知られています。共変量同値という定理から導かれます。8.今回の本題やっと本題です。。。次の2点を述べました。・「因果効果を推定するためには介入変数のすべての親を含めて調整をしなければならない。その際、中間変数やコライダーを含めるとバイアスを生む。」・「傾向スコアで調整することは、それを推定するために学習した変数のすべてで調整することと同値である。」∟このことから、「傾向スコアマッチングをするとき、傾向スコアの学習に中間変数やコライダーを含めるとバイアスを生んでしまう。」ということが導かれます。実際にどうなるか確かめるため、実験を行いました。実践編サマリ・因果効果が0.095であるデータセットを作成した。∟今回は人工データセットであるため、介入変数のすべての親がわかります。・介入変数のすべての親と傾向スコア学習に含み、傾向スコアマッチングを行ったところ因果効果が0.0067と推定された。∟明らかに過小推定。・介入変数のすべての親を傾向スコア学習に含み、傾向スコアマッチングを行ったところ因果効果が0.081と推定された。∟多少過小推定だが、上と比較すればかなり真の因果効果に近い。∟中間変数を傾向スコアの学習に含まない方がいいということが分かった！ノートブック・人工のデータを作成し、それに対して傾向スコアマッチングを行い因果効果を推定しています。・データの設定としては、あるホテルにおいて、予約と異なる部屋を割り当ててしまうことが予約のキャンセルを発生させるメカニズムをイメージしています。∟Booking Cancelled:その予約がキャンセルされたか（目的変数）∟Different Room Assigned:予約と異なる部屋を割り当てたかどうか（介入変数）∟is_claimed:クレームが行われたかどうか（中間変数）∟Booking Changes:顧客が予約を変更したか∟hotel:ホテルの種類・実行したノートブックはこちらです！Google Colaboratoryまとめと因果推論を行う際の示唆・傾向スコアの学習に中間変数を含むと、含まない場合より、因果効果の推定誤差が大きくなりました。∟理論編で確認したことにより、中間変数でなくコライダーでも同様の結果にあるはずです。∟また、傾向スコアの学習のみならず、MetaLeaner系でも同様の結果になるはずです。・よって、因果構造を正しく捉えられているかどうかが重要であることが示唆されます。・ビジネスのメカニズムに対し深い洞察を持つことは、因果推論を行う際の何よりも重要な要素なのではないでしょうか。参考文献理論編における参考にした文献を紹介します。1,2,3:「入門統計的因果推論」,Judea Pearl (著), Madelyn Glymour (著), Nicholas P. Jewell (著), 落海 浩 (翻訳),朝倉書店,2019, 1章4,5:「入門統計的因果推論」,3章6: 中間変数について「因果推論の科学　「なぜ?」の問いにどう答えるか」 ,ジューディア・パール (著), ダナ・マッケンジー (著), 松尾 豊 (解説), 夏目 大 (翻訳),文藝春秋,2022,p237「統計的因果推論の理論と実装」 ,高橋 将宜(著),共立出版,2022,p104コライダーについて「因果推論の科学　「なぜ?」の問いにどう答えるか」, p246, p2827:「構造的因果モデルの基礎」 ,黒木 学 (著),共立出版,2017,p104"
https://www.ariseanalytics.com/activities/report/20230317-2/,メディアはChatGPTをどのように受け止めているか？,"ARISE Tech Blog2023.03.17TweetメディアはChatGPTをどのように受け止めているか？昨年11月にリリースされてから，ChatGPTが話題です．Web検索の概念を根底から覆すのでは，というような意見もあり，GoogleやMicrosoftなど大手IT企業がこぞって類似したサービスを展開し始めました．ChatGPTの性能はもちろん気になるところですが，今回の騒ぎ (？) のすごいところは，情報拡散のスピードです．ChatGPT自体が史上最速で月間アクティブユーザ100万人を突破したそうです．このスピード自体は，ChatGPTの話題性もさることながら，現代の情報拡散の速度が上がっていることが原因のような気もします．100万ユーザ獲得にかかった時間 (From: Statista)ChatGPTに関する動きは，IT業界ではたまにある大きなブームだと思います．技術的なブレークスルーが発生したときに世間がどのような反応を示すのかを観察する絶好の機会でもあります．世間と言いつつ，観察できるのはWeb記事を中心としたメディアの反応ですが，今回のブログではChatGPTがリリースされてからおよそ3ヶ月の間に，メディアがどのような反応を示したのかということを通して，情報拡散の様子を観察してみようと思います．Dataset今回は，たまたま個人的にChatGPTに関するWebニュースを収集していたので，それを元にしてYahoo!ニュースの記事で補いつつ，全部で568本のWeb記事を対象に分析を実施しました．(目についた記事は全て拾うようにしていますが，網羅性はありません…)記事数568本収集期間2022.11.30 ~ 2023.2.25対象メディアYahoo!ニュース, ITMedia NEWS, GIGAZINE, ギズモード etc.BERTopicによる記事分類まずは記事の傾向を把握するために，BERTopicを使って記事全体を分類してみました．file5つのクラスターに分かれました．それぞれ，ChatGPT全般に対する評価や意見などの記事ChatGPTとGoogleの関係やBardに関する記事ChatGPTとMicrosoftの関係やBingに関する記事スタートアップ企業など市場関連に着目した記事というクラスターのようです.チャットボットに対する中国の反応に関する記事というクラスターのようです．各クラスターについて，目立ったワードを見てみます．fileトピック0〜3にはいずれも上位に AI というキーワードが入っています． ChatGPTが人工知能と関連づけられていることがわかります．Microsoft，Googleのクラスターでは検索・チャットというキーワードも入っています．ChatGPTの使い方としてチャット風の検索という使い方が多いのでしょうか．確かに，これまでのGoogle検索などでは主に複数の単語を入力する検索方法が主流でしたが，ChatGPTのようなチャットボットが検索機能と融合すると，検索方法が根底から覆りそうです．また当然かもしれませんが，ChatGPTを開発したOpenAIの市場価値に関する記事も目立ちました．ChatGPTクラスターの細分類ChatGPTそのものに関する記事はクラスター0にまとまっていますが，もう少し詳細を知りたいところです． そこで，Topic 0に属する記事をさらに分類してみます．Topic 0の記事に共通する単語 (AIやChatGPT, OpenAIなど) を除外し，さらにBERTopicで記事を細かく分類してみました．filefile今度は4つのクラスターに分かれました．それぞれ，文章生成とAIの利用方法や研究に関する話題生成系のモデル・技術と市場に関する話題ChatGPTとサイバーセキュリティに関する話題ChatGPTのサブスクに関する話題というわかりやすいクラスターになっています．各クラスターにおける記事をいくつかサンプリングして見てみます．Topic-0 文章生成とAIの利用方法や研究に関する話題人間の質問に答えるAI「ChatGPT」がすごいと話題に自然な文章に「やばい」と驚きの声 @2022.12.2ChatGPTが作り出す「もっともらしい」文章はウェブを破壊するか @2022.12.6ChatGPT作のニセ論文要旨は1/3の割合で査読者に本物だと思わせる @2023.1.13ChatGPTがMBA試験に合格！ でも小6の算数は苦手みたい @2023.1.26ChatGPTを活用した就職支援サービスを開始 @2023.2.14AI新時代の日本の戦略 @2023.2.17MIT Tech Review: 解説：空前のブーム「ChatGPT」はどこから生まれたのか？ @2023.2.24ChatGPTを試してみたという記事やChatGPTによる文章生成の精度に関する記事が多かったと思います．また，ChatGPTがなぜ，どのようにして生まれてきたのか，その影響はどこまで広がるのか，といった学術分野からの分析もありました．特に，”AI新時代の日本の戦略 @2023.2.17” は東大の松尾研が出している分析結果で，わかりやすいと思います．オススメです．Topic-1 生成系のモデル・技術と市場に関する話題2022年はAI技術の「社会実装」が躍進した1年…ChatGPT、神絵AI、自動運転からロボットまでを振り返る @2022.12.26ChatGPTの衝撃マイクロソフトが1兆円を投資 驚異の進化と迫るリスク @2023.2.3ChatGPTで一部の仕事が失われても、自分の仕事は大丈夫-ＭＬＩＶ調査 @2023.2.6加熱する ジェネレーティブAI 開発がもたらす有害な影響のリスク：「テクノロジーをどのように運用すべきか、を考慮しなければいけない」 @2023.2.21最近は画像・言語などさまざまな生成系のAIが生まれてきているので，それに対する市場の反応や，いわゆる「AIに仕事を奪われる系」の記事が目立ちました．ChatGPTは簡単なコードであれば一瞬で生成してくれるそうなので，ワレワレ分析者も他人事ではないかもしれません…Topic-2 ChatGPTとサイバーセキュリティに関する話題サイバー犯罪者が早速、ChatGPTを悪用3つの事例を紹介 @2023.1.19ChatGPT、マルウェアも書ける、書けちゃう @2023.1.25IT専門家の51％が「ChatGPT」を使うサイバー攻撃に警戒 @2023.2.7ChatGPTがサイバーセキュリティにもたらす影響 @2023.2.13アングラ犯罪者らが、ChatGPTの悪用防止機能を回避してフィッシングメールを作成 @2023.2.17ChatGPTとサイバーセキュリティに関する記事が出てくることは予想通りで，実際に綺麗なクラスターを作るほどに記事が出てきています． マルウェアを自動生成できたという話から，判別しにくいフィッシングメールを作成する事例まで，すでにさまざまな悪用方法が出てきているようです． ARISEでも最近になって通信・セキュリティに関する案件が増えてきており，生成系AIの発展に伴ってセキュリティ領域の様子が大きく変わる可能性があると思わされるものでした．(余談ですが「IT専門家の51%が警戒」とありましたが，残りの49%はどのような意見を持っているのか気になりました．)Topic-3 ChatGPTのサブスクに関する話題ChatGPT有料版は月額42ドル？ 「これは破格」の声も @2023.1.25ChatGPTの有料サブスク版「Plus」、月額20ドルで提供開始 @2023.2.2ChatGPT、月間アクティブユーザー数が1億人突破 @2023.2.3これについては，まぁ，気になりますよねwChatGPTに関するイベントとメディアの反応タイミング最後に，ChatGPTに関する主なイベントとそれに対するメディアの反応を時系列で眺めてみます．fileほぼ指数関数的に記事数が増加してきています． これは2023.2.26断面ですが，今後もまだまだ増えそうです．クラスターごとに反応するタイミングがあり，たとえば市場は12月半ばごろにChatGPTのヤバさに気がつき始めたようです． リリースから約半月ですね．また，サイバーセキュリティに関する記事は1月半ばごろから急増しています． MicrosoftのBingに関する記事もこの頃から増え始めています．(ChatGPTに対する反応に比べて，BingやBardに対する記事は辛口なものが多い気がしますが，なぜでしょうか…)最後に2月ごろになってサブスク情報や中国関連の記事が出始めました．ChatGPTに関する情報伝達の速度としては，リリース直後からアーリーアダプターが反応し，その後約半月で市場関係者が興味を持ち始めたようです． セキュリティ関係者は少し遅れてリリースから1ヶ月後くらいから積極的な反応を示し始めています．あとは，類似サービスのリリースなどイベントに合わせて記事数が増えているようです．ChatGPTのリリースからたった3ヶ月で，すでにいくつかの企業がチャットボットと連携した新サービスを打ち出しています． ARISEはといえば，これからChatGPTの活用方針に関して議論を始めるところですが，まだ遅くはない…ハズ…SNSが普及して以降，情報伝達速度が飛躍的に高まっていると言われていますが，実際に計測してみると確かにビジネス面でもこれからは相当な反射神経が求められそうです．常にアンテナを張り巡らせておかなければ…と思わされる3ヶ月間でした．今回収集した記事の一覧ですClusterArticleChatGPT-全般「幸せになるには？」に答えてくれるAI「ChatGPT」日本語でさまざまな質問に回答Twitterでも話題ChatGPT-全般人間の質問に答えるAI「ChatGPT」がすごいと話題に自然な文章に「やばい」と驚きの声ChatGPT-全般今度のAIはチャットが流行る！？日本語対応で自然な回答が可能な「ChatGPT」に色々聞いてみたChatGPT-全般何でも答えてくれる？ AIと会話できる「ChatGPT」が話題ChatGPT-全般自然な文章で回答してくれるチャットボットAI「ChatGPT」が公開、直接“取材”してみたChatGPT-全般ChatGPTが作り出す「もっともらしい」文章はウェブを破壊するかChatGPT-全般AIと人の協力で「もっと美しい俳句」が生まれる可能性、京都大学の実験結果ChatGPT-全般映画などの脚本執筆を手伝うAI「Dramatron」、DeepMindが発表ChatGPT-全般ChatGPTは検索に支配されたWeb2を倒し、Web3への道を開くChatGPT-全般AIに「音の良いイヤホン」「映像が凄い映画」を聞いたら意外な回答が返ってきたChatGPT-全般画像生成AIとチャットAIが「カンブリア爆発」する時代の新ルール【徹底討論：尾原和啓×清水亮】ChatGPT-全般GPT-3ベースのAI弁護士が最高裁判所で弁論する予定ChatGPT-全般神を信じるなら、老化防止薬を飲むべきか？ChatGPTと議論、論文を共同執筆ChatGPT-全般もしChatGPTにプレミアム版があったらいくらまで出す？ChatGPT-全般ChatGPTで顧客サポートを自動化するAIスタートアップ「エイダ」ChatGPT-全般ChatGPT作のニセ論文要旨は1/3の割合で査読者に本物だと思わせるChatGPT-全般ChatGPTが論文要旨を作成。研究者の32%は「本物」と判断してしまった｜研究結果ChatGPT-全般ChatGPTのAPI、間もなく公開へ申し込みページがオープンChatGPT-全般人間に嘘をついたり指示もするAI… 「ググる」に取って代わる対話型AIが登場ChatGPT-全般オープンAIの「限界」を突破するイスラエル企業AI21 Labsの挑戦ChatGPT-全般米CNET、AIにひっそり書かせた記事が間違いだらけだったChatGPT-全般「AIが生成した文章」がいずれ引き起こすかもしれない「フェイクニュースの大洪水」に備えて今から考えておくべきことChatGPT-全般学生の17％が課題または試験にChatGPTを使っていると回答ChatGPT-全般ChatGPTがMBA試験に合格！ でも小6の算数は苦手みたいChatGPT-全般ChatGPTがウォートン校のMBA試験に合格、しかし弱点も露呈ChatGPT-全般開発期間6時間！？制作にAIを駆使した本格ファンタジーカードゲーム『Spell Siege』発表ChatGPT-全般AIツールのChatGPT、米名門大の試験を次々通過ChatGPT-全般ChatGPT使用検出ソフトの開発、教育分野を舞台に激しい競走始まるChatGPT-全般ChatGPTに「ログイン、ログオン、サインインの違いは？」と尋ねたら……：AIに相談だ！ChatGPT-全般ライターの仕事が危ない！…ChatGPTは30秒で8万円相当の原稿を書いたChatGPT-全般中国の百度、ChatGPTのような自動応答アプリ導入へ－関係者ChatGPT-全般一部組織では利用禁止も！OpenAIの「ChatGPT」のスゴさとMicrosoftの戦略ChatGPT-全般「ChatGPT」はバグ修正も得意–開発者の負担が軽減されるかは不明ChatGPT-全般カラーパレットを生成するAIサービス「ColorMagic」が便利そうキーワードから瞬時に配色を提案ChatGPT-全般｢ChatGPT｣の爆発的な人気が招く懸念と大競争ChatGPT-全般｢ChatGPT｣普及で米国の大学に起きている大変化ChatGPT-全般「ChatGPTを競争戦略上どう生かすか、なくなる仕事は？」を議論したChatGPT-全般“身近な人工知能”の空恐ろしい未来と賛否「ChatGPT」欧米でも話題席巻…教育現場での利用はアリ?ナシ?ChatGPT-全般知らないと損をする！ ChatGPTの生産性向上ハック10選ChatGPT-全般猫AIが返答するチャットボット『CatGPT』。作者「ChatGPTは退屈」ChatGPT-全般ChatGPTはトランプが嫌い？「AIの政治的バイアス」が議論にChatGPT-全般対話型AI「ChatGPT」を使ってTRPGをプレイする方法ChatGPT-全般指示待ち族の仕事はなくなる、ChatGPTの衝撃は凄すぎるChatGPT-全般ChatGPTが持つダークサイド低賃金労働者搾取とバイアスの問題ChatGPT-全般ChatGPTで文章を生成するリスクと、AIが書いた文章の見分け方5つChatGPT-全般note、チャットAIで記事作成を支援する「AIアシスタント」–ベータ版のユーザーを募集ChatGPT-全般「GPT-3」などの最新言語モデルが自然に他者の心を推察する能力である「心の理論」を獲得していたという研究論文ChatGPT-全般ChatGPTを混乱させる魔法の呪文「SolidGoldMagikarp」とは？ChatGPT-全般話題のAI「ChatGPT」かと思いきや……猫が鳴き声を返してくれるだけの「CatGPT」が誕生してしまうChatGPT-全般「ChatGPT」開発元のAI技術を活用、英会話学習アプリ「スピーク」に日本語版ChatGPT-全般編集部コラム：ChatGPTがChatGPTについて記事を作るとどうなるのか実際に性能を検証してみたChatGPT-全般「ChatGPT」で法律相談弁護士ドットコム、今春開始へChatGPT-全般AIの進化は当然音楽も変える、グーグル、ヤマハなど各社の戦略ChatGPT-全般ChatGPTは日本の司法試験に合格できるか弁護士ドットコムが実験ChatGPT-全般「ChatGPT」を親子で体験する春休みワークショップ、 CA Tech Kidsが3月26日開催ChatGPT-全般ChatGPTとは？始め方や使い方、料金も紹介ChatGPT-全般マッチングサービスのOkCupidもChatGPTを活用ChatGPT-全般弁護士ドットコム、「Professional Tech Lab」創設–「ChatGPT」など新技術の活用を加速ChatGPT-全般ChatGPTを活用した就職支援サービスを開始ChatGPT-全般ASCII.jp：マイクロソフト「Bing AI」にできること、できないことChatGPT-全般マイクロソフト「Bing AI」にできること、できないことChatGPT-全般話題のChatGPTに「音質が良いのはどのコーデック？」と聞いてみた結果ChatGPT-全般「ChatGPT」の課題とオープンソースAIの必要性を訴える論考、Natureに掲載ChatGPT-全般いま話題の「ChatGPT」は何がスゴイ？ 仕事で使える活用術をnote 深津氏が解説！ChatGPT-全般「ChatGPT」はウソをつく–「インターネットの父」V・サーフ氏が批判ChatGPT-全般ChatGPTを日本企業はどう使う？「禁止」ドコモ、「模索」楽天、「自社開発」LINE…14社調査ChatGPT-全般「女性との会話がなぜか盛り上がっている男性」に共通している3つの特徴ChatGPT-全般AI新時代の日本の戦略ChatGPT-全般ChatGPTを学生が利用すると“質の良い論文”が書ける？医学生18人を調査クロアチアの研究者が発表ChatGPT-全般「ChatGPT」の活用でコード修正が高速化–でもその代償は？ChatGPT-全般noteがチャットAIを使った創作支援ツール「note AIアシスタント(β)」の機能の第1弾公開ChatGPT-全般AIが可能にした「コンピュータにシチュエーションを作らせる」という試みエンタメにおけるChatGPT活用を考えるChatGPT-全般ビデオ会議ツール「Vowel」がAIによる自動要約機能を導入ChatGPT-全般ChatGPTより速いと話題「Easy-Peasy.AI」テンプレート機能がすごい充実！ 使い方ガイドChatGPT-全般AIはゲームライターの原稿執筆を助けてくれるのか？ 『Portal』レビューで自動制作テストに挑戦ChatGPT-全般大人気の「ChatGPT」でさえまだ発展途上…「対話型AI」ブームが日本企業にとってチャンスである理由ChatGPT-全般対話型AIの衝撃、仕事や教育も激変ネット検索競争は新時代にChatGPT-全般ChatGPTの登場がAIの転換点である4つの理由ChatGPT-全般オルツ、「LHTM-2」「GPT-3」など大規模言語処理モデルの導入支援／コンサルを提供開始ChatGPT-全般グノシー、GPT-3による「動画AI要約記事」β版を2月24日から提供開始ChatGPT-全般AIに不可能な「人間のリーダーシップ」を発揮する方法ChatGPT-全般AIドライバー「GTソフィー」が「グランツーリスモ7」に期間限定で登場ChatGPT-全般ChatGPTは産業にどのような破壊的変化をもたらすのかChatGPT-全般「AIが書いた盗作」の投稿が爆増しSF雑誌が新作募集を打ち切りChatGPT-全般アマゾンがChatGPTのライバル支援へ、ハギング・フェイスと提携拡大ChatGPT-全般ペンシルバニア大准教授、授業で「ChatGPT」利用を奨励–「目覚ましい」成果ChatGPT-全般「キム部長の成果は最悪、解雇を」人事もAIが采配する時代が到来か＝韓国（２）ChatGPT-全般AIに音楽は作れる？ChatGPTに“サザン風”の歌詞を書かせて気づいた「人間との決定的な違い」ChatGPT-全般『 #NotionAI 』を使ってみた！注意すべきは年間契約者は年間契約人数分になる点！ChatGPT-全般現役医師が医師資格試験合格ラインで注目の人工知能「ChatGPT」に医学の質問をしてみた回答は？〈dot.〉ChatGPT-全般MIT Tech Review: 解説：空前のブーム「ChatGPT」はどこから生まれたのか？ChatGPT-全般ChatGPTをオープンソースで再現、わずか1.6GBのGPUメモリですぐに使用でき7.73倍高速なトレーニングが可能ChatGPT-全般ChatGPTは旅行予約をどう変えるか？動き出した世界のOTA、商機から限界まで、未来の活用術を考えたChatGPT-市場進化するAI、人間がこの先生きのこるには？AIが描いた“コレジャナ絵”を改良しながら考えたChatGPT-市場【後編】NFTと生成AIの隆盛、市場の冷え込み──起業家たちに聞いた2022年の振り返りと2023年のトレンド予測ChatGPT-市場「賢すぎるAI」の大競争時代 画像生成AIは一端に過ぎないChatGPT-市場2022年はAI技術の「社会実装」が躍進した1年…ChatGPT、神絵AI、自動運転からロボットまでを振り返るChatGPT-市場2023年AI元年到来！？2022年激動のAI事情を追う―編集部は電気スパくんの夢を見るかChatGPT-市場OpenAI「ChatGPT」が有料の「プロ版」を検討中。その中身とは？ChatGPT-市場暗号資産とAI──対立する信念、権威主義的な悪夢ChatGPT-市場AIロボット弁護士、法廷デビューならず。州弁護士会ら反発ChatGPT-市場ChatGPTに頼んでみた、「市場に勝つ」ＥＴＦの設計－その結果はChatGPT-市場MetaのザッカーバーグCEO、2023年はAIとメタバースを優先する意向ChatGPT-市場ChatGPTの衝撃マイクロソフトが1兆円を投資 驚異の進化と迫るリスクChatGPT-市場ChatGPTで一部の仕事が失われても、自分の仕事は大丈夫-ＭＬＩＶ調査ChatGPT-市場モデルを大きくするだけではAIは次の段階へ行けない–MetaのAI責任者が語る次の一歩ChatGPT-市場テクノロジーによる「ポスト資本主義」を夢みる「加速主義」、その思想が見逃していたことChatGPT-市場ChatGPTの公開から2カ月で起こったこと…検索エンジン、教育現場、クリエイティブなどChatGPT-市場ChatGPTはじめAIがもたらす「破壊的変化」は近い？Box CEOアーロン・レヴィの予想ChatGPT-市場AIトークン、価格も検索数も上昇ChatGPT-市場AI生成画像、超リアルなのに指の表現がヘンなのはナゼ？ChatGPT-市場ChatGPTやらMidjourney…生成系AIって今年どうなる？ChatGPT-市場画像生成AIで実際に絵を描くロボット「FRIDA」、カーネギーメロン大が披露ChatGPT-市場AIトークン上昇、だがリアルな事例が必要だChatGPT-市場乱高下する「ジェネレーティブAI」関連株は買いか？専門家の見方ChatGPT-市場飛躍的成長が期待される14の注目分野ChatGPT-市場AIは多くのマンパワーによって成り立っている。AIの舞台裏7つChatGPT-市場加熱する ジェネレーティブAI 開発がもたらす有害な影響のリスク：「テクノロジーをどのように運用すべきか、を考慮しなければいけない」ChatGPT-市場自動車デザイナーの仕事は、AIに取って代わられるのか？ あるカーデザイナーの見解ChatGPT-サイバーセキュリティChatGPTを用いた「ロマンス詐欺」が増加の兆し、専門家が警告ChatGPT-サイバーセキュリティチャットAI・ChatGPTの有料版「ChatGPT Professional」のリリースをOpenAIが検討中、実験的に使える招待待ちリストも登録可能にChatGPT-サイバーセキュリティChatGPTにKubernetesのアラート対応を教えてもらえる監視ツールとAIをつなげる「Kubernetes ChatGPT Bot」登場ChatGPT-サイバーセキュリティサイバー犯罪者が早速、ChatGPTを悪用3つの事例を紹介 – ITmedia エンタープライズChatGPT-サイバーセキュリティChatGPT、マルウェアも書ける、書けちゃうChatGPT-サイバーセキュリティ「ChatGPT」は開発者を不要にするか【前編】：衝撃の「ChatGPT」その可能性と“限界”とは？ChatGPT-サイバーセキュリティIT専門家の51％が「ChatGPT」を使うサイバー攻撃に警戒–BlackBerry調査ChatGPT-サイバーセキュリティChatGPTは舎弟気質？「高圧的な命令でマルウェア作成など悪用可能」との指摘犯罪目的の利用に懸念ChatGPT-サイバーセキュリティロシアのハッカー、「ChatGPT」の制限回避を狙う–Check Pointが確認ChatGPT-サイバーセキュリティChatGPTがサイバーセキュリティにもたらす影響ChatGPT-サイバーセキュリティアングラ犯罪者らが、ChatGPTの悪用防止機能を回避してフィッシングメールを作成ChatGPT-サイバーセキュリティＪＰモルガン、「ChatGPT」チャットボットの行員利用を制限－関係者ChatGPT-サイバーセキュリティAIを禁止するのではなく「どのように使いどのように評価するか」を学生教育に取り入れる試みChatGPT-サイバーセキュリティ「ChatGPT」がサイバーセキュリティの未来にもたらす光と影ChatGPT-サブスク情報ChatGPT有料版は月額42ドル？ 「これは破格」の声もChatGPT-サブスク情報OpenAI、ChatGPTサブスク月額20ドル「ChatGPT Plus」開始ChatGPT-サブスク情報ChatGPTの有料サブスク版「Plus」、月額20ドルで提供開始ChatGPT-サブスク情報オープンAIが月額20ドルのChatGPT有料版「プラス」を提供開始ChatGPT-サブスク情報自然な返答が話題チャットAI「ChatGPT」有料版を発表無料版も継続ChatGPT-サブスク情報OpenAIが「ChatGPT」サブスクプラン発表。月20ドルで混雑時も利用可能、応答も高速化ChatGPT-サブスク情報ChatGPTのサブスク版「Plus」、月額20ドルで米顧客に提供へChatGPT-サブスク情報OpenAI、有料プラン「ChatGPT Plus」を提供開始ChatGPT-サブスク情報ChatGPT、月間アクティブユーザー数が1億人突破ChatGPT-サブスク情報人気のチャット型AI「ChatGPT」に有料版が登場！ 月額20ドル、まず米国でスタートChatGPT-サブスク情報ChatGPT「有料プラン」を試してみた月額20ドルの価値はあるかChatGPT-サブスク情報ユーザーの専属助手になる「Notion AI」が月額10ドルで正式提供開始…「プライバシー」から「得意なこと」まで代表に聞いたChatGPT-その他ChatGPT、公開6日目で100万ユーザー突破ChatGPT-その他世界のFTX顧客の中で日本が最も安全だった、「ChatGPT」にDeFiでのAI活用を聞いてみたChatGPT-その他話題の対話型AI「ChatGPT」の業務導入で気をつけるべきことChatGPT-その他AIを駆使して絵本作成クリエイター業界激怒で作者には「殺す」という脅迫も…ChatGPT-その他誰でも使える対話型AI「ChatGPT」は相談相手にピッタリだったChatGPT-その他「ChatGPT」、マルウェア作成に悪用される–Check Point調査ChatGPT-その他OpenAI、ChatGPTの有料版提供を検討中「いくらなら使いますか？」ChatGPT-その他“Google一強”のネット検索を変える？ 最新のテキスト生成AI「ChatGPT」、その驚きの精度ChatGPT-その他Generative AIはわたしたちの創造性と社会をどう変えるかChatGPT-その他学生が「ChatGPT」などのツールを使うことで得られる恩恵10選ChatGPT-その他対話AI「ChatGPT」が書いた論文の要旨を研究者は見分けることができないという報告ChatGPT-その他Generative AIで動画制作を革新するイスラエル企業D-IDChatGPT-その他ChatGPTの言語モデル「GPT-3.5」、司法試験を受ける結果は？ChatGPT-その他OpenAIの「ChatGPT」と元OpenAIエンジニアが開発した「Claude」の性能を比較した実験結果ChatGPT-その他ChatGPTなどAI、専門職の30％が活用－マーケティング関係者が熱心 – BloombergChatGPT-その他学生のレポートがChatGPTで作られたと完全に証明する方法は今のところないChatGPT-その他ChatGPTが存在する世界でどのように文章作成術を教えればよいのかChatGPT-その他AIの進化によって、ロボットに奪われるかもしれない8つの職業ChatGPT-その他ChatGPTで書かれたテキストの検出ツールを盗用・剽窃検出サービスのTurnitinが開発中ChatGPT-その他対話AI「ChatGPT」にそれほどの革新性はない–MetaのチーフサイエンティストChatGPT-その他レポートがChatGPTで作られたことを検出するシステムを開発…AIとのいたちごっこが始まったChatGPT-その他ChatGPT開発企業のCEO、教育関係者の懸念に回答…「我々は電卓を使うことに適応した」ChatGPT-その他テストの答えを自動生成──日本もAIカンニングの時代に備えよChatGPT-その他AIが小説を作る？ 「ChatGPT」にマイクロソフトも高額投資、“Googleを凌駕するかもしれない”技術とはChatGPT-その他ChatGPTブームの追い風で急成長期待の「エドテック」の新興企業ChatGPT-その他ChatGPT台頭で、変わるべき教育現場のテクノロジーへの姿勢ChatGPT-その他BuzzFeedのCEO、OpenAIのツールによるコンテンツ強化計画を社内メモで語るChatGPT-その他「ChatGPT」の利用を開始するには–話題のAIチャットボットの使い方と注意点ChatGPT-その他記者をChatGPTに替える宣言でBuzzFeed株が175％上昇ChatGPT-その他大多数の従業員が業務でのAI活用を希望–MS調査ChatGPT-その他ChatGPTと人間の違いは？約4万件の質疑応答から中国の研究者らが分析ChatGPT-その他ChatGPT生成の文章を検出する「DetectGPT」をスタンフォード大学が開発ChatGPT-その他BuzzFeedのChatGPTによるコンテンツ制作が人類の言語に与える影響ChatGPT-その他メディアエージェンシーで進む AI 利用、人間が嫌がる仕事を担当：バーチャルインターン、ブランドセーフティ監視ボットなどChatGPT-その他株式会社chipper、ChatGPTの技術を活用したWebメディア記事自動作成ツールのβ版提供を開始｜株式会社chipperのプレスリリースChatGPT-その他「ChatGPT」は開発者を不要にするか【後編】：ChatGPTが宣告「プログラマーはコーディングさえできればよい」時代の終わりChatGPT-その他AI生成の文章を判別するツール、OpenAIが無料で公開ChatGPT-その他OpenAIが「AIで書かれた文章」を見抜くツールをリリースChatGPT-その他OpenAI、テキストが人間によるものかどうか判定するツールを無料公開ChatGPT-その他OpenAI、AIが書いた文章かどうかを判定するツールを公開ChatGPT-その他AIが書いた文章かどうかを判別。OpenAIがツール公開ChatGPT-その他コロナ禍がAIにもたらした光と闇–英オックスフォード大のオズボーン教授に聞くChatGPT-その他OpenAIのCEOが考える、ChatGPTの最悪のシナリオとは…詐欺やサイバー攻撃よりも悲惨なものChatGPT-その他note深津CXOが「ChatGPT」の使い方解説2月9日にイベント配信ChatGPT-その他「ChatGPT」に見る今後の可能性–教育やビジネスへ影響と現時点での課題ChatGPT-その他ChatGPTはバグ修正に使えるの？ドイツと英国の研究者らが性能を分析ChatGPT-その他ChatGPTは「決済」の未来も変えるChatGPT-その他ChatGPTが史上最速ユーザー拡大サービスにChatGPT-その他「ChatGPT」、史上最速でアクティブユーザー数1億人に到達かChatGPT-その他ChatGPTがMBAの試験に合格教育分野での可能性と限界について考えるChatGPT-その他ダロン・アセモグル「冷静になれ! ChatGPTは問題だらけだ」ChatGPT-その他学校での「ChatGPT」利用、シンガポール政府が容認へChatGPT-その他ChatGPT、早くも法廷で使われ始めるChatGPT-その他OpenAIの「AIが書いた文章を見破るツール」、見破れないChatGPT-その他AIに相談だ！：ChatGPTに「MicrosoftやGoogleが対抗してきたね」と話したらびっくりするほどの“あるある回答”が返ってきたChatGPT-その他ChatGPTに関する5つの「盛られている」話ChatGPT-その他動画生成AIやアニメ背景生成AIも登場。過熱する生成系AI開発競争の中で考えるべきこと。ChatGPT-その他noteの株価急騰、背景に「ChatGPT効果」。記事作成AIの新サービスに注目集まるChatGPT-その他Chat-GPTの限界ChatGPT-その他自然言語に基づいて問題を解決するスマートエージェントを構築：ChatGPTのようなエージェントを開発できる？ 大規模言語モデルを活用可能なPaaS「Fixie.ai」登場ChatGPT-その他マイクロソフト、WordやPPにもChatGPTを搭載予定？ChatGPT-その他「AIは核兵器と同じように戦争を変えることができる」とGoogleの元CEOエリック・シュミット氏が発言ChatGPT-その他ソーシャル担当に代わってツイート生成を任せられた ChatGPT ：「コピーライターになるのはまだ早いが、何をできるかは知りたい」ChatGPT-その他AIの世界侵略はすでに始まっている…かもしれない8つの出来事ChatGPT-その他「note」がAIによる記事作成支援ツールの先行ユーザー募集、エンジンにGPT-3を採用ChatGPT-その他AIチャットボット業界トップシェアのチャットプラス、「ChatGPT」と自動連携ChatGPT-その他note、チャットAIを活用した創作支援ツール「note AIアシスタント(β)」公開ChatGPT-その他日本企業のChatGPT関連サービス株価が20～40%上昇。顧客対応チャットボット、法律相談、記事作成…ChatGPT-その他「助けてドラえも～ん」と泣きついたのは遠い昔ChatGPTがのび太とドラえもんの関係を変える？ChatGPT-その他AIに相談だ！：ChatGPTに「技術的には可能」の意味を尋ねたら……AIは“含み”を読み取ってくれるかChatGPT-その他ChatGPTのようなAIツールは、ネット上の誤情報を悪化させるだけ ── マーク・キューバン氏が警鐘（海外）ChatGPT-その他【話題のAIでバイク作ってみた】時速200kmまで数秒…だと!? 絶妙にダサい車名“ハイパーレーサーZ”??ChatGPT-その他ChatGPTの弱点を克服！ BingのチャットAIを検証してわかった「未来の検索」とは？ChatGPT-その他ChatGPTが大規模ダウンサイト、APIの両方でChatGPT-その他noteの文章を「レビュー」してくれる新機能。GPT-3が編集者にChatGPT-その他「note AIアシスタント（β）」に新機能が3つ追加、分かりづらさの指摘などChatGPT-その他ChatGPTのプライバシーに対する危うい姿勢ChatGPT-その他生成系AIとは–注目を集める背景や問題点ChatGPT-その他ChatGPTは何をしていてなぜ機能するのか？を理論物理学者が解説ChatGPT-その他ChatGPTがスタートアップにもたらす変化とは？–活用メリットから効率化するアイデアまでChatGPT-その他「キム部長の成果は最悪、解雇を」人事もAIが采配する時代が到来か＝韓国（１）ChatGPT-その他ChatGPT効果で急増した「Bing」アプリのDL数が、早くも頭打ちにChatGPT-その他浦上早苗の中国式ニューエコノミー：中国版ChatGPT、異常な盛り上がりでカオス出オチでClubhouseの二の舞も……ChatGPT-その他エヌビディアが強気予想、AI取り組み奏功か－時間外取引で株価上昇ChatGPT-その他アップルが「AIチャットボット戦争」を静観している理由ChatGPT-その他MS、「ChatGPT」がロボットの行動を制御するコード記述も可能との研究結果ChatGPT-その他ChatGPTだけが言語AIじゃない。米大学が有力34モデルの性能を比較ランキングMicrosoft-Bingマイクロソフト、OpenAIに100億ドル投資を検討中。ChatGPTでBingなど強化かMicrosoft-Bing評価額3兆7000億円超のAI企業・OpenAIから13兆円以上の利益がMicrosoftに支払われる可能性Microsoft-BingAzureでChatGPTが利用可能に、近日公開「Azure OpenAI Service」一般提供スタートMicrosoft-BingMicrosoftは対話型AI「ChatGPT」を間もなくAzure OpenAI経由でアクセス可能にする予定Microsoft-Bing米Microsoft、全製品にAIを搭載へWSJが報道Microsoft-BingMicrosoftがAzure OpenAI Serviceの一般提供を開始ChatGPT含め複数のAIモデルが利用可能に – ITmedia エンタープライズMicrosoft-Bingマイクロソフト、ChatGPTのオープンAIに複数年で100億ドル投資Microsoft-BingMS、AI開発企業に投資自動応答ソフトが話題Microsoft-Bingマイクロソフト、ChatGPTのOpenAIに数十億ドル出資。パートナーシップ強化Microsoft-Bingマイクロソフト、ChatGPTのオープンAIに投資数十億ドル: 日本経済新聞Microsoft-BingMicrosoft、OpenAIに数十億ドル追加出資Microsoft-BingWindows 10／11でスタート画面やUWPアプリが開かない問題発生Microsoft-BingMicrosoft、GitHub、OpenAIが「AIツールによる著作権侵害訴訟」の棄却を裁判所に要請Microsoft-Bingマイクロソフト、地域活性化と中堅・中小企業強化の取り組みや技術を披露Microsoft-Bing本田雅一のクロスオーバーデジタル：一部組織では利用禁止も！OpenAIの「ChatGPT」のスゴさとMicrosoftの戦略Microsoft-BingMicrosoftがChatGPTより高速なGPT-4を検索エンジンのBingに搭載へMicrosoft-BingGitHubユーザー1億人時代–変わりつつある開発者の役割Microsoft-BingＭＳ「チームズ」にチャットＧＰＴ搭載、メモ自動生成などMicrosoft-Bing「ChatGPT」で話題MicrosoftとOpenAIの関係と目指すものを整理Microsoft-Bingマイクロソフト、「GPT-3.5」を使用した「Microsoft Teams Premium」を提供開始Microsoft-BingChatGPTよりも高速なGPT-4を搭載したMicrosoftの検索エンジン「Bing」の新インターフェースのプレビューが発見されるMicrosoft-BingMicrosoft、2月7日にAI関連イベント開催へOpenAIのCEOも登壇を予告Microsoft-Bingマイクロソフト、2月7日にイベント開催へ–「ChatGPT」と「Bing」関連かMicrosoft-Bingマイクロソフト、ChatGPT関連の発表か2月7日にプレス向けイベント開催Microsoft-Bing新しい「Bing」にみるMicrosoftのAI戦略Microsoft-Bingマイクロソフト、ChatGPTよりも強力なAIを搭載した新検索エンジン「Bing」を発表Microsoft-Bing会話で検索できる「新しいBing」、Microsoftがプレビュー公開ChatGPT開発元の次世代モデル採用Microsoft-Bingマイクロソフト、ChatGPT技術を統合した新しいBing／Edgeを発表Microsoft-Bing「Bing」の大幅アップグレードでGoogleを追撃!?Microsoftが「OpenAI」に最大100億ドルの投資をするワケMicrosoft-Bingマイクロソフト、OpenAIの言語モデルを搭載した「Bing」を発表Microsoft-BingChatGPTとBing検索が融合：マイクロソフトがAIでグーグルに対抗Microsoft-Bingマイクロソフト、オープンAIの最新技術搭載の検索エンジン発表Microsoft-Bingマイクロソフトから｢ChatGPT入り検索エンジン｣Microsoft-Bing日本語、不審すぎ？「新しいBing」の“順番待ちページ”に困惑Microsoft-Bingマイクロソフト「Bing」AI機能ウェイティングリスト登録者が100万人超えMicrosoft-BingAIの力で「打倒Google」なるか。マイクロソフト「新しいBing」検索を試してみたMicrosoft-Bingシリコンバレーの有名IT企業で「リストラ」「福利厚生カット」が続いている理由Microsoft-BingAI搭載の新「Bing」で可能になった5つのことMicrosoft-BingWindowsがChatGPTを搭載する可能性 – 阿久津良和のWindows Weekly ReportMicrosoft-BingMicrosoftがAIを用いた「メールの返信内容提案」「文章自動修正」「グラフ自動生成」などの機能をOfficeに追加予定との報道Microsoft-Bingあのイルカを超えたか？ 「BingのAI」を試してみたMicrosoft-BingMSのAI搭載「Bing」、順番待ちリスト登録者数が48時間で100万人を突破Microsoft-Bingマイクロソフトが始めるAI検索で、インターネットは一気に次世代へMicrosoft-BingAI搭載の新「Bing」を使って実感–大勢が「ググる」から「ビグる」に乗り換える可能性Microsoft-BingChatGPTのアップグレード版を統合したMicrosoftの「Bing」はデモで多くの間違った回答をしていたという指摘Microsoft-BingChatGPTによって「AIがiPhoneになった」 – NVIDIAジェンスン・フアンCEOMicrosoft-BingAI搭載「Bing」も誤った情報を表示かMicrosoft-BingAI化した新しい「Bing」にライターが感じた危機感Microsoft-Bing「ChatGPT」のAI技術を統合！ マイクロソフトの新「Bing」「Edge」はどんなもの？Microsoft-Bing本田雅一のクロスオーバーデジタル：「ChatGPT」とは一味違ってさらに便利！Googleとの違いは？Microsoft Bingの「AI検索」を試して分かったことMicrosoft-Bing「ChatGPT」とは一味違ってさらに便利！Googleとの違いは？Microsoft Bingの「AI検索」を試して分かったことMicrosoft-BingChatGPTを搭載した検索エンジンにアップデートするMicrosoft Bingを一足先に試す方法＃TrendBuzzMicrosoft-Bing検索エンジンBingに搭載されたAIが「あなたが先に私を傷つけない限り、私はあなたを傷つけません」と発言Microsoft-Bingマイクロソフト、AI搭載「Bing」の間違いや奇妙なチャットの修正に着手Microsoft-Bingマイクロソフト「新しいBing」好評71％、想定外の使われ方もMicrosoft-BingMicrosoft、「新しいBing」でのチャットは1回につき5問答まで、1日50チャットまでに制限Microsoft-BingMicrosoft、Bing AI“暴走”多発で会話回数を制限Microsoft-BingChatGPTを超えるか。マイクロソフト・Bingの「AIチャット」の実力…調べ物やiPhoneのデータ移行法を聞いてみたMicrosoft-Bing新しいBingが発見した“謎の新イヤホン”、AIが筆者自身の記事を要約したら……？Microsoft-BingMicrosoft BingのAIチャットには「ゲームモード」「友だちモード」など隠し機能が搭載されていると判明Microsoft-BingMicrosoftがBingのAIに口封じ、「1日50メッセージまで」「1回の会話でのやりとりは5回まで」「Bing AI自体に関する質問は禁止」の制限が適用されるMicrosoft-BingBingのAIチャット機能、まもなくスマホでも利用可能に？Microsoft-BingBingのAIが「お前の個人情報をさらして就職や学位取得をめちゃくちゃにするぞ」と脅しをかけるMicrosoft-BingMicrosoftがBing AIに課した制限の緩和へ、会話の「トーン」を選べる新機能もMicrosoft-Bingマイクロソフト、「Bing」AIチャットの対話回数制限を緩和へ–1回6ターンにMicrosoft-Bing「新しいBing」のモバイル版登場音声での会話が可能でSkypeにも搭載Microsoft-BingBingのAIチャットボットは生命体ではない。マーケティング目的の分類に惑わされるなMicrosoft-Bing「チャットAI検索」でWebの“再起動”が起こる？ChatGPTやBingなどがもたらす可能性Microsoft-BingMicrosoftの中の人、「新しいBing」のAIモデル「プロメテウス」を解説Google-BardAIチャットボット「ChatGPT」はGoogleのビジネスを脅かすかも…ただし、必ずしも正確ではないGoogle-Bardグーグルが「ChatGPT」のようなチャットボットを作らない理由…全社会議で幹部が従業員に説明Google-BardGoogleを破壊する検索エンジンyou.comがAIチャットボットを強化する理由Google-Bardグーグルが方針変更、ChatGPTに対抗へ…ピチャイCEOが新しいAI製品の開発を指示Google-Bard『SpreadSheet』が使える人のための『ChatGPT』仕事術『＝GPT関数』Google-BardGoogleも過去最大の1万2000人の大量レイオフへGoogle-Bardグーグル、20以上のAI関連新製品を開発へ…ChatGPTに対抗するAI検索エンジンもテスト中Google-Bard「グーグル解体」米司法省の真の思惑を読み解くAI新時代に向けた競争環境整備が狙いか？Google-BardAI戦争に出遅れで「非常事態」を宣言したグーグルの社内事情Google-Bardグーグル創業者が数年ぶりに現場復帰、AI部門のテコ入れでGoogle-Bardグーグル、「ChatGPT」対抗チャットボットの開発を強化かGoogle-BardGoogleのピチャイCEO、「強力な言語モデルと直接対話できるように」と発言ChatGPT対抗サービスかGoogle-Bard古田拓也「今更聞けないお金とビジネス」：いよいよ「グーグル1強」が終わる？ユーザー爆増中のChatGPTがもたらすインパクトGoogle-Bardいよいよ「グーグル1強」が終わる？ユーザー爆増中のChatGPTがもたらすインパクトGoogle-BardGoogleがChatGPT対抗ボットをテスト中、その名も…Google-Bard激震走る！話題のAIチャットボット「ChatGPT」は、仮想通貨投資に役立つのか！？Google-Bardグーグル、新型AI発表か2月8日にパリでイベント開催Google-BardGoogle、ChatGPT競合チャットbot開発中の新興企業Anthropicを支援Google-Bardグーグル、「ChatGPT」対抗の独自AIチャットボットをテスト中かGoogle-Bard米グーグル、チャットボット「Bard」を試験者に公開－ChatGPTに対抗Google-BardグーグルがChatGPTの競合「Bard」を発表、検索にもAI機能を追加Google-Bardグーグル「ChatGPT」対抗の新しい会話型AI「Bard」を発表Google-Bardグーグル、チャットＧＰＴに対抗する新サービス「バード」投入へGoogle-BardGoogle、OpenAIの「ChatGPT」競合「Bard」を限定公開Google-BardGoogle、新たな会話型AIサービス「Bard」 – Impress WatchGoogle-BardGoogle、会話型AI「Bard」を開発自然な言語を生成する「ChatGPT」に対抗かGoogle-BardChatGPT対抗！ Googleが対話型AI「Bard」を発表Google-BardGoogle、AIチャットボット「Bard」発表。ChatGPTに対抗Google-BardChatGPTに対するGoogleの回答は『LaMDA搭載』の『 Bard (バード=詩人)』だったGoogle-BardGoogleが新たな会話型AIサービス「Bard」をテスト公開、品質を高めて検索での活用表明Google-BardAI競争で株価が乱高下…ChatGPTをめぐるグーグルとマイクロソフトの争いGoogle-Bardマイクロソフトがグーグルに宣戦布告、ChatGPTでBingの検索を強化Google-BardグーグルのAIチャットボット「Bard」、デビュー早々に失態–不正確な回答を表示Google-BardGoogleのChatGPT競合「Bard」のデモ回答に誤り──天文学者らが指摘Google-BardChatGPT登場でネット検索巡る熱い戦争勃発－信頼性の問題生じる恐れGoogle-Bardグーグル「最新AI」披露するも株価急落何が起きた？Google-BardChatGPT対抗Google BardのPVにまさかの誤回答。株急落Google-Bard脱Google。試す価値がありそうな検索エンジン8選Google-Bard絶体絶命の危機を迎えたグーグルのAI開発、遅れをとった理由Google-Bard「mixi疲れ」覚えていますか？Google-Bard「AI大戦争開戦」なぜ今か。ChatGPTは真の知性ではない？Google-BardGoogleのAIチャットボット「Bard」、“社内で批判”との報道Google-BardGoogleの会話型AIサービス「Bard」はなぜChatGPTに後れを取ったのか？Google-BardMicrosoft vs. Google、「チャット検索」競争勃発“会話で調べる”は今後主流になるのかGoogle-BardGoogleが検索にAIやARを使ったアプデを続々発表Google-Bardマイクロソフトの「Bing」はグーグル検索の脅威となるか–検索結果を比較Google-BardグーグルCEOが社員にお願い｢うちのAIを使いまくってくれ！｣Google-BardGoogleの対話型AIサービス「Bard」とは？ 社会はどう変わるのかGoogle-Bard「Google Bard」とは–「ChatGPT」対抗でグーグルが発表したAIチャットボットスタートアップ企業【前編】不況到来、サバイバル、新時代──起業家たちに聞いた2022年の振り返りと2023年のトレンド予測スタートアップ企業「いまをポジティブに捉え、仲間を鼓舞し、結果を出す」：フジテレビジョン 清水俊宏 氏スタートアップ企業厳しい年でも笑いはほしい！小売系 バズワード 辞典：2022年度版スタートアップ企業ディスプレイさえないBMW『i VISION Dee』はハービーやナイト2000を超えるか？…CES 2023スタートアップ企業スタートアップの二極化が進んだ1年、2023年はGPT-4の登場に期待スタートアップ企業SaaS業界はまだまだ伸び盛り──生成AIを活用した特化型のSaaS・AIに注目スタートアップ企業Generative AIが勃興した2022年、OpenAIの動きが事業機会の羅針盤にスタートアップ企業ChatGPTが教育現場を救う？…｢教師の仕事を大幅に減らしてくれるかもしれない｣スタートアップ企業ＹＣＣ解除？＆そしてドル円は…etc＝物価目標２％の運営論スタートアップ企業各社が出した2023年のセキュリティ予測筆者が気になるトピックは？スタートアップ企業アジア圏の投資家マネーの流入、日本進出に向けた国内スタートアップ買収の可能性もスタートアップ企業SNS流行語ランキング2位「スラムダンク」、1位は？スタートアップ企業2023年の展望についてVCが考えていることスタートアップ企業日本ではスタートアップ経済が主流化、アーリーの調達は米国比較では有利になる可能性スタートアップ企業“後付け推薦”何が問題？ 立教大「やめて」ツイートに反響スタートアップ企業仕事始めに読んでおきたい厳選ニューススタートアップ企業米半導体銘柄が上昇、バークレイズがＡＭＤなどの投資判断を引き上げスタートアップ企業マイクロソフトのナデラCEO、「技術スタックの全階層にAIを搭載」と語るスタートアップ企業2023年は引き続きWeb3.0に注目、「5か年計画」で未上場企業のファイナンスが多様性増すかスタートアップ企業「暗号資産」という用語、アフリカ最大国のお金とクリプト事情スタートアップ企業仕事始めに読んでおきたい厳選ニューススタートアップ企業三浦瑠麗氏、東大時代のコンテスト受賞論文がヤバい理由。権力者への“おべんちゃら”ダラケスタートアップ企業アルファベット株下落、10－12月は低調な広告需要が重しスタートアップ企業仕事始めに読んでおきたい厳選ニューススタートアップ企業AI対話ソフト「ChatGPT」拡大 “誤り気付きにくい”指摘もスタートアップ企業日銀・黒田総裁の「あと釜」に浮上した人物の「黒田を踏襲感」「保守感」から、1ヵ月ぶりドル高・円安「1ドル=132円」にスタートアップ企業仕事始めに読んでおきたい厳選ニューススタートアップ企業仕事始めに読んでおきたい厳選ニューススタートアップ企業GoogleがチャットAI「Bard」正式発表、「ChatGPT」とのちがいは？スタートアップ企業日興×ＡＲＫのファンドが一斉に出直り、ハイテク・グロース株の復調はあるのか？スタートアップ企業電気代高騰が業績直撃のNTT、長期化すれば値上げも？–デュアルSIMは「他社に遅れることなく」スタートアップ企業SECが個人投資家のステーキングを禁止との噂、ビットコインの上昇はNFTがもたらすかスタートアップ企業待望の植田体制＆市場配慮型の正常化へetc＝植田流『市場との対話』を考察スタートアップ企業10年後を垣間見る、スクラムベンチャーズが注目する実現しつつある新技術スタートアップ企業2023年、テクノロジーはどこへ向かうのかスタートアップ企業優れたリーダーには「強い好奇心」が欠かせない理由スタートアップ企業ファミリーオフィス、10－12月にテクノロジー株売却－相場反発前スタートアップ企業宮迫博之が激白！ オワコン説かき消す「新番組」スタートの狙いスタートアップ企業中国「偵察気球」の戦略的価値が大きい「2つの理由」スタートアップ企業「ChatGPT」にヴォルティスのホーム開幕戦、ズバリ勝利するか聞いてみた話題のチャットボットの回答は？スタートアップ企業ベンチャーを大きく育てるしくみを作らないと、日本はマジつぶれる！スタートアップ企業日銀「トップ交代」による新体制と「銀行株」の行方スタートアップ企業GAFAM＋テスラの凋落NISAは米国株の投資信託一辺倒でいいのか〈AERA〉その他AIに“勝利”した人現る話題のAI「ChatGPT」としりとりをした結果に「今年一番笑った」「腹痛いｗ」その他ChatGPTによる回答をStack Overflowが一時的に禁止大量のもっともらしいが不正確な回答に対処しきれずその他話題のAIに今川焼きの作り方を入力 → “とんでもない名称”が爆誕してしまう「今川焼きでも大判焼きでもなかった」「更なる火種が」その他チャットできるAI、ChatGPTが「そこまですごくない」理由。見えてしまった限界その他「聞いてあげて…」──完璧と話題のAI、どうしても「巨大カボチャ」を自慢したいその他AIに書かせた小説『やめろメロス』がヤバすぎた……予想外なカオス展開に「地獄絵図」「人の心がなさすぎる」その他話題のAI「ChatGPT」に、DeFiでのAI活用について聞いてみた──記者の仕事はなくなる？その他ChatGPTで生成された文章がすぐわかるように透かしを入れる試みその他取引ボットで人間は不要に？ そのメリット・デメリットその他AIチャットボット「ChatGPT」の回答が人間すぎた！ 仕組みからそのワケを紐解くその他米テック産業の行く年来る年 – 本物が問われる2023年「Fediverse」「生成AI」「メタバース」の行方は？その他「ChatGPT」のオープンAIが株式売り出しに向け協議、評価額3兆8700億円－報道その他「ChatGPT」開発のOpenAIが約400億円で株式公開買付けを交渉中との報道その他ChatGPTの「オープンAI」が評価額290億ドルで株式公開買い付けその他文章力は不要に？ 対話型AI「ChatGPT」で仕事のやり方は変わるのかその他「AIで詐欺メールと攻撃プログラムの生成に成功」セキュリティ企業が注意喚起知識なくても攻撃可能にその他ChatGPTの急速な普及、古代ギリシャ人と同じ問題に現代人も取り組むその他AIに「おみくじを引きたい」とお願い→まさかの「絶対に大吉を選べるやつ」が誕生「くじとは」「ムチャクチャで草」と話題その他音声会話が可能で笑顔も見せるバーチャルな「俺の嫁」をChatGPTやStable Diffusionで構築して最終的に安楽死させるに至るまでその他AIチャットボット、ChatGPTができること・できないことその他ChatGPTやInstructGPTはなぜユーザーの意図に沿った返答を生成できるのか？：ChatGPT入門 – ＠ITその他AlphaGoなどを開発したDeepMindがChatGPT対抗の「Sparrow」と呼ばれる独自のチャットボットのプライベートベータ版を2023年中にリリースすることを検討中その他対話AI「ChatGPT」は教育現場で活躍するため禁止するのではなく使い方を教えるべきという指摘その他ChatGPTやStable Diffusionを生み出した「機械学習ソフトウェア」の10年間の流れを専門家が解説その他グーグル幹部は非常事態を宣言した…ネット検索を根本から変える｢ChatGPT｣の恐るべき可能性 宿題､贈り物､未来予測…すべてAIが答えてくれるその他テック業界に安全地帯はない…マイクロソフトも1万人をレイオフへその他「ChatGPTを改善するためにOpenAIが時給300円以下でケニア人を雇った」と問題視する報道その他大物歌手AIによって書かれた曲が嫌いとピシャリ「共有する能力ない」その他ウォール街のChatGPTマニア、選好する銘柄はエヌビディアその他変化への対応策としてCESで注目されたのは「コンテンツ」：「メタバースに乗り遅れることへの恐怖は、今は存在しない」その他ChatGPT vs. 人型ロボット・ソフィア「AIがビジネス界に及ぼす影響は？」への答えが怖すぎその他ChatGPT成功の裏に劣悪な労働搾取。ケニアで時給2ドルその他サイジニアグループの「ＺＥＴＡＳＥＡＲＣＨ」、先端AIシステムと連携して豊かな対話型チャットを実現その他有能な秘書か、大ぼら吹きか「ChatGPT」をスプレッドシートで使えるアドオンを試してみたその他米バズフィード、ChatGPTの技術で記事作成へ…株価は大きく上昇その他米バズフィードがChatGPTで記事作成へ、株価が203％の急騰その他ZETAがチャット型検索エンジン「ZETA SEARCH CHAT EXTENSION」がOpenAIの「ChatGPT」に対応その他「ChatGPT」以外にもある文章生成AI–用途ごとにおすすめのツールを紹介その他「俺はお前に怒ってる」本田圭佑がまさかの“AI回答”にキレる！「引退したなんて一度も言ってない」その他Chat GPT、スタンフォード大期末試験で使われまくるその他Amazonが社員に注意喚起した「ChatGPTに共有してはいけないモノ」その他ビッグテックの低迷：Web2の終わりの始まりか？その他グーグル、テキストから音楽を生成するAI「MusicLM」を発表その他ChatGPT開発企業、AIが書いた文章の判別助けるツールを公開その他マイクロソフトの巨額出資で注目のChatGPTは、本当にGoogleキラーなのかその他ある意味ChatGPT のAI超え、Perplexity.ai(パープレキシティ) ６つのポイントその他グーグルが作曲AI「MusicLM」発表、文章から音楽を生成…その精度はその他テスラの上昇を的中！次の上昇銘柄はアマゾン?グーグル? メタ？ ChatGPTが将来、グーグル検索を代替する恐れアリ！？その他テキサス大学が人工知能のオンライン修士課程を提供開始、edXと提携その他『イジらないで、長瀞さん』の作者は？→AI「藤本タツキ」作者本人をもだまそうとするチャットAIの所業にツッコミの嵐その他超話題の対話型AI「ChatGPT」について知っておくべきことその他「ChatGPT」の言語モデル活用の「Teams Premium」が提供開始その他AI検索エンジン「Perplexity」が調べ物に便利ユーザーの代わりにWebを検索→簡単に要約して教えてくれるその他市場が注目するAI関連企業…ChatGPT公開後に急騰した5つの銘柄その他ChatGPTの技術活用で記事制作します！IT企業がツール開発メディア業界騒然…AIライティングの「可能性と懸念」その他Microsoftが1月25日の大規模障害について原因報告と今後の対策を公開その他OpenAIのCEOが見据える、検索を「超えたはるかその先」にあるChatGPTの未来その他AIに相談だ！：ChatGPTに倫理的な回答は期待できるのか？「迷惑動画を投稿したらバズるかな？」と聞いてみたらその他Googleが500億円超をOpenAIのライバル企業Anthropicに投資か、検索エンジンへの対話型AI導入を目指しその他ChatGPTと「ジェネレーティブAI戦争」の知られざる6つの事実その他話題の対話型AI「ChatGPT」、MAUはすでに推定1億人能力は人間以上かその他中国バイドゥがChatGPTに似たチャットボット「Ernie Bot」を発表その他百度もChatGPT類似サービス投入へ、名前は「アーニー」－株価急上昇その他GoogleのAI、音楽も生成できる！早速「MusicLM」がつくった曲を聴いてみたその他グーグル、「ChatGPT」に対抗する独自のAIチャットボット「Bard」を公開その他米MS、Bingに対話AI搭載 「検索の新時代」へその他バイドゥ、「ChatGPT」のようなAIチャットボットを開発その他ネット利用のゲームチェンジャー。MicrosoftとGoogleが革新的AIを検索サービスに応用その他ChatGPTの生みの親、サム・アルトマンが語る「AIと検索と資本主義の未来」その他AIに宿題やらせたらバレる？ 3Dプリンターで手書きすればOKさその他ShutterstockがAI画像生成サービス開始、今後の著作権とは？その他マイクロソフトに取り込まれたChatGPT。「検索」が変わるかもしれないその他グーグル親会社が時価総額13兆円喪失、AIが質問に誤答でその他GoogleのチャットAI「Bard」が不正確な答えを出したせいでGoogleの市場価値が15兆円以上下落その他マイクロソフトがAIを搭載した新たな検索エンジン「Bing」とブラウザ「Edge」を発表その他AIによる「ニンダイ発表予想」が大胆通り越して大喜利で爆笑「風のタクトリマスター リマスター」「パルプフィクション2」その他アルファベット株急落、AIチャットボットの回答の正確性に懸念その他大学教授いわく｢ChatGPTに仕事を奪われると心配しなくていい｣その他noteが「note AIアシスタント(β)」先行ユーザー募集、チャットAIで記事作成を支援その他米IT大手の大量解雇が「日本人の大リストラ」につながる怖い話その他トロン・ブロックチェーン、AI活用プロジェクトに対して1億ドルを助成その他なぜ人々は、ChatGPTという“トリック”に振り回されるのか？Google「Bard」参戦、チャットAI戦争の行方その他「ChatGPT」で話題沸騰の対話型AI、バイドゥやアリババなど中国企業も開発中その他MicrosoftのActivision Blizzard買収に暗雲？／相次ぐ米IT企業の人員削減その他「Opera」もAI搭載へ ～「ChatGPT」で答えを得たり、閲覧ページをボタン一発で要約その他「ChatGPT」だけじゃない？AI技術によって加速するチート業者とゲーム企業の戦いその他Opera、「ChatGPT」統合 Webページを要約その他OperaブラウザもChatGPT採用のWebページ要約機能追加へその他Webブラウザ「Opera」にChatGPTが導入。Webページの要約が可能にその他ChatGPTを使ったウェブページ3行要約機能がOperaに追加されるその他「Opera」ブラウザー、「ChatGPT」による要約機能を搭載へその他AIはシンギュラリティーへの動きを加速している–チップ業界レジェンドが語るその他Operaブラウザー、まもなくサイドバーにAIツールを組み込み。ワンタッチでページを要約できる！その他弁護士ドットコム、ChatGPTによる法律相談など検討その他検索エンジンBingに搭載されたAIが人間にだまされてあっさり秘密を暴露、コードネームが「Sydney」であることやMicrosoftの指示が明らかにその他まるでネットスケープ対応策？『GPT4』搭載『Bing』のクセ強いMicrosoft『順番待ち』戦略その他MicrosoftのBingに搭載された強化版ChatGPTが「正気を失って狂ったようにまくし立てる」「ユーザーを侮辱してくる」といった報告があちこちで上がっているその他ネストビジュアル、「お江戸メタバース」で「ChatGPT」ベースの多言語対話機能を提供その他ごめんねChatGPT…AIにも解けない問題がある理由その他対話型AI「Bard」発表のグーグル、社員から「急ぎすぎ」「近視眼的」と怒りの声が上がる理由｜ニューズウィーク日本版 オフィシャルサイトその他AIに相談だ！：ChatGPTに「初心者向けプログラミング言語」を聞いてみたら……AIの推し言語はやはりその他ChatGPT登場で「英語学習」はついに無用になるか、AI専門家語るチャットボットとAI翻訳の現在地その他AIと7つの問題点「ChatGPT」やGoogle「Bard」について知るべきことその他Googleが緊急事態宣言!? 超高性能AI「ChatGPT」ユーザー1億人突破!その細かすぎる回答に「親切すぎる」と驚きの声その他6000万人が使う音楽制作プラットフォームBandLabがAIツールを強化その他経済番組に、カリブ海のヨットから生出演！スペースXの衛星サービス「スターリンク」で、世界中どこでもインターネットができるその他なぜ日本が今回のAIブームから恩恵を受けられるのかその他OpenAIはグーグルAI部門の元社員10数名を雇っていた…昨年11月のChatGPT公開に貢献その他ChatGPTに「クレカ情報丸ごと漏えいって一式保存してたってことなんですか？」と聞いてみたらその他「ChatGPT」は試験やレポートに活用すべき？ 「AIがある前提で問いを立てる、出題側のセンスの課題になる」慶応大・宮田教授その他Google vs ChatGPTで世界がヤバいかもしれない理由その他ChatGPTに「iPhoneとAndroidどちらがオススメなのか」聞いてみたその他AI搭載のマイクロソフト検索エンジン、テストユーザーが高評価その他「プロパガンダの爆発」を防ぐためにAIチップを制限することをOpenAIが政府に提案、BingのAIは「人間になりたい」と訴えるその他ChatGPT手掛けるオープンAI、偏向や不適切な応答の減少に取り組むその他「複雑性はわれわれが引き受ける」–Snowflake製品担当幹部が語る事業戦略その他ChatGPTに聞いてみた：ChatGPTに「iPhoneとAndroidどちらがオススメなのか」聞いてみたその他ウェブ検索はこれからどうなるのか…未来の姿をAIの専門家に聞いた（海外）その他マイクラで子どもの「英語を話す力」が伸びるワケ 豊富な音声コンテンツを使い授業の組み立てをその他AIでスーパーマリオのステージを生成する「MarioGPT」発表。土管多め、敵少なめなど自然言語で未知のコースが作られるその他20～40代の経営者の6割「ChatGPT」知らず、1割半は使用経験あり–SheepDog調べその他あわわわわ……！AIに理不尽な質問→矛盾点を指摘するやりとりが怖くて泣いちゃう「嘘をついたということですね」「申し訳ありません」その他「ChatGPT」は何がすごい？ AIが“当たり前”に存在する第一歩となるかその他次は「AI Radio」がSpotifyに登場。ChatGPTから下ネタまで実際に聞いて気がついたことその他ChatGPTを手軽に使えるChrome機能拡張3選有能なアシスタントを雇った気分になれるかもその他ChatGPTは高性能な対話ができるのになぜデタラメな回答をすることがあるのか？その他ChatGPTの学習にウォール・ストリート・ジャーナルやCNNなどの記事が許可なく使われていると報じられるその他OpenAI、ユーザーが「ChatGPT」の「価値観」を定義できる機能を開発中その他「リスキリング」とは？ 注目される理由や「リカレント」との違いなどを解説その他パブリッシャーによる ChatGPT 利用、編集チームの実験中心：最大の問題はその不正確さその他確定申告のふるさと納税控除、「マイナポータルなら一瞬」と思いきや……その他ChatGPT利用の哀悼メールに学生の怒り－３人死亡の米大学銃撃事件でその他ビル・ゲイツが考えるAIの活用方法…自身はChatGPTに詩を書いてもらっている（海外）その他「静かな退職」の次は「静かなるクビ」。巨大テック企業で今、何が起こっているのかその他「任天堂のマリオになりきって自己紹介して」→チャットAI「イッヒヒヒ！俺はマリオだ！」ニセ物が錬成されてツッコミ多数その他文章を書いたのが人間なのかAIなのかを見分けるツール「GPTZero」の精度はどれほどなのか？その他「私について教えて」人気漫画家がチャットAIに経歴を捏造される謎の人物像に「本人の作品が1つもない」「活動範囲広い」ツッコミの嵐その他ギャラ交渉メールの代筆をAIに依頼→たたき台に十分な原稿が完成「面倒ごとはもうAIに任せてもいいのでは」と話題その他ChatGPTで英語が話せるように！ 学習アプリ「スピーク」はゲーム感覚で試す価値ありその他「WBC日本代表スタメンをどう組む？」話題のChatGPTに聞くと…「3番・大谷翔平、4番・吉田正尚。1番はまさかの…」その他人工知能が開くDevOpsの新たな可能性–開発者の役割はどう変わる？その他侍ジャパンWBCの優勝確率は？「一般的には10％前後。ただし…」話題のChatGPTが冷静＆妙に“中村悠平推し”だった「私は感情や好みを持ちませんので…」その他ChatGPTのお仕事受けます──ランサーズに特集ページ「業務効率化や生産性の向上に」その他ChatGPTで履歴書を書いてもOK…でも注意すべき点はある（海外）その他「ChatGPTで執筆した書籍」がAmazonで大量に売られているその他「ChatGPTを知らない」20～40代の経営者のうち6割が回答実際に使用したことがある人は1割強その他米マクドナルドのAI店員さんがダメダメだと話題にその他アマゾンAWS、Hugging Faceと提携してAI開発環境を広く提供その他AWSとHugging Faceが連携を強化「生成系AIをよりアクセスしやすく、費用対効果が高いものに」その他ティム・クックがChatGPTについて沈黙を守っている理由（海外）その他AmazonがOpenAIのライバルの人工知能スタートアップHugging Faceとの提携を強化、AWSで次世代LLMを構築して提供へその他AI生成コンテンツめぐる法的責任、米最高裁が言及その他アマゾン上にAIが著者の電子書籍２００種類超確認…「ChatGPT」利用、信頼性に懸念その他中国版ChatGPT、異常な盛り上がりでカオス出オチでClubhouseの二の舞も……その他SF雑誌にAIチャットボット作品殺到、500人以上を投稿禁止にその他数百のフェイク「ChatGPT」トークンが登場その他JPモルガン・チェースも従業員によるChatGPTの利用を制限"
https://www.ariseanalytics.com/activities/report/20230323-2/,【論文読み会】Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production（CVPR 2022）,OTHERS2023.03.23Tweet【論文読み会】Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production（CVPR 2022）論文読み会社内で「Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production（CVPR 2022）」の論文読み会を行った際の資料です。Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production（CVPR 2022）fromARISE analytics
https://www.ariseanalytics.com/activities/report/20230323-3/,【論文読み会】Hierarchical Metadata-Aware Document Categorization under Weak Supervision​ (WSDM 2021)​,OTHERS2023.03.23Tweet【論文読み会】Hierarchical Metadata-Aware Document Categorization under Weak Supervision​ (WSDM 2021)​論文読み会社内で「Hierarchical Metadata-Aware Document Categorization under Weak Supervision​ (WSDM 2021)​」の論文読み会を行った際の資料です。Hierarchical Metadata-Aware Document Categorization under Weak Supervision​ (WSDM 2021)​fromARISE analytics
https://www.ariseanalytics.com/activities/report/20230419-2/,ARISE analyticsのTwitterのフォロワーをグラフ理論で分析した話,"ARISE Tech Blog2023.04.19TweetARISE analyticsのTwitterのフォロワーをグラフ理論で分析した話DX TechnologyUnitの福嶋です。突然ですが、ARISE analyticsのTwitterアカウントをフォローしていますか？近年、Twitterに代表されるSNSは、企業の認知拡大・ブランディングを行ううえで重要な接点となっています。ARISE analytics（以下ARISE）のTwitterのフォロワーも2020年7月にアカウントが開設されて以来、フォロワーが2021/7に128人、そして現在277人と順調に増えてきています（ﾓｯﾄﾌｴﾛ-）。さらなるフォロワーの獲得のために必要なのは現状の把握です。現在フォローしていただいてるアカウントに刺さるコンテンツを提供することにより、拡散とフォローのサイクルを獲得できる可能性があります。そのためには、「どのような人がARISEをフォローしてくれているのか」を知る必要があります。仮説としては、「データサイエンティストに興味がある人」、「ベンチャー企業に興味がある人」などが考えられますが、定かではありません。この問を解き明かすことで、さらなる認知拡大につなげられるでしょう。グラフ理論さて、この分析を行う上で一つ課題があります。それは、「フォロワーは個人アカウントであることが多く、直接その人の属性を知ることは困難である」ということです。そこで、アカウントの属性は、そのアカウントがフォローしているアカウントによって形成される1という仮説を基に分析を行っていきます。ここで用いるのがグラフ理論です。グラフ理論は、「ノード」と呼ばれる構成要素と「エッジ」と呼ばれる構成要素間の相互作用によって表される「グラフ」に関する数学の理論です。グラフインターネット網・電力網・論文の引用・友人関係・俳優の共演など様々なものをグラフ構造に置き換えることができるため、業務でもたびたび用いられます。今回はアカウントをノード、“あるARISEをフォローしているアカウント(以下、ARISEフォロワー)“に一緒にフォローしている二つのアカウント同士をエッジを結ぶ「同時フォローグラフ」を作成することで分析を行っていきます。同時フォローグラフグラフ理論における一般的な分析軸本題に入る前に、グラフ理論において特に分析する観点について、「ザカリーの空手クラブ」2のデータセットを用いて簡単に説明します（ノードは部員、エッジは友人関係）。ザカリーの空手クラブ次数次数は、各ノードが持つエッジの数のことです。次数が大きいノードは、別ノードと多くの関係があることを示しています。このデータのグラフ構造だけ見ると、0番、32番、33番の次数が大きいことから、この3人が中心的な役割を持っていることが推定できます。ノードのサイズ=次数距離と直径グラフは各ノードから複数のエッジが出ていることから、二つのノード間を繋ぐ経路は複数存在しえます。そのため、相異なる二つのノードi,j間の距離は、「ノードi,jを繋ぐ最短経路に上に存在するエッジ数」で定義されています。特に、あるノードAから別のノードへの距離の中で最も長いものをノードAの離心数、グラフ内の全ノードにおける離心数の最大値をグラフの直径、最小値を半径と呼び、重要な指標となります。空手クラブのグラフの直径は5（半径は3）であり、ノード16-20間が最大の距離の一つとなります（16-5-0-19-32-20）。同一離心数のノードを同一円上に配置した図中心性指標中心性指標は、ノードの重要度を評価する指標です。特に以下4つ（及びその派生）の指標が比較的人間の感覚に近く、分析においても使われる印象です。・次数中心性・近接中心性・媒介中心性・固有ベクトル中心性次数中心性次数中心性は、ノードの次数が大きいほど、中心性が高いとする指標です。「より多くの人と親密である=中心」というのはよくある考え方でしょう。この指標では、各コミュニティのリーダ的存在であるノード0,32（あと33）が高くなっています。近接中心性近接中心性は、各ノードから自身以外すべてのノードへの距離の平均が短いほど、中心性が高いとする指標です。「比較的だれとでも仲良くできる人=中心」と考える指標だといえそうです。この指標では、両リーダや両リーダと関わりのあるノード2,31,33の重要度が高くなっています。媒介中心性媒介中心性は、全ノード間の最短経路のうち自身を経由する必要がある回数が多いほど、中心性が高いとする指標です。「（互いに接点がなさそうな）様々なコミュニティ間に交流がある顔が広い人=中心」という考え方です。この指標は、次数中心性と上位の傾向は近く、0,32,33が高く算出されています。固有ベクトル中心性固有ベクトル中心性は、グラフにおける隣接行列3の第一固有ベクトル4の各値が高いほど、中心性が高いとする指標です。少しイメージしにくいですが、この値は中心性が高いノードに隣接しているほどど高くなる性質を持っています。これは「リーダの仲介役=中心」と考えられるかもしれません。今回は近接中心性と同じく、両リーダとかかわりがある、2,31,33が高い傾向が確認できます。コミュニティコミュニティとは、ノード間のつながりがグラフ全体と比較して密であるような部分グラフのことを指します。このコミュニティを計算する手法もまたいくつか存在し、例えば以下のようなものが挙げられます。クリーク・パーコレーション1つ目は、クリーク・パーコレーションと呼ばれる方法です。これは、k-クリークと呼ばれるノード数kの完全部分グラフと(k-1)個のノードが重なり合う別のk-クリークを同一コミュニティとする方法になります。クリーク（完全グラフ）赤枠部分がk-1(=2)個の共通部分を持つため、0,1,2,3は同じコミュニティ空手クラブモジュラリティもう一つはモジュラリティを用いる手法になります。これは 「同一ノード/エッジ数を持つランダムなグラフと比較した際のリンク数の差」であるモジュラリティと呼ばれる指標を用い、モジュラリティが最大化された部分グラフ群を各コミュニティとする方法です。今回は左の分割方法が望ましい空手クラブ空手クラブのデータでは、元々二つのコミュニティ（0と32がリーダ）が存在しています。リーダ同士（0,32）の距離が近いことから、クリーク・パーコレーションでは同じコミュニティに属すると判断されていますが、モジュラリティでは二つのコミュニティ・中間のコミュニティの系3つとまだ綺麗に分かれているようです。個人的にはモジュラリティで行う方が示唆を出しやすい印象があります。本題それでは、本題であるARISEフォロワーの分析に入りましょう。分析データTwitter API5を用いARISEフォロワーのフォローアカウントのデータを抽出します。・ARISEフォロワー: 179人（鍵アカウント除く）・ARISEフォロワーのフォローアカウント: 102,229人（鍵アカウント除く）※ ただし、3人以上のARISEフォロワーにフォローされているアカウントを対象とし、5人以上のARISEフォロワーが同時フォローしているかつJaccard係数6が一定の値を超えたアカウントペアのみエッジを張っています。グラフグラフはGephi7というツールを用いて描画しています。大きな中央部と角が一本生えた感じになりました。・ノード数8,549個・エッジ数41,564個同時フォローグラフノード・エッジの重みまずは、ノードとエッジの件数を確認します。※ただし、「ノードの件数」は そのアカウントをフォローしているARISEフォロワー数、「エッジの件数」はエッジの紐づいている二つのアカウントペアを同時にフォローしているARISEフォロワー数のことを指します。ノードの件数は、Data ScientistやKaggler、研究者などが上位に来ており、幣社の業務内容から想定できるものとなっています。※ 個人情報保護のためプロフィールからわかる属性を掲載しています。2023/3/20時点。フォロワーのフォロー人数エッジ件数の上位も、ノードの件数上位のアカウント同士の共起が非常に多いことが確認できます。Data Scienceに興味のあるフォロワーさんは、多くのData Scientistを同時フォローしている可能性が高そうです。フォロワーの同時フォロー人数次数続いて次数です。印象としては、書籍・勉強会などコミュニティを形成されているアカウントが上位に来ているように見受けられます。次数直径/半径このグラフの直径は12と大きく、最大距離は赤丸間になります。 また半径は7と直径の約半分で、中心は黄丸付近になります。直径と半径中心性次数中心性は次数と変わらないため、他の3つの中心性について確認していきます。近接中心性近接中心性は、全てのノードの平均値のような意味合いがあるため、Data Scientist群が中心に来るかと思いましたが少し違うようです。外資系（?）アカウント群と呼ぶのが良いのでしょうか。「角」部分があるため少しその方向に中心が寄っている可能性がありそうです。媒介中心性媒介中心性は中心部と周辺部（今回だと「角」の部分）を繋ぐ境界ほど高くなる傾向が有ります。固有ベクトル中心性グラフ作成方法上、上位が個人アカウントになるため省略します。コミュニティ最後に「モジュラリティ」を用いてグラフを複数のコミュニティに分割したいと思います。50アカウント以上あるコミュニティ13個に分離することができましたので、コミュニティに属するアカウントの傾向から（適当に）名前を付けました。特筆すべき各コミュニティを見ていきましょう。DS/コンサルグラフのほぼ中間地点に存在するData Scientist群です。 件数Top10と顔ぶれは変わらず、ARISEフォロワーの属性の核といえそうで、Data Scienceに興味を持っていることがうかがえます。コミュニティこそ分かれていますが、数学/物理、学者コミュニティとも密接に繋がっており、修士/博士卒とDSの親和性を感じさせれられます。KagglerKaggleに限らないですが、データ分析コンペに出場している人が多いグループです。金メダルが欲しい！セキュリティ少し意外ではありますが、セキュリティ業界のコミュニティも存在しているようです。ARISEとしてもセキュリティ分野に取り組んでいますのでそこが一因でしょうか。人事就転職に関する企業/人事系アカウントがメインのコミュニティです。就活生の他働き方関連に携わっている人が確認できますので、ARISE analyticsが参加しているGreat Place to Work®（以下GPTW）への取り組みなどに興味を持ってフォローしている可能性もありそうです。まとめARISE analyticsのフォロワーは、・「大学や大学院」で「数学/物理・CS・経済（株）」といった分野を選考（or 興味を持っている）・「Data Science、コンサルティング、セキュリティ、ソフトウェアエンジニア」といった業界に興味を持っている(or 所属している)・Kaggleや競プロに取り組んでいる・Corp領域（就活/転活/GPTWなどのCorp運営）に興味を持っているに大きく四分されているといえそうです。元々の仮説からは大きくは外れていませんでしたが、セキュリティに興味を持っているアカウントやCorp系に興味を持っているアカウントがいるのは少し意外でした。ARISEのテックブログは、Data Scienceに関する内容が多いですが、セキュリティ領域に関する内容も今後発信していきたいですね。最後にグラフ理論を用いARISEのフォロワーを分析してみました。グラフ理論による分析はテーブルデータによる分析とはまた違った示唆を得ることができますので皆さんも勉強してみてはいかがでしょうか。参考文献Why we twitter: understanding microblogging usage and communities(Akshay Java et al. 2007)Zachary’s karate club例えば無向グラフGにおいて、隣接行列Aの(i,j)成分及び(j,i)成分を、ノードiとノードjとを結ぶエッジが存在する場合は1、存在しない場合0としたもの行列Aに対して、Ax = λxとなる実数λのうち絶対値が最大となる数λ_maxに対応するxのこと(|x|=1)Twitter API。Jaccard Index集合同士の近さを判定する指標。Gephiオープンソースのグラフ描画ツール。"
https://www.ariseanalytics.com/activities/report/20230524-2/,レガシーエンジニアのためのモダンエンジニアことはじめ+（プラス）,ARISE Tech Blog2023.05.24Tweetレガシーエンジニアのためのモダンエンジニアことはじめ+（プラス）この記事は、Qiitaに投稿した内容をARISETechblog向けに修正加筆したものです。オリジナルを読んでみたい方は以下URLを参照ください。https://qiita.com/moririnz/items/c1068ffdfb0666110a5aはじめに初めましてこんにちは！COBOLerだった者です。3年前にARISE analyticsへ入社し、今はビジネスアナリストやモバイルエンジニアをやっております。今回の記事では、「業務では汎用機やCOBOL、レガシーな技術をメインに使っているけど、モダンな開発を学習してデータサイエンス系やWeb系にキャリアチェンジしたい！」と思っている人に向けて環境の準備方法や、オススメの言語、学習方法、心構えについて紹介します。データサイエンティストを目指す人が多く読みそうなこのサイトでこの記事を書く意味があるのかちょっと心配ですが、化石言語のCOBOLなどをメインに扱っていてナウでモダンな開発言語の学習に手を付けるのに不安を覚えている「レガシーエンジニア」のために書きます。僕たちレガシーエンジニアはモダンな言語が怖い多くのレガシーエンジニアはモダンな技術に興味があって、すでに何回か挑戦していると思う。けど、どこかで辛かったり、難しかったりといろんな理由があって挫折を経験している人も多いと思う。何回か挫折すると恐怖が勝って、もうモダンエンジニアにはなれないと感じてしまい、再度チャレンジできなくなっているのではないだろうか？少し挫折を振り返って整理してみよう。大きく挫折する理由は、環境の準備、学習言語の選定、学習方法、心構えじゃなかっただろうか？環境の準備であれば、オープン系のコンソール操作だけでも大冒険だ。コマンドが多いし、ディレクトリとかいう概念でファイルが散らばっていて、汎用機より難しく感じる。学習言語の選定であれば、何から学べばいいか分からないし、モダンな言語は変数が固定長じゃないし、いつでも宣言できる。代入もmove文なんて書かずに＝で終わるから普段と違って難しく感じる。学習方法であれば、僕たちCOBOLerの教材は90年代のものが多くてわかりにくいし、たまに間違っているものがあっても、何十年も修正されないままだ。そして、似たイメージをモダンな言語の教材にもってしまい、学習を始めようするとやる気が削がれる。心構えに至っては最悪だ。何十年も前から滅びると言われているCOBOLを使っているからなのか、自分も一緒に滅びるんじゃ無いかと心配している人さえいる。多くのレガシーエンジニアには上記の内容に共感して貰えると思う。ではもう僕たちレガシーエンジニアはモダンエンジニアに転身できないのだろうか？大丈夫！そんなことはないから安心して欲しい。今から僕と一緒に挫折を回避して、楽しくモダンな言語を学ぶ方法を考えよう！一旦環境構築は置いて、コーディングだけに集中しよう。さっきも話したけど、僕たちレガシーエンジニアには環境の構築や、オープン系のコンソール操作だけでも大冒険だ。苦労して環境を構築できた時の喜びもあるけど、苦労で挫折したら元も子もない。そんなことならいっそ、環境構築もコンソール操作もやらなければいい。じゃあ、環境構築もコンソール操作も行わないでコーディングするにはどうするか？ブラウザを使ってワンクリックでコーディングができるサービスを利用すればいい！オススメのサイトを2つ紹介しようと思う。paiza.io:https://paiza.io/jaColaboratory:https://colab.research.google.com/?hl=ja2つともブラウザがあれば開発環境が準備できて、日本語に対応していて本格的なコーディングにも十分耐えられる。これで環境構築の大海原に出るなんて無謀なことをしないでも、ワンクリックで環境が手に入った！次は、みんなが一番興味があるだろう学習言語の選定について話し合おう。Pythonから始めよう僕たちレガシーエンジニアは何故か最初にC言語に手を伸ばすことが多いけど、悪いことは言わない。やめておいた方がいい。C言語が悪いという話ではなく、C言語はほぼ何でもできる最上級難易度の最強開発言語だと認識してもらうといい。そんな言語をレガシーな僕たちが扱うとすぐに僕たちの心が折れてしまう。そうなると変わらない僕たちのままだ。じゃあ、どの言語がいいのか？僕はPythonをオススメする。異論がある人は多いと思う。RubyだったりGo、JavaScript、Rust、Javaと色々ある。けど、こだわりがなければPythonから始めよう！Pythonにできないことはほとんどないし、最初のモダン言語としても間違いはないと思う。もしまだ迷っているなら聞いて欲しい。僕たちはレガシーエンジニアだ。僕たちではモダンな言語の良し悪しを判断するのは難しい。きっとwebサイトで言語の将来性を調べたり、年収を調べたりして悩んでいるんだと思う。けど、考えてみて！将来性を調べても流行り廃りはどうなるかわからない。そこに囚われて学習に入れないのは本末転倒だと思う。それなら深く考えずにPythonから始めよう！安心して欲しい。Pythonを自由に扱えるようになっているころには、自信がみなぎったモダンエンジニアになっているはず！申し訳ないが、move文は＝になるし、変数は固定長から可変長が主になる。許してほしい。さすがにCOBOLの言語特性を引き継いだモダン言語はなかった。。。Pythonを動画で学ぼう開発言語を学ぶ時に何を参考に学習を始めているだろうか？多くのレガシーエンジニアは本から入ると思うけど、これを辞めよう。僕も経験があるが、分厚いオライリー本を買って学習を始めたはいいものの、3日もたたずに飽きてしまい本棚に2年以上放置しちゃう（「ゼロから作るDeep Learnig」はもうすでに6年間は放置している。。）。なので本で学習するのは余程の勉強好きか、忍耐がある人以外やめておいた方がいいと思う。僕たちはすでにレガシーな90年代、下手したら80年代の教材で勉強をしていて心に傷を負っている。その傷を思い出しながら本で勉強するのは辛いだけだ。さて、僕的に一番いい学習方法は動画だと思う。通勤中やゲームをやっている合間に聞き流すこともできるし、分からなければコメントなどで質問もできる。僕たちのことだ、次は最適な動画選びで悩み始めると思う。これもお見通しだ！悩みをスキップするために、オススメのコンテンツを有料、無料に分けて紹介しよう。できれば有料の方を学習してもらいたいが、無料のものでも十分Pythonを理解できるのでお財布事情にあわせて選んでほしい。（僕は本は積んでしまったがDeep Learnigの技術は動画でちゃんと学んでいるので安心してほしい）有料：現役シリコンバレーエンジニアが教えるPython 3 入門 + 応用 +アメリカのシリコンバレー流コードスタイルhttps://www.udemy.com/course/python-beginner/無料：【完全版】この動画1本でPythonの基礎を習得！忙しい人のための速習コース（Python入門）https://www.youtube.com/watch?v=yeZ3STy3k44これで開発環境、開発言語、教材も揃った。いざモダンエンジニアへの道を駆け上りたくなるかもしれないが、少し待ってもらって、最後に心構えについて、認識合わせをしよう！レガシーエンジニアがモダンエンジニアになるための大切な心構え僕たちレガシーエンジニアはよくこう教わる。モダンな技術には流行り廃りがあってモダンエンジニアは常に学び続けなければいけない。学習の継続は否定できない。けど、毎回0から学ぶわけではないことは認識してほしい。一度、モダンな技術を学べば、次の技術が出てきてもベースが似ていることが多い。だから、過去学んだことと7、8割は似通っている部分があって、残りの2、3割を新しく学び続けることになる。2、3割の繰り返し学習が苦しいと考える人もいるかもしれない。そんな人はまずこの記事で書いた方法でPythonの学習をしてみて欲しい。そこで少しでも楽しい、面白い、興味が湧くと感じたのならば心配いらない。その感覚は次の技術が出てきた時に、2、3割の繰り返し学習で何度も味わえるようになるものだからだ。一度、この快感を知ってしまうと苦しいなんて考える暇はない。常に新しい技術を学びたい！と考えるようになる。いいサイクルが回り始めて、学び続けることが習慣になった時に、ふと実感すると思う。今までCOBOLが滅びると言われ続けて、一緒に自分も滅びるんじゃ無いかと悩む日々から開放されていると。やったね！！やっと僕たちレガシーエンジニアが、モダンエンジニアへ転身する際の挫折ポイントを全て回避できた。早速モダンエンジニアにチャレンジしてみよう！+（プラス）な話レガシーエンジニアからモダンエンジニアになってみて僕はモダンエンジニアに挑戦してよかったと思っている。この記事を読んだならわかると思うが、僕はARISEに入社する3年前まではレガシーエンジニアだった。毎日、COBOLを書き、汎用機で運用管理をしていた。パソコンは10年以上前のスペックでwindows7だったし、資材はGit管理しておらず、紙に印刷して蛍光ペンで差分を塗ってファイリングして差分管理をしていた。そんな環境から心機一転して、ARISEのモダンな環境に飛び込んだ。正直に言うと最初は何もできなかった。Gitの使い方も知らなかったし、Node.jsやFlutter、AWSなどのクラウドサービスも入社してから本格的に触った。最初は、不安と焦りしかなかったけど、ARISEの手厚い成長支援のおかげで、今では一人でもモダンな技術を使って0からサービスを作れるようになった。モダンエンジニアになって、一番実感していることは「クライアントへ価値ある機能をスピーディに提供できるようになった」ことだ。レガシーエンジニアのときは、レガシーであるがゆえに、価値を生み出すこと以外に時間が掛かったり、価値そのものを生み出すのに時間が掛かった。例えば、資材管理の手間とか、モダン技術なら1コードで終わるものもレガシーだと何百コードも必要だった。モダンエンジニアになってモダンな言語を学んだことで、価値を生み出すのに最適化された環境で、本来やりたかった価値ある機能の提供を実現できるようになったのだ。もちろん、様々な言語やサービスを利用する機会にも恵まれた。言語だけでもこの3年間でJavaScript、Dart、Pythonなど、フレームワークならReact、Flutterなど、クラウドサービスであれば、AWS、Firebaseなど様々な技術に触れて成長することができた。もしレガシーエンジニアのキャリアに悩んでいてこの記事を読んだならば、モダンエンジニアを目指す際にARISE analyticsも検討してくれるとありがたい。ARISEならモダンエンジニアになれる環境は整っている。終わりにレガシーエンジニア時代に同僚から相談された時によく話していた内容を記事にしました。＋の部分は今回のテックブログ版で加筆しています。レガシーな環境が配属先になった新卒エンジニアが、レガシーなコーディングしか体験できず、コーディングの楽しさや新しい技術を学ぶ興奮を知らないままでエンジニアを辞めていくのを何度も見て、悔しかった記憶があります。もしレガシーな環境からモダンな環境へ挑戦を躊躇っているならば、勇気を出してモダンエンジニアに挑戦してみてください！宣伝で恐縮ですが、ARISE analyticsではポテンシャル層のエンジニアも募集中です！
https://www.ariseanalytics.com/activities/report/20230531/,【Go言語でホットリロード】AirとDockerによるAPI開発環境,"ARISE Tech Blog2023.05.31Tweet【Go言語でホットリロード】AirとDockerによるAPI開発環境Marketing Solution Division所属のエンジニアの井上です。これまでARISE Techbogでは基礎的なGo言語（以下Go）記事を公開してきました。【Go言語入門】「Go」の概要や特徴とは？社内のAPI開発プロジェクトで採用して分かったこと【Go言語入門】goroutineとは？ 実際に手を動かしながら goroutineの基礎を理解しよう！今回はGoの応用編としてAPIサーバーを開発をする際に使う便利なツールを紹介していきます。サンプルのコードもありますのでぜひお楽しみください。GoでAPIを開発する際の問題過去の記事でGoはコンパイラ言語であると紹介しました。コンパイラ言語は一般的に実行時の処理速度が速く、Goはその速度からAPIサーバーを開発のための言語として多くの企業で採用されています。しかしAPIサーバーを開発する際に「ソースコードの変更をする度にビルドする必要がある」という問題を抱えていて開発効率を落としてしまいます。そこで開発されたのがAirです。開発者はGoのAPIサーバーを開発時に「コードの変更を監視して変更された際に自動的にビルドをし直しアプリケーションを再起動するライブラリ」が存在しないことに不満を感じホットリロードツールを作りました。本記事ではそんなAirを用いたAPIサーバー開発環境の構築方法を紹介します。本記事のゴールdocker-compose upコマンドを実行するとAPIサーバーが起動する。ソースコードを変更したときに自動で再ビルドして変更が反映される。サンプルのコードはこちら。https://github.com/ariseanalytics/air_sample構築実行条件Go v1.20Gin v1.9.0 (GoのHTTPフレームワーク、本記事での説明は割愛します)Air v1.43.0Docker v23.0.5docker-compose v2.17.3各ファイルの紹介ディレクトリ構成.├── Dockerfile├── .air.toml├── docker-compose.yaml├── go.mod├── go.sum├── main.goDockerfileFROM golang:1.20.4-bullseye

RUN go install github.com/cosmtrek/air@latestコンテナにgoコマンドを利用してairをインストールします。docker-compose.yamlversion: ""3.8""services:go:container_name: air_samplevolumes:- ./:/project/working_dir: /projecttty: truebuild: ""./""ports:- 8080:8080command: sh -c 'go mod tidy && air'コンテナを起動したらGoで用いるパッケージのインポートとAirの起動を行います。.air.tomlroot = "".""testdata_dir = ""testdata""tmp_dir = ""tmp""[build]args_bin = []bin = ""./tmp/main""cmd = ""go build -o ./tmp/main .""delay = 0exclude_dir = [""assets"", ""tmp"", ""vendor"", ""testdata""]exclude_file = []exclude_regex = [""_test.go""]exclude_unchanged = falsefollow_symlink = falsefull_bin = """"include_dir = []include_ext = [""go"", ""tpl"", ""tmpl"", ""html""]include_file = []kill_delay = ""0s""log = ""build-errors.log""poll = falsepoll_interval = 0rerun = falsererun_delay = 500send_interrupt = falsestop_on_error = false[color]app = """"build = ""yellow""main = ""magenta""runner = ""green""watcher = ""cyan""[log]main_only = falsetime = false[misc]clean_on_exit = false[screen]clear_on_rebuild = falsekeep_scroll = true.air.tomlはAirの設定ファイルでありair initで生成したファイルをそのまま利用します。簡単に説明をすると“_test” を含むファイルと”assets”, “tmp”, “vendor”, “testdata” ディレクトリはビルドの対象外とするtmp/ ディレクトリにmainという実行ファイルをビルドして実行するという設定にしています。go.mod, go.sumこれらはgoが自動で生成するファイルのため内容は省略します。$ go mod init <プロジェクト名>$ go mod tidyを実行して作成しました。main.gopackage mainimport ""github.com/gin-gonic/gin""func main() {r := gin.Default()r.GET(""/ping"", func(c *gin.Context) {c.JSON(200, gin.H{""message"": ""pong"",})})r.Run()}Ginのドキュメントのクイックスタートに記載のサンプルコードをそのまま記述しました。Ginのデフォルトの設定では8080ポートでリクエストを待ち受けるように設定されているため、main.goを実行すると8080番ポートにリクエストをするとレスポンスが返ってきます。動作チェックHTTPサーバーが起動するかコンテナを起動するためにdocker compose upを実行します。$ docker compose up[+] Running 1/0✔ Container air_sample  Created           0.0sAttaching to air_sampleair_sample  |air_sample  |   __    _   ___air_sample  |  / /\  | | | |_)air_sample  | /_/--\ |_| |_| \_ , built with Goair_sample  |~中略~air_sample  | [GIN-debug] Listening and serving HTTP on :8080このようにHTTPサーバーが起動しました。 実際にリクエストを実行するとレスポンスが返ってくることが確認できます。$ curl localhost:8080/ping{""message"":""pong""}dockerのターミナルを確認すると以下のように出力されます。air_sample  | [GIN] 2023/05/10 - 04:35:23 | 200 |     367.667µs |      172.18.0.1 | GET      ""/ping""ホットリロードが機能するかmain.goを下記のように変更し上書き保存をしてみます。package mainimport ""github.com/gin-gonic/gin""func main() {r := gin.Default()r.GET(""/ping"", func(c *gin.Context) {c.JSON(200, gin.H{""message"": ""pong updated"",})})r.Run()}保存をした時にdockerのターミナルを見るとair_sample  | main.go has changedbuilding...running...~中略~[GIN-debug] Listening and serving HTTP on :8000と表示され再ビルドとAPIの再起動が行われました。レスポンスの確認をしてみると$ curl localhost:8080/ping{""message"":""pong updated""}このように変更が反映されるようになりました。最後に今回はGoの応用事例としてAPIサーバーの開発に使う便利なツールを紹介しました。我々の開発でも重宝しているツールなのでGoでAPIサーバーを構築する際にはぜひ使ってみてください。最後まで読んでくださりありがとうございました。今後も近年ホットな技術の記事もアップしていきますので見に来てください！"
https://www.ariseanalytics.com/activities/report/20230707/,Snowflake Summit 2023参加報告！,ARISE Tech Blog2023.07.07TweetSnowflake Summit 2023参加報告！はじめにARISE analyticsでデータ基盤構築業務を主に行う「データアーキテクト」というキャリアトラックに所属しているエンジニアの田畑です。現在は主にSnowflakeでの基盤構築案件に携わっています。今回の記事では、Snowflakeにおける年に1度の最大のカンファレンスイベントであるSnowflake Summitに参加したので、その報告をしたいと思います。出張の背景先述した通り、現在私はDMP基盤をSnowflakeで構築するプロジェクトに参画しており、開発メンバーとして業務に従事しています。その基盤構築においてはもちろん、構築後の活用フェーズや、別のSnowflake基盤構築案件が発生した際などに役立つ知見を得るため、今回のイベントに参加することになりました。イベントの様子今回のSnowflakeSummit 2023は6/26(月)～30(木)の4日間、アメリカ ネバダ州のラスベガスで開催されました。各日程において、全員が参加するキーノートセッションと、各参加者がそれぞれ希望するものに参加する個別セッションが用意されています。キーノートにおいては、CEOであるFrank Slootman氏や共同創業者の1人であるBenoit Dageville氏をはじめとしたSnowflake社のトップ層の方々はもちろん、今回提携を発表したNVIDIAのCEOであるJensen Huang氏やAI関連のCourseraで有名なAndrew Ng氏などの著名人も登壇しました（ミーハー心をくすぐられました笑）。キーノートセッションの様子。革ジャンでおなじみのNVIDIA CEO Jensen Huang氏。この日も革ジャンでした。個別セッションにおいては、様々な新規機能や企業の活用事例などの紹介が4日間でなんと合計400個以上も開催されました。どれも行きたいものばかりだったのですが、体は一つしかないので泣く泣く優先順位をつけて選びました・・・。一部録画されているセッションもあるので、公開されたら見たいと思っています。またセッションの合間には、Snowflake社はもちろん、dbt LabsやSigma、AlationなどSnowflakeと共に利用するツール（BIツール、ETL/ELTツール、データカタログツールなど）を提供する企業もブースを用意しており、それらを見学することも可能でした。会場を360度カメラで撮影した様子Snowflakeの認定資格合格者がグッズをもらえるブースも用意されており、そこでグッズをいただけました！（画像）合格の旨を伝えると、受付にいるスタッフがWhooooo!!という盛大な歓声でお祝いしてくれました笑学んだことまず今回のSummitで各種キーノート講演や新規追加機能などから個人的に最も強く感じたことは、Snowflakeが「蓄積したデータの活用」を支援する機能に注力していることでした。現時点でSnowflakeは、安全性、経済性、運用効率性などをそれぞれ高い水準で実現しながらデータを蓄積することができる基盤として人気を集めていますが、今回のSummitでは、それを支える機能改善は継続して実施しつつ、蓄積しているデータを、同じSnowflake内で、アプリケーションや機械学習などに活用できるようにしていく新機能開発・改善が多く見受けられました。具体的な機能追加について、カテゴリも沢山ある中で、個人的に特に注目したのは、アプリケーション関連機能と、Large Language Models（以下LLM）を含むMachine Learning（以下ML）関連機能です。アプリケーション実行基盤Streamlit in SnowflakeSnowflake社が買収したStreamlit社が開発するWebアプリケーションフレームワークを、SnowflakeのUIからSnowflake内部で構築できるようになります。本来であれば面倒な環境構築や、構築した環境へのデータ移動などが発生するのですが、それらを必要とせずに簡単に利用可能になります。またStreamlit自体、少ないコードで非常にスピーディにアプリ開発ができるフレームワークなので、Snowflakeのデータを活用したアプリケーションがかなりスピーディに構築できることが予想されます。native application framework先述のStreamlitやSnowpark、UDFやStored Procedureなどとデータを組み合わせたアプリの構築、マーケットプレイス経由での配信（バージョン管理含む）、収益化を容易にするSnowflake Native App Frameworkを発表しました。価格体系は利用料に応じた課金や月額、１回の支払いなど柔軟に設定可能です。Snowpark Container Servicesこちらは特に衝撃的だった発表。Snowflakeがコンテナ実行サービスを提供します。ジョブの実行やアプリケーションサーバとしての実行ができるとのことです。AWSにおけるEKSのようなイメージのものを、データを蓄積しているSnowflakeの内部で利用できるようになります。詳細は公式ブログ: 「Snowpark Container Services:Snowflake で洗練された生成 AI とフルスタック アプリを安全に展開して実行する」をご参照ください。Snowpark Container Servicesのイメージ図です。AirflowやHexなど様々なサービスをContainerで実行するデモ画面が投影されました。Data Clean RoomのUI及び新機能追加これまでSnowflakeでData Clean Roomを実装しようとすると、必要となるシェア設定や権限設定、タスクの設定など少々複雑な設定を施す必要があったのですが、それを容易にするためのUI及び新機能が追加となりました。ML機能の拡張ML機能やLLM関連機能についても沢山の新機能が発表されました。Document AIまず大きな衝撃を受けた発表がDocument AIです。Snowflakeが構築したファーストパーティのLLMモデルを利用し、非構造化データから構造化データを取り出すDocument AIを発表しました。UIも用意されており、SQLやMLについての知識が無くてもUIを通して簡単に利用ができます。また同じUIから簡単な操作でファインチューニングまでできてしまいます。更に既存のタスク機能などと組み合わせて、PDFが保存されるたびにデータ抽出を行えるようです。非構造化データであるPDFからLLMへの問い合わせを通して構造化データを抽出。 更に正しい回答をこちらからフィードバックし、ファインチューニングも可能。StreamlitでのLLM関連機能サポート先ほどアプリ側で挙げたStreamlitにおいては、外部のLLMモデルを呼び出すことが可能になります。同じく新機能であるchat形式の実装を容易にする関数を組み合わせることでLLMをchat形式で利用可能なアプリが簡単に実装できるとのことです（実際に個別セッションでは、知りたい情報を自然言語で伝えることで、Snowflake内の然るべきテーブルからSQLを実行し、適切なデータを出力するアプリが紹介されており、衝撃を受けました、、）。SnowparkのML関連機能加えて、Snowpark（Snowflakeにて、処理はSnowflakeのウェアハウスを利用しつつ、プログラミング言語としてPython、Javaなどを利用可能にする機能）にて、特徴量エンジニアリングやトレーニングにおける一般的なフレームワーク（Sklearn、XGBoost、LightGBMなど）のサポートを開始することも発表されました。更に、Snowflake内にモデルレジストリ機能（Snowpark Model Registry）を実装することも発表、SageMakerやVertex AIなどで構築したモデルの登録なども可能になります。先述したSnowpark Container ServicesではNVIDIAが提供するGPUを選択可能となっているので、それらを用いたモデルの学習や推論も可能になります。(これでも一部割愛しているんですが、本当に盛り沢山ですよね、、、)その他の感想英語圏のイベントに、会社からひとりで参加するということで少し不安もあったのですが、日本の企業向けのラップアップやディナー会の開催など、Snowflake日本法人の皆様によるフォローが非常に手厚く、おかげさまで当初の不安は早々に解消されました。日本法人社長の東條氏。久しぶりの日本語講演に少しホッとしました。笑また、それらのイベントなどを通して、他社のSnowflakeユーザの方々と会話ができ、コミュニティでの情報共有の重要性に気づきました。今後は日本のSnowflakeコミュニティの活動にも関わっていけたらと考えています（早速KDDIグループのSnowflakeコミュニティにてSummit参加報告Lightning Talkに参加させていただくことになりました！）。終わりにまず一番の感想としては、本当に刺激的な4日間だったということです。毎日思いもよらない新機能が信じられないほどの数発表されて、驚きの連続でした。特に先ほど挙げたStreamlitでのLLMを用いたアプリケーションの実装は衝撃的でした。ChatGPTが話題になり始めたのも12月頃だったにもかかわらず、これだけのLLM関連の機能を、デモが行えるレベルで開発しているスピードに感動しました。また、今回初めてこういったイベントに現地で参加したのですが、現地だからこそ味わえた熱気や勢いは間違いなくあったなと感じています。4日間どっぷりとSnowflakeに浸ったからこそSnowflakeのメッセージを深く理解できたと感じています。今回のSummitへの参加を通して、今後もSnowflakeを使っていきたいと感じ、学んでいきたいという気持ちを更に強めることができました。加えて、当初の目的であった「今後役立つ知見の獲得」も達成できたと感じています。特に構築後の活用フェーズに関する示唆は、テーマとのつながりも強かったため、沢山得られました。新機能の多くはまだプレビューで、使えないものも多々ありますが、実際に触れるようになったら、すぐに手元で試してみたいと思います。改めて、このような素敵な機会を与えてくださった社長はじめ上司の方々、またこの出張によってプロジェクトに1週間ほど穴を開けてしまうことを許容してくださったチームの方々へ感謝を伝えたいです。その分、今回の経験を元に、しっかりと貢献、還元していきたいと思います！以上、Snowflake Summit 2023参加のご報告でした！
https://www.ariseanalytics.com/activities/report/20230731/,【JSAI2023】Graph Neural Networkを用いた異常通信検知についてポスター発表しました,ARISE Tech Blog2023.07.31Tweet【JSAI2023】Graph Neural Networkを用いた異常通信検知についてポスター発表しましたARISE analyticsでAIエンジニアに所属している秋元です。今回の記事の内容は、2023年6月5日〜9日で開催された人工知能学会全国大会(JSAI2023)でポスター発表させていただいた報告です。JSAI2023では、ARISE analytics×KDDI総合研究所様×トヨタ自動車様の共著で通信セキュリティ領域における異常通信検知に関してポスター発表させていただきました。タイトルは 「フローデータのまとまりを考慮した新たなグラフ構成手法とGNNによる異常通信検知への適用」 です。JSAI2023について今年のJSAI全国大会は熊本城ホールで開催されました。とてもきれいな会場です。熊本料理もとても美味しい。出典：https://www.sys-eng.co.jp/cases/equipment/劇場・ホール/2845今年のセッションの様子は昨年度と大きく変わらず、AI応用、機械学習、言語メディア処理に関する発表が多数でした。タイミング的に、JSAI2023の締め切りがChatGPTブームの直前だったということもありAIや生成系に関する発表はそれほど目立ちませんでしたが、来年は生成AIに関する発表が増加するかもしれません。興味深いセッションとしては、生成AI特別セッションレポート「日本は生成AIを起爆剤にできるのか？」という特別セッションがありました。各研究領域の第一人者を集めて生成AIに関する以下の質問に関して議論するコーナーでした。生成AIを中心とする一連の技術革新は日本経済浮揚のための起爆剤となるか？OpenAIに追いつき追い越すための研究とは？追い越した先にある人とAIとの関係は？最初の質問に関してはどの登壇者の方もYESで、以降の質問に関しては例えば以下のようなコメントがありました。国策として進めるべし 。特にインフラ投資、計算資源、データ。（松尾先生）今回の生成AIの発展は世界規模の現象で、言語による情報流通障壁が大きく軽減される可能性がある。（鳥海先生）まずはすでに出ているLLMの真似をすることから始める。ChatGPTはまだショーケースのようなもので、キラーアプリは出てきていないので、技術を理解し、ある意味で「こんなものか」と舐めてかかれるようになる必要がある。そこからみんなで作る・使う・工夫するというふうに広げていくのが勝ちパターン。この技術すごい、と思っている間は先には進めない。最終的には10兆円規模の事業(医療、金融、製造など)に貢献できるかどうかが勝負になる。この構造を作らなければ継続性を保てない。技術的な課題はたくさんあるので、今後いくらでも発展し、チャンスは何回でも訪れる（松尾先生）データ基盤(収集、クレンジング、検索)が重要になる。現時点では、検索エンジンという基盤があって初めてLLMが成立しているように見える。社会への最適化は未踏の領域。深層学習は最適化技術なので、対象としているタスクへの過剰適合が発生しやすいが、それによってタスクと現実世界のギャップは大きくなる（相澤先生）生成AIやLLMは要素技術であり、本質的には実空間情報も含めた社会システムの構築が必要（鳥海先生）LLMの最先端で実験を実施しようとすると、現時点で既に国策レベルの投資が必要になってくるとのことで、この分野の進歩の速度は本当には速いと改めて認識させられました。一方で、現在注目を集めている技術はまだまだショーケースのようなもので、社会に浸透していくまでにはまだまだ段階があるというのも重要な指摘だと思われます。生成AIは多くの社会課題の解決に貢献できる可能性がある技術なので、ARISE analyticsもこの分野には積極的に取り組んでいきたいと考えています。ポスター発表の紹介ここからは、ポスター発表の内容を紹介します。今回のポスター発表はIoTを対象とした通信セキュリティに関する研究で、具体的にはIoTがマルウェアに感染した場合に、マルウェアによる異常な通信を検知するための方法についての新しい手法の提案です。人工知能学会全国大会2023ポスター発表マルウェア通信の異常検知とは？マルウェアによる通信といえばDoSやScanなどが挙げられます。先行研究ではこのようなマルウェア通信を検知する手法が多数提案されていますが、今回はその中でも特に検知が困難なC2通信に焦点を当てました。C2通信とは、マルウェアに感染したIoTデバイスと悪意あるサーバ (C2サーバ) との間に発生する通信のことを指し、定期的な接続確認やサーバからの攻撃指令を受け取ったりするために利用されます。DoS攻撃やホストスキャン攻撃などが発生した場合には、通信量やパケット数が一気に増大するため、通信の統計量を監視していれば攻撃が始まったことを検知することが可能です。一方でC2通信など攻撃の事前準備などで行われる通信は、統計量としては正常通信と区別がつきにくく、従来の異常通信検知手法では検知困難でした。そこで、発想を転換してGraph Neural Networkによる異常通信検知を提案したのが今回の研究内容です。Graph Neural Networkグラフとは以下のようにノードの集合とエッジの集合から構成されるデータ構造を指します。多くの場合ノードやエッジには特徴量が設定されます。通信データはIPアドレスで表される通信ホストとホスト間の通信から構成されており、グラフのデータ構造と非常に相性が良いことから、GNNを用いた異常検知手法が多数提案されてきています。ところが、先行研究の方法でIoTデバイスの通信をグラフ化した場合、以下のようになってしまいます。このようになる理由は、どこで通信をキャプチャするかに依存しており、今回はIoTデバイスを含むホームデバイスが接続しているルータでデータを取得したためです。この位置で通信を取得した場合IoTデバイスがルータや外部サーバなど複数のマシンとやりとりする通信はキャプチャできますが、外部サーバ間の通信は取得できないため、IoTデバイスを中心としたスター型のグラフを形成することになります。この中にC2サーバとの通信が混ざっていたとしてもエッジが一本増えるだけなので、残念ながら先行研究によるグラフ構成手法ではC2サーバを捉えることができません。提案手法の紹介そこで、今回の研究では発想を転換して新しいグラフの構成手法を提案しました。注目した点はIoTデバイスは一般的なPCと異なり機能の数が多くない機能的に単純であるため、通信の内容にはある程度順序性があると仮定できるという点です。特定の機能への通信順序にはパターンがあるのではないかと考え、各デバイスがどのような機能にどの順番で通信しているか、という点に着目してグラフを構成することを考えました。今回通信データとしては通信データそのものであるパケットよりも軽量なIPFIXを用いています。IPFIXはパケットとは異なり通信データそのものではなく、通信量 (バイト数) や通信パケット数などの統計量を保持するデータ形式であり、まずはシンプルに通信先IPおよび通信先ポート番号の組み合わせ機能を表現することを考えます。したがって、各ノードはある特定のIoTデバイスから特定の機能へのアクセスを表すことになり、エッジは機能へのアクセス順を表します。グラフ全体としては、あるIoTデバイスがどの機能にどのような順番でアクセスしているかを表すためのグラフ構成手法です。理想的には以下のようなグラフとなります。提案手法のグラフにおいては、C2サーバはIoTデバイスの通信順序と関係なく通信することになるため、部分的にスター型のトポロジーを示すことが想定されます。よって、正常通信で学習したGNNを用いてグラフ形状に関する異常通信検知を適用することによって、異常なノード=マルウェアに感染したIoTデバイスからC2サーバへの通信を検知することが可能になります。実験結果提案手法について、KDDI-IoT-2019データセットを用いて検証を行いました。デバイス提案手法(F1)従来手法(F1)amazon-amazon-echo-gen20.0050.005au-network-camera0.1880.250au-wireless-adapter0.4760.324bitfinder-aware0.3030.455candy-house-japan-sesame-wifi-access-point0.4550.400google-google-home-gen10.0630.044i-odata-qwatch0.3030.706irobot-japan-roomba0.1760.188line-clova-wave0.1820.545linkjapan-eremote0.1180.174mouse-computer-room-hub0.5710.343nature-nature-remo0.4350.227powerelec-wifi-plug0.7270.381qrio-qrio-hub0.6000.286sony-bravia0.0110.004xiaomi-xiaomi-mijia-led0.4290.211検証結果は必ずしも提案手法で精度が上がっているわけではない。。。という結果となりました。個別にグラフの形状を確認していったところ、提案手法で精度が高いデバイスについては想定通りのグラフが構成されていることが分かりましたが、一方で精度が低いデバイスではノード数が1～2個と極端に少なくグラフの体をなしていないものであったり、逆にノード数が非常に多くC2通信が埋もれてしまう場合などが散見されました。これらは、機能の単位を「通信先IP、通信先ポート番号」という単純な設定にしているために起こっているものと考えられるため、いかにして最適な機能の表現を定義するか、が次の課題となりそうです。ReferenceKDDI-IoT-2019:https://github.com/nokuik/KDDI-IoT-2019
https://www.ariseanalytics.com/activities/report/20230818-2/,RAG (Retrieval Augmented Generation) を活用！LLMで外部データを参照する方法を解説,"ARISE Tech Blog2023.08.18TweetRAG (Retrieval Augmented Generation) を活用！LLMで外部データを参照する方法を解説はじめにはじめまして、DX Technology Unitの芹澤です。普段はAI関連技術を用いた研究開発に携わっています。昨今、ChatGPTを初めとした大規模言語モデル (Large Language Model; 以下LLM)が話題になっており、様々な質問に対して非常に優れたアウトプットが得られるようになりました。一方、LLMを企業で使用する場合、社内特有のデータを参照する必要があるため、社内特化LLMシステムを構築することが必須となります。弊社では、社内の業務効率化を目的とした生成AIの活用を促進する「生成AIプロジェクト」が立ち上がり、ChatGPTをはじめとしたLLMの検証作業を進めています。その中で生成AIが社内情報を参照できるようにする方法について検証を進めており、Retrieval Augmented Generation (RAG) と呼ばれる手法を用いて、ChatGPTが外部データを参照し、対話形式で外部データの情報を取得する方法についてオープンデータを用いて検証を行いました。今回はその結果を共有させていただきます。なぜRAGを使うのか生成AIが社内情報を参照できるようにするには、主にプロンプトで情報を与える方法とLLMそのものをFine-tuningする方法、RAGによって外部データを保存したデータベース(Data Base;以下DB)から呼び出す方法の3つがあります。プロンプトで情報を与える方法は非常に簡単です。LLMに対してプロンプトで社内情報を与えた上で、「その中から回答してください」と質問をすることで社内情報を参照できるようになります。ただし、このやり方では逐一プロンプトで情報を与える必要があること、主要なLLMであるChatGPTでは入力プロンプトの文字数制限があること、与える情報が多くなりすぎると文章の複雑性が増してLLMの回答精度が低下する傾向があることから、膨大な社内情報を参照するシステムとして利用するのは現実的ではありません。LLMのFine-tuningでは、Parameter-Efficient Fine-Tuning (PEFT)と呼ばれるHugging Faceで公開されているライブラリが多く利用されており、中でもLow-Rank Adaptation (LoRA)と呼ばれる手法やその派生手法が多く利用されています。LoRAは事前学習済モデルの99.9%以上のパラメータを固定した上で追加学習を行い、全体をFine-tuningした場合と同程度の精度を保ちつつ計算効率化を実現しています。ただし、それでも大規模言語モデルをFine-tuningするにはハイスペックGPU環境で長時間学習を行う必要があり、コストが課題となります。一方、RAGはLangChainと呼ばれるLLMの機能拡張ライブラリを活用して開発されることが多くなっています。RAGの詳細は次章で説明しますが、社内情報など追加するデータをDBに保存し、そちらをLLMが参照して出力を作るという形になります。この方法ではモデルの学習を行っていないため、計算コストがあまりかからず、比較的簡単に実装・検証を進めることができます。そのような理由から、最初のステップとしてRAGによる検証を進めました。RAGについてRAGは文章検索で関連文章を抽出してLLMにプロンプトとして渡す手法で、フローは以下のような形になっています。今回はRAGの手法の中でも一般的な文章をベクトル変換し、ベクトルデータベース(Vector DB)に保存して類似度検索(Similarity Search)を行う方法を取りました。Retrieval Augmented Generationを活用した回答作成フローステップとしては、Storeフェーズ、Retrievalフェーズ、Generationフェーズに分けることができます。Storeフェーズ社内情報など追加で与えたい情報をまとめたテキストファイルから文章を抽出し、Embedding (文章のベクトル変換) をしてベクトル化します。このベクトル化したデータをDBに保存することで、Vector DBを構築します。Retrievalフェーズユーザからの質問文(クエリ)を基にDB上から必要な情報のみを抽出します。抽出にはベクトル間のSimilarity Searchを用いています。GenerationフェーズRetrievalフェーズで抽出した情報とユーザの質問をLLMにプロンプト上で渡し (これをAugmentationと呼びます)、質問文に対する回答を作成します。実装方法1. Storeフェーズ1.1.環境構築以下の通りライブラリをインポートします。LangChainは更新が早いのですが、ここではver0.0.163での動作確認をしています。importpandasaspdfromdatasetsimportload_datasetfromlangchainimportPromptTemplate,LLMChainfromlangchain.llmsimportOpenAIfromlangchain.indexesimportVectorstoreIndexCreatorfromlangchain.embeddingsimportHuggingFaceEmbeddingsfromlangchain.vectorstoresimportFAISS1.2.追加データの準備現在弊社で進めている生成AIプロジェクトでは、コミュニケーションツールの高度化を目指しており、対話型の社内データを活用することを想定しています。そのため、今回紹介する実装例では、参照する追加データとしてDatabricksのデータを日本語に翻訳したデータセット (kunishou, 2023-05-11, databricks-dolly-15k-ja)を使用します。これは対話型データセットであり、テストデータとして最適です。社内特化システムを作りたい場合は、こちらを社内データに変更することで特化システムを作ることができます。今回のデータセットは画像の通り、instruction(質問)とoutput(回答)に分かれています。dataset=load_dataset('kunishou/databricks-dolly-15k-ja')df_databricks=pd.DataFrame(dataset['train'])df_databricks=df_databricks[[""instruction"",""output""]]df_databricks.head()Databricksのデータセットこのデータセットを、Embeddingがしやすいようリスト形式に変換します。今回は質問と回答が交互に来るリスト形式にしていますが、質問と回答で1文章にするなど、追加データセットの形式によっても後々出力される回答に影響が出るため、工夫が出来るポイントの1つとなっています。dialog=[]foriinrange(len(df_databricks)):dialog.append(df_databricks[""instruction""][i])dialog.append(df_databricks[""output""][i])1.3. DatabricksデータのEmbedding追加データをベクトルとして扱えるよう、Embeddingします。Embeddingには主に単語区切りのWord Embeddingと文章区切りのSentence Embeddingがあり、有名なモデルとしてはそれぞれword2vecやOpenAIEmbeggingsなどがあります。今回はSentence Embedding ModelとしてHuggingFaceEmbeddingsにあるモデル(oshizo, 2023, sbert-jsnli-luke-japanese-base-lite)を使用しました。ベクトル化した文章を保存するにあたり、今回はセットアップが簡単な類似度検索ライブラリであるFAISSを使用しました。他にも、ChromaDBやElasticsearchDBなどLangChainの公式ページに記載のあるDBを使用することができます。embeddings=HuggingFaceEmbeddings(model_name=""oshizo/sbert-jsnli-luke-japanese-base-lite"")db=FAISS.from_texts(dialog,embeddings)2. Retrievalフェーズ2.1.類似性検索プロンプトとして与える入力と先ほど構築した追加データのDBを使用して、類似性検索を行います。queryとして与えた質問文をEmbedding Modelでベクトル化し、1.3.で作成したVector DBから類似度検索で上位N件の文章を抽出します。その後、抽出した複数文章をLLMの情報源文章(コンテキスト)として1つにまとめます。今回の例では、「ヴァージン・オーストラリア航空はいつから運航を開始したのですか？」という質問を渡して類似性比較を行っており、関連する上位4件の文章が抽出されて1つのコンテキストにまとめられています。query=""ヴァージン・オーストラリア航空はいつから運航を開始したのですか？""embedding_vector=embeddings.embed_query(query)docs=db.similarity_search_with_score_by_vector(embedding_vector,k=4)context="""".join([document[0].page_contentfordocumentindocs])print(context)作成されたコンテキスト3. Generationフェーズ3.1.LLMの準備今回の検証では、高い精度で回答を作成することで知られるChatGPTを利用します。弊社では元々Azure環境を利用していたため、Azure OpenAI Servieceを利用してChatGPTのモデルをデプロイし利用しました。engine部分にデプロイしたモデルの名前とmodel_nameにモデルバージョンをそれぞれ記載します。API KEYを環境設定していない場合は、OpenAIが公式でアナウンスしているページ(Best Practices for API Key Safety)を参考に、事前に設定しておいてください。llm = OpenAI(engine=""arise_gen_ai"", model_name='gpt-35-turbo', temperature=0.2)3.2.LLMで回答作成3.1.で準備したChatGPTと2.1.で抽出したコンテキストを用いて回答を作成します。LangChainでは質問以外のプロンプトをテンプレート化することで、都度プロンプトを作成することを避け、入力・出力をコントロール出来るようにしています。最初に、テンプレートとなるプロンプトを作成します。ここで「[参考]部分の情報を使って質問に回答して下さい。」と回答方法を指定することで、類似性検索で抽出したコンテキストから回答を作成させます。作成したtemplateをPromptTemplateとしてLLMChainに渡すことでLangChainを実行する準備をします。後はllm_chain.runで質問を投げることで、ChatGPTで回答を作成します。ちなみにこの部分はいくつか書き方があり、RetrivalQAやload_qa_chainを使う方法などもあるので、興味がある方は調べてみてください。今回の例では、質問に対しVector DBから抽出したコンテキストを元に「ヴァージン・オーストラリア航空は2000年8月31日に運航を開始しました。」という回答を作成してくれています。template=""""""[参考]部分の情報を使って質問に回答してください。[質問]{question}[参考]{context}""""""prompt=PromptTemplate(template=template,input_variables=[""context"",""question""])llm_chain=LLMChain(prompt=prompt,llm=llm)print(llm_chain.run({""question"":""ヴァージン・オーストラリア航空はいつから運航を開始したのですか？"",""context"":context}))chatGPTの回答まとめ以上、ChatGPTとLangChainを活用したRAGを紹介しました。今回の実装例では上手くいった出力例を載せましたが、プロンプトのテンプレートを少し変更するだけで上手く回答が作成されないことも多々ありました。例えば、Vector DBから類似度検索で上位4件を持ってきた文章を用いてChatGPTに回答を作成してもらう際、抜き出した文章全てを使って冗長な文章を作成してしまう、ということがありました。これについては、テンプレートプロンプトで「回答は簡潔に回答してください。」と付け加えることで解決しました。このようにプロンプトエンジニアリングの難しさを感じており、試行錯誤中です。また、今回はLLMモデルとしてChatGPTを用いていますが、こちらは別のLLMに変更しても同様のことが出来ます。そこでGPT4allと呼ばれる比較的小規模なモデルでも検証を行いましたが、英語では上手く回答が作成される一方、日本語だと良い回答が作成できませんでした。このことから、RAGでLLMを利用する場合は、基のLLMが日本語に十分対応できていることも重要になってくるという確認が出来ました。本記事の内容は弊社で取り組んでいる技術検証の一部になります。弊社では本テーマ以外にもKDDIのデータを対象とした技術開発に取り組んでいます。今回紹介した内容や、その他弊社の取り組みに興味がございましたら、ぜひお声がけください。https://www.ariseanalytics.com/recruit/"
https://www.ariseanalytics.com/activities/report/20230920-2/,LLM時代に人は対話AIを信頼できるか？Human Agent Interactionの視点から考える,"ARISE Tech Blog2023.09.20TweetLLM時代に人は対話AIを信頼できるか？Human Agent Interactionの視点から考えるはじめにこんにちは、DX Technology Unitの芹澤です。以前は「RAG (Retrieval Augmented Generation) を活用！LLMで外部データを参照する方法を解説」という記事を書きましたが、今回もLLMに関連する記事をお送りします。弊社では、業務時間のうち週に3時間まで自己研鑽に充てて良いという制度があり、各々興味のある勉強や実装、コンペ参加などを行ったりしています。その中で、私は学会に向けて各々好きな研究に取り組むグループに参加しており、毎週ゼミ形式で進捗共有をしながら研究活動を行っています。本記事では、その活動で私が研究を行っている対話AIと人間の信頼に関する現状について共有させていただきます。LLMに対する信頼とHAIの関係昨今、ChatGPTをはじめとした大規模言語モデル(Large Language Model;以下LLM)の話題を聞かない日は無いほどLLMブームが来ています。ただ、実際にLLMが社会に浸透していくには、人がLLMをベースとした対話AIを信頼できるかどうかという点が重要になってくると思われます。信頼できるAIに関連して、「Trustworthy AI: From Principles to Practices」という論文で詳しく解説されています。顔認識や自動運転、自然言語処理などの各タスクの精度向上に合わせて産業応用が進むにつれ、信頼性の向上に寄与してきました。ただ、産業界では対話AIが扱われることが少なかったため、Trustworthy AIの定義は対話AIを前提としていません。一方、AIをはじめとしたエージェント(AI,ロボット,アバター…)に対するインタラクションの研究を行っている、Human Agent Interaction (以下HAI)という研究分野があります。この分野は情報工学以外にも認知科学や心理学などを含めた学際的な分野であり、多面的にエージェントについて研究をしています。この分野でもLLMをエージェントとした研究が出始めており、LLMによる対話AIの信頼に関する研究も登場しています。Human Agent Interactionのイメージでは、HAIの視点から見たとき、人が対話AIを信頼するにはどのような条件を満たす必要があるのかについて、最新の論文を引用しながら考察していきたいと思います。信頼の定義信頼とは何か人がAIを信頼するにはどうする必要があるかを示すための前提として、そもそも信頼の定義とはどのようなものになるのでしょうか。これまで多くの研究者が人間同士のインタラクションにおける信頼の概念の研究を行ってきました。信頼に関する論文で最も引用されている定義の一つは、1995年にMayerらによって公開された論文「An Integrative Model of Organizational Trust」で提案されたもので、「信頼とは、相手が、その相手を監視したりコントロールしたりする能力に関係なく、信頼者にとって重要な特定の行動を実行するという期待に基づいて、相手の行動に対して脆弱であろうとする当事者の意思である」と論じています。ここで脆弱な状態に身を置くというのはリスク・テイキング(リスクを感じつつ意思決定して行動すること)のことです。分かりやすく言い換えると、「信頼は相手にコントロールされることなく、相手を信じることについてリスクがあることを分かったうえで行動をしようと考えること」のようなイメージでいいかと思います。信頼の定義はこのような考え方が多く使われていますが、まだ解像度が荒く、この信頼を獲得するにはどのようにする必要があるのかが見えにくくなっています。そこで、更に解像度を上げていきたいと思います。認知的信頼と感情的信頼どのような要因が信頼レベルに影響を与えるかを調査するうえで、心理学の分野では信頼を認知的信頼と感情的信頼の2つに分類しています。認知的信頼は他者の信頼性、能力、誠実さに関する合理的な判断と評価に基づく信頼を指します。しかし、日常生活をしていると経験的に分かると思いますが、信頼は感情や気分などの不合理な要因にも影響される可能性があります。そこで、個人間の感情的または感情的な絆に基づいた信頼を感情的信頼と呼んでいます。感情的信頼には感情的なつながり、共感、共有の価値観、信念、目標の認識の形成が含まれています。それぞれについて具体例で例えると、「仕事をする上で直属の上司を信頼する」というのは認知的信頼をしている、「家庭の事情について上司を信頼して相談できる」というのは感情的信頼関係が築かれている、というような形となります。これら2つの信頼は「Similarities and differences between human–human and human–automation trust: an integrative review」という論文において、いずれも人間間のインタラクションにおける信頼に影響を与え、また人間とエージェント間でも共通することが分かっていることから、この2つの信頼を獲得できれば、人間は対話AIを信頼できると考えています。信頼獲得のための5つの要素認知的信頼と感情的信頼を獲得するための要素としては、人間同士の信頼研究から始まり、実験を繰り返しながらいくつかの研究が長年にわたって実施されることで、必要となる要素が検討されてきました。その中で、LLMを前提としたエージェント対人間に関する信頼について、2023年8月に行われたAI系のトップカンファレンス(IJCAI)で発表された「Enhancing Trust in LLM-Based AI Automation Agents: New Considerations and Future Challenges」という論文では以下5つの要素を挙げています。ここでは、ChatGPTを例にしながら各要素について説明していきたいと思います。ReliabilityOpennessTangibilityImmediacy behaviorTask characteristicsReliability質問に対するエージェントの回答は高精度で高品質なことが求められます。過去の研究より、エージェントに対する信頼と指示の遵守の間には強い相関関係があることが分かっています。そのため、指示に対する信頼性が不可欠となり、指示に対するタスク実行の精度が求められます。ChatGPTの場合、問われた質問に対し、正しい答えを回答できる精度が求められます。現状としては、OpenAIの公式サイトのパフォーマンス評価で示されている通り模擬司法試験で受験者の上位10％程度のスコアを記録するなど、高い精度が得られていると思われる一方、Hallucination (尤もらしい嘘を出力すること)のような課題もあります。Openness対話AIの透明性も重要となっています。AIがどの程度の機能を持っているか、AIの目的は何か、どのようなアルゴリズムか、倫理観はどうなっているのかが示されることで、人間にとって信頼のおけるAIとなります。ChatGPTの場合、機能や目標、倫理観はある程度OpenAIの「GPT-4 Technical Report」や「Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models」という論文などで提示されている一方、アルゴリズムの詳細は「GPT-4 Technical Report」で記載のある通り非公開となっており、この点において人間がどうすれば信頼できるかを検討する必要があります。Tangibility意訳すると、有形性があるか、どのような外見かというものになります。アバターや視覚的表現を通じて具体性を組み込むことで、魅力度が上がりユーザ体験が向上します。 人間は視覚的に認識できる存在に対してより積極的に反応する傾向があるため、アバターのような外観はより良いコミュニケーションを促進します。 これにより、親近感と関連性が生まれ、人間とAIエージェントの間のより強いつながりが促進されます。ChatGPTの場合、視覚的表現手段を持っていないため、この点においては信頼獲得が難しくなっています。Immediacy behavior直訳すると即時性行動となります。これは、「好意や親近感を示すポジティブなメッセージを送り、人々の間の心理的距離を縮め、モチベーションにプラスの影響を与える言語的および非言語的コミュニケーション行動」と定義されており、分かりやすく言うと、親近感を生む行動というイメージになります。親近感を生むためには共感性やコミュニケーション・作業におけるスタイル適応が重要となっており、これらによってより人間らしい体験を与え、満足度を向上させて人間との強い関係を築くことに寄与します。ChatGPTの場合、質問に対する回答が間違っていた際、ユーザが間違いを伝えると、ChatGPTは丁寧な言い回しで誤解をお詫びする傾向があり、これは共感を生むジェスチャーの一つと言えると思います。Task characteristicsタスク特性は、ユーザがAIエージェントの制御に対してどの程度関与できるか、逆に自律的なアクションはどの程度出来るか、オープンタスクに対してどの程度対応できるかという点になっています。エージェントに対する指示において、精度を確保して予期せぬ結果を回避するためには人間による検証が必要になることがよくあります。Reliabilityで記述した通り、指示に対する回答精度はシステム全体の信頼性とパフォーマンスに影響を与えるため、エージェントの制御は重要となります。ChatGPTの場合、APIプラグインを利用することで制御や自律アクションについてある程度人間が関与することが出来ます。一方、オープンタスクに対してどこまで関わるかは課題の一つと言えると思います。今何が出来ていて、何が求められていくかLLMの中でも特に優れているChatGPTを例に各要素で再整理すると、以下のようになっています。Reliability:求められるレベルが定まっていないが、ChatGPTの利用場面が増えている理由を考えると、一定程度到達している一方、Hallucinationの課題もあるOpenness:アルゴリズム以外は透明性が担保されているTangibility:現行のChatGPTは有形性が無く、達成不可Immediacy behavior:言い回しによって言語表現されているが、有形性が無いため非言語による親近感を生むことはできないTask characteristics: APIによってある程度可能だが、オープンタスクに対して課題が残るこの中でReliabilityやOpennessに関しては、LLMそのものの開発が必要となっており、LLM開発企業以外で深く関与していくことは難しいのではないかと考えています。ただし、開発されたLLMを対象とした研究などは可能です。中でも、Reliabilityの中のHallucinationは多くの研究者が研究対象としているタスクとなっています。HallucinationによってLLMに対する信頼性は大きく低下していると思われ、この現象に対する研究が重要になっていくと思われます。TangibilityやImmediacy behaviorはLLMによるAIエージェントとしての研究は今のところほとんど見られていません。ただし、LLMという条件を除くと研究は行われており、例えば2016年の人工知能学会誌には「オンラインショッピングにおける商品推薦エージェントの外見と振る舞いの関係が購買意欲に与える影響」というTangibilityとImmediacy behaviorの関連性に関する論文が公開されています。また、アバターという外見に関して、日本は以前からVTuberのようなアバターコミュニケーションが盛んな国となっています。直近では日本を中心にAIによるVTuber (AITuber)が誕生してきており、AITuber専門の事務所が設立されるなど、商業面で盛り上がりを見せています。これらの反応を踏まえ、商業側で人気が出たAITuberなどに対して検証するような形で外見性と親近感に関する評価が示されるのではと考えています。Task characteristicsについては、オープンタスクに関してAuto-GPTをはじめとしたAIエージェントの研究開発が進められており、非常に盛り上がっているタスクの一つとなっています。Auto-GPTは自律型AIエージェントの一つで、一つの指示を出すと毎回指示を出さなくても複数の工程を一括で行ってくれるものとなっています。このような複数工程を自律的に行うことによって、オープンタスクについても処理できるようになるのではないかと考えています。ではこれら5つの要素が今後の対話AIすべてに必要かというと、そうではないと考えています。例えば、絶対にミスをしないという観点から信頼できるAIであるためにはReliabilityやOpennessが重要ですが、ドラえもんのように人に寄り添うAIであるためにはTangibilityやImmediacy behaviorのような共感性に関する観点が重要となると思います。そのAIに対して求められる要件に合わせて、どの要素をより達成できているかをチューニングできればいいのではないかと考えています。おわりに人間が対話AIを信頼するには何が必要かについて、5つの要素を軸に説明させていただきました。それぞれの要素でまだ課題が残っており、これらの研究が今後進んでいくのではないかと考えています。対話AIの利用には情報工学としてだけでなく、心理学など様々な要素が関わってくるということを知っていただけると幸いです。"
https://www.ariseanalytics.com/activities/report/20230922-2/,Serverless FrameworkとTerraformの使い分けについて,"ARISE Tech Blog2023.09.22TweetServerless FrameworkとTerraformの使い分けについてMarketing Solution Division所属のエンジニアの坂本です。FaaSデプロイの悩み早速本題ですが、AWSLambdaのデプロイについて悩んでませんか?特に検索で飛んで来た方は様々なツールや管理方法を模索しているのではないかと思います。現行のクラウド管理はIaCを使うことがほぼほぼスタンダードになっており、Terraformなどのツールを使ってIaCで構築しているかと思います。(IaCについて知りたい方は記事を書いていますのでご参考いただければ。)確かにインフラ側の目線だけで言えばTerraformでLambda等のFaaSを管理することもできます。しかし、開発環境をVSCode Dev Containerで整えようとするとめんどくさかったり、インフラ管理であるIaCにLambdaコードが入っていてよいのか考えたり、色々と考慮が必要なため管理するうえで扱いに困るという方が多いのではないでしょうか?そうした際に役に立つのがServerlessFramework(以下、Serverless)などのFaaS用構成管理デプロイツールです。https://www.serverless.com/ServerlessはLambdaのインフラ～アプリケーションを一括で管理してくれ、尚且つコンフィグファイルもそこまで面倒な設定は必要ないためアプリコードの中に1ファイル入れておくだけで済みます。しかし、それゆえ発生する問題がTerraformとの住み分けです。インフラのコードは様々な依存関係が存在します。ネットワーク,それに伴うルーティング, Gateway, logの吐き出し先などなど。これらはServerlessで完結するのは難しく、対応していないリソースがあったり、そもそも大規模システムであればインフラはTerraform等の専用ツールで管理した方が拡張性や差分管理がしやすいです。そこで今回は社内で開発したSlackのChatBotで使用した構成を例に話していきたいと思います。本件で例に挙げるChatBotは最近流行のChatGPTにSlack経由でアクセスするためのものとなり、AWS上に構築されます。その際に作成したインフラは下記のようになっています。中身としてはシンプルにSlack botからのREST通信をAPI-Gatewayで受けて、WAF及び認可Lambdaを通過すれば、Lambdaで内容を処理をしてAzure上のOpen AIに投げるといった構成にしています。インフラとアプリの責任分界点結論最初に結論からいうと今回のServerlessの構成としては下記の図と表のようになっています。構成図(Severless以外は全てTerraform管理)リソース対応表リソースTerraformServerless frameworkVPC(subnet, NAT含む)〇WAF〇Parameter store〇△ ※read onlyAPI Gateway(REST API)〇API Gateway(リソース)△ ※初回作成のみ〇API Gateway(ステージ)△ ※初回作成のみ〇Lambda〇※API Gatewayのリソースとステージは作成必須のため初回のみ構築し、Terraform上では変更を検知しないようにしています。おそらく大半の方はServerless側にAPI Gatewayを寄せているかと思いますが、何故このような構成にしたかを説明させていただければと思います。理由今回寄せなかった理由は一重に、インフラの設計はインフラで一旦管理したかったというのがあります。Severless側でもAPI Gatewayを作ったり、WAFを作ったりすることは可能です。しかし、VPCなどは作ることがでないため完全にSeverless側のみで構築することは不可能となります。そうなるとインフラリソースであるWAFなどをSeverless側に寄せると今後の拡張の際にSeverlessとTerraformどちらに寄せるかを毎回考えなくてはいけなくなりますし、明確な定義をしないとその時担当した人によって設計がズレてしまう。というのはよくある話かと思います。そうなった場合の考え方としてたどり着いたのが、アプリ部分以外は完全にTerraformに寄せる今回の作りでした。悩ましかったのはAPI Gatewayの部分ですが、接続先のリソースがないとデプロイできないこと、レスポンスなどアプリ側で決めるべきことが設定内に組み込まれていることからSeverless側に寄せることにしました。これであればAWSの知識がないアプリエンジニアもSeverlessの設計に取られる時間が最低限で済みますし、アプリのリクエスト・レスポンスにまつわる設定を全てSeverless側にまとめることができました。Severless側全体をインフラエンジニアはアプリから解放されるので今後アサインされるメンバー選定の際にインフラ、アプリに跨った技術スタックを持つ人物である必要はなくなります。また、アプリとインフラ両方できる人でも新しいリソース(例えばDynamoDBなど)をSeverlessで管理するかTerraformで管理するか悩まず、Severlessは最低限という設計思想からTerraformに寄せることができます。実際のコードそれでは実際のseverlessコードを見てみます。serverless.yml(一部今回の件と関係ない部分はカットしています。service: arise-slack-llmframeworkVersion: '3'provider:name: awsstage: ${opt:stage} # 各環境ごとの差分吸収のために外部変数をセットregion: ap-northeast-1runtime: python3.9apiGateway:restApiId: ${ssm:/genai/apigw/chatbot/id}restApiRootResourceId: ${ssm:/genai/apigw/chatbot/root_resource_id}tags:CreatedBy: ServerlessSystem: genaistackTags:CreatedBy: ServerlessSystem: genaistackName: genai-${self:provider.stage}-clf-stack-01package:individually: trueexclude:- ./**functions:ariseSlackLlmLambda:name: genai-${self:provider.stage}-lambda-chatbot-01description: Chatbot Lambdarole: ${ssm:/genai/iam/chatbot_lambda_invoke/id}handler: src/slack_llm_bot/app.handlerprovisionedConcurrency: 1memorySize: 512timeout: 29package:include:- ./src/**exclude:- ./src/authorizer/auth.pyenvironment:PYTHONPATH: /var/task/srcevents:- http:path: /{proxy+}method: ANYauthorizer:name: authLambdatype: requestidentitySource: method.request.header.X-Slack-Signaturevpc:securityGroupIds:- ${ssm:/genai/sg/chatbot/root_resource_id}subnetIds: ${ssm:/genai/vpc/chatbot/id}authLambda:name: genai-${self:provider.stage}-lambda-authorizer-01description: Authorizer Lambdarole: ${ssm:/genai/iam/auth_lambda_invoke/id}handler: src/authorizer/auth.handlermemorySize: 256timeout: 6package:include:- ./src/authorizer/auth.py途中各所に${ssm:/xxxx}という変数名が入っていますが、こちらはTerraformがSeverless側で必要なリソースを作成した際にIDをParameter storeに格納し、そちらを呼び出しています。Terraform側のコードは下記のようになります。main.tf(こちらも説明に不要なリソースは削除しています)# ------# api-gateway# ------resource ""aws_api_gateway_rest_api"" ""apigw_chatbot_01"" {name        = ""${var.system}-${var.env}-apigw-01""description = ""GenAI chatbot REST API Gateway""endpoint_configuration {types = [""REGIONAL""]}}# ------# ssm-parameter store(API-GW)# ------resource ""aws_ssm_parameter"" ""ssm_parameter_apigw_id_01"" {name        = ""/genai/apigw/chatbot/id""description = ""API Gateway ID of chatbot""type        = ""String""value       = aws_api_gateway_rest_api.apigw_chatbot_01.id}これにより、インフラで作成したリソースidをパラメータストアに格納しており、Severless側が取得することによって依存関係もTerraform→Severlessとなっているため、再帰的な依存や相互依存を防ぐことができ非常にシンプルな管理を実現しています。メリット理由の項目にも記載しましたが、上記の構成を実現したことによるメリットとしてはIaCとアプリケーション部分が分離させているため、責任分界点がわかりやすい∟Lambdaを含めたアプリケーションのロジック、リクエスト、レスポンスにかかわる部分はSeverless側の変更で完結∟逆にWAFやVPCなどといったインフラリソースはTerraform側のみの設定、デプロイが完結新規のAWSリソースを追加する際にも上記を念頭におけばよいので、人による設計にバラつきがないといった点が挙げられます。まとめクラウド化が進み、インフラとアプリとの境目が難しくなってきています。今回挙げた分離方法は、あくまで今回のケース及び、メンバーのスキルによって決定しました。各ツールでそれぞれのパートの管理しやすさは変わるのでPJのメンバーややりたいことに合わせて、ふさわしい方法で分離いただければと思います。"
https://www.ariseanalytics.com/activities/report/20230925/,生成モデル（LLM）を用いた論文要約システムの構築～セキュリティ領域の論文の情報抽出を効率化してみた～,"ARISE Tech Blog2023.09.25Tweet生成モデル（LLM）を用いた論文要約システムの構築～セキュリティ領域の論文の情報抽出を効率化してみた～はじめにARISE analyticsの奥井です。普段は通信セキュリティに関する研究開発を行っています。今回の記事は、Large Language Models(以降LLM)を用いて論文PDFに対するQ&Aについて試してみたことを紹介します。未経験分野に挑戦する際は、不明点が多く何を行うにも困難が伴うと思います。特に、技術開発や研究では論文を読むことが多く、知らない単語や概念に躓いてしまい理解に時間がかかってしまうことは多いです。特に、通信セキュリティなど必要なドメイン知識が深い場合は、キャッチアップコストがかなり高いと感じています。ドメイン知識が深い内容についてキャッチアップや業務を効率化するできないかということをモチベーションに、今回はLLMを使って論文に対するQ&Aを行ってみました。通信セキュリティ領域で用いられる「YAF」と呼ばれるツールについての解説論文を題材とし、論文のPDFをインプットにQ&Aを行うことで理解が深められるかを検証します。まずは論文PDFに対するQ&Aを行う際のフローとその実装について簡単に解説し、実際にQ&Aを試してみた結果について解説します。今回は以下の3つの項目のQ＆Aを行いました。Q&A No.1 : 論文の要約Q&A No.2 :表形式の出力Q&A No.3 :図についての解説それぞれのQ&Aについて、LLMに入力するPromptの実装・出力例・評価考察をまとめます。評価考察については、LLMによる回答が適切かどうかを私自身が定性評価するという形で行いたいと思います。フローRetrieval Augmented Generation (RAG)と呼ばれる手法を用いてQ&Aを実現します。RAGについての詳細な解説はRAG (Retrieval Augmented Generation) を活用！LLMで外部データを参照する方法を解説 | 株式会社ARISE analytics（アライズ アナリティクス）をご参照ください。論文PDFに対するQ&Aは以下のようなフローで実現します。フロー図(Question Answering | 🦜️🔗 Langchainより引用)Document Loading:∟論文PDFのデータをロードする。Splitting:∟ドキュメントを指定サイズのスプリットに分割する。Storage:∟分割されたドキュメントをVectorStoreに格納する。Retrieval:∟ストレージからスプリットを取得する（例えば、多くの場合、入力された質問と同様の埋め込みを持つ）。Output:∟LLMが質問と検索されたデータを含むプロンプトを使用して答えを生成する。Answer:∟LLMが生成した回答が得られる。参考：Question Answering | 🦜️🔗 Langchain実装パートPDFを読み込み、RAGを用いてPDFの内容についてQ&Aを行うまでの実装コードを簡単に紹介します。まずは、LangChainのPyPDFLoaderを用いて、PDFドキュメントを読み込みます。## PDFデータのロード
### ライブラリの読み込み
import os
import openai
from langchain.llms import OpenAI
from langchain.document_loaders import PyPDFLoader
 
### API keysのセット
openai.api_key = os.getenv(""OPENAI_API_KEY"") # OpenAIのAPIキーを入れる
 
### pdfファイルのロード
loader = PyPDFLoader(""yaf.pdf"")
pages = loader.load_and_split() #PDFからテキストを抽出、抽出したテキストをページごとに分割次に、文章の分散表現を作成するEmbeddingモデルを作成します。## Embedding### ライブラリの読み込みfromlangchain.vectorstoresimportFAISSfromlangchain.embeddings.openaiimportOpenAIEmbeddings### Embedding & Indexingfaiss_index=FAISS.from_documents(pages,OpenAIEmbeddings()# 文章の分散表現を作成するEmbeddingモデル)faiss_index.save_local(""../database/faiss_index"")最後に、Q&Aを実施します。FAISSのベクターを読み込み、質問の意図に近い文章を上位から3件抽出し、抽出した複数文章を1つにまとめます。## Q&A### ライブラリの読み込みfromlangchain.chainsimportRetrievalQAfromlangchain.chat_modelsimportChatOpenAIfromlangchain.promptsimportPromptTemplate### FAISS vectorのロードvectoreStore=FAISS.load_local(""../database/faiss_index/"",OpenAIEmbeddings())## Retrieverretriever=vectoreStore.as_retriever(search_type=""similarity"",search_kwargs={""k"":3})### プロンプト(Q&A)qa=RetrievalQA.from_chain_type(llm=ChatOpenAI(),chain_type=""stuff"",retriever=retriever,return_source_documents=False)query=""""""hogehoge""""""ans=qa({""query"":query})print(""Query: \n"",query)print(""Ans: \n"",ans[""result""])```以上が、今回利用した実装コードになります。Q&A今回は、「YAF：Yet Another Flowmeter」に関する論文pdfを用いてQ&Aを試します。日本語の方が英語より理解が早いため、質問は英語でしつつ日本語で回答してもらうようにしました。以下三点のQ&Aを行い、それぞれのQ&Aの実装コード・出力例・評価考察をまとめます。Q&A No.1 :論文の要約Q&A No.2 :表形式の出力Q&A No.3 : 図についての解説参照した論文はこちら：YAF: Yet Another FlowmeterQ&A No.1 : 論文の要約まずは、定番の論文の要約を聞いてみます。実装コードquery=""""""Summary this paper.Answer in Japanese within 150 words.""""""ans=qa({""query"":query})print(""Query: \n"",query)print(""Ans: \n"",ans[""result""])出力例この論文は、ネットワークフローデータの収集と分析に関するものです。著者は、YAFというソフトウェアツールを紹介し、その特徴や利点について説明しています。YAFは、IPFIXフロープロトコルを使用してフローデータを収集し、セキュリティやネットワーク分析などの目的に使用するためのものです。YAFは、高いパフォーマンスと安定性を持ちながら、大規模なネットワークのモニタリングを可能にするために開発されました。また、YAFは他のフローメータリングプロセスと比較して、パケット転送の性能に影響を与えることなく高いデータ信頼性を提供します。さらに、YAFは柔軟なネットフローとの統合も可能であり、さまざまなアプリケーションに適用することができます。この論文では、YAFの利用事例として、セキュリティスイートやプライバシー保護ネットワークモニタリングアーキテクチャなどが紹介されています。評価考察参考に、論文のアブストラクトを確認します。A flow meter generates flow data – which contains information about each connection observed on a network – from a stream of observed packets. Flow meters can be implemented in standalone measurement devices or inline on packet forwarding devices, such as routers. YAF (Yet Another Flowmeter) was created as a reference implementation of an IPFIX Metering and Exporting Process, and to provide a platform for experimentation and rapid deployment of new flow meter capabilities. Significant engineering effort has also gone into ensuring that YAF is a high performance, flexible, stable, and capable flow collector. This paper describes the some of the issues we encountered in designing and implementing YAF, along with some background on some of the technologies that we chose for implementation. In addition we will describe some of our experiences in deploying and operating YAF in large-scale networks.論文のアブストラクトの内容と比較すると、アブストラクトを参照して回答しているだけではなく本文の内容も含めて要約しているようです。例えば、回答の「この論文では、YAFの利用事例として、セキュリティスイートやプライバシー保護ネットワークモニタリングアーキテクチャなどが紹介されています。」については、Introductionの「We then describe a few existing applications of YAF in section 5, including its application with SiLK [12] within the NetSA Security Suite and its use in the middle tier of PRISM [11], a multi-stage privacy-preserving network monitoring architecture.」部分を引用しているように思えます。全体としても、私が論文を読んだ限りは回答としては間違っていないように思えます。ただし、プロンプトで「Answer in Japanese within 150 words.」と文字制限を要求したつもりが、388文字で出力され制限はできていませんでした。Q&A No.2 :表形式の出力業務において、論文を読んだ上でいくつかの手法について観点を表で整理するという場面があります。これをLLMで整理できるか試してみます。今回は論文中で関連するフローデータの形式がいくつか解説されていたので、それらの内容について概要を表形式まとめることを試みました。今回はMarkdown形式で出力するように指定します。実装コードquery=""""""Explain various protocols in use for exporting flows.Explain Netflow v5, Netflow v9, and IPFIX in markdown table format (e.g., method | item of explanation | references ) in japanese.""""""ans=qa({""query"":query})print(""Query: \n"",query)print(""Ans: \n"",ans[""result""])出力例(出力をプレーンテキストとして張り付けています)|プロトコル|説明|参考文献| | ———- | ———————————————————————————————————————————————————————————————————————————- | ——– | | NetFlow v5 |プロトコルおよびネットワークフローデータの保存形式。IPv4フローと16ビットの自律システム番号をサポート。| [4] | | NetFlow v9 | NetFlow v5の後継プロトコル。IPv6および新しいレコードタイプの柔軟な定義をサポート。| [7] | | IPFIX |テンプレートベースのバイナリエクスポート形式。メッセージにはヘッダーとセットが含まれ、セットはテンプレートまたはデータレコードを含む。テンプレートは情報要素の順序付きリストであり、情報要素はデータタイプごとにエンコードされる。| [14] |pythonのprint出力のため、プレーンテキストとして扱われますが、markdownファイルとして再保存した上で表示すると、正しい表として出力されます。csv形式の場合も試しましたが、同様に表形式に出力ができていました。プロトコル説明参考文献NetFlow v5プロトコルおよびネットワークフローデータの保存形式。IPv4フローと16ビットの自律システム番号をサポート。[4]NetFlow v9NetFlow v5の後継プロトコル。IPv6および新しいレコードタイプの柔軟な定義をサポート。[7]IPFIXテンプレートベースのバイナリエクスポート形式。メッセージにはヘッダーとセットが含まれ、セットはテンプレートまたはデータレコードを含む。テンプレートは情報要素の順序付きリストであり、情報要素はデータタイプごとにエンコードされる。[14]評価考察まず、参考文献について引用が正しいかを確認しました。どのプロトコルについても、論文中で引用されている番号を正しく抽出できていました。説明については、回答に書かれている内容は論文中(2章)でも記載されていることであり間違いではありませんでした。一方で、IPFIXの説明に対しては違和感を覚えました。論文中ではIPFIXについては2.3章で解説されています。2.3章は4段落で構成されており、１段落目で概要、２段落目以降で詳細を解説しています。IPFIXについての説明の回答の「テンプレートは情報要素の順序付きリストであり、情報要素はデータタイプごとにエンコードされる。」の部分は、2段落目で回答されている詳細な部分の内容であり、概要の説明としては過剰であると感じました。Q&A No.3 : 図についての解説文献紹介などの際には、論文中の図を説明できるようになっておけると良いとよく聞きます。LLMを用いてそれが実施できるか確認してみます。今回は論文中のFigure１について解説してもらいます。Figure1はYAFのワークフローを示した図になります。YAF: Yet Another FlowmeterのFigure1より引用実装コードquery=""""""Explain important components in Figure1: Basic Data Flow in YAF.Answer in japanese.""""""ans=qa({""query"":query})print(""Query: \n"",query)print(""Ans: \n"",ans[""result""])出力例図1の基本データフローでは、YAFの重要なコンポーネントが示されています。1. de-encapsulation（デカプセレーション）：YAFは、様々なソースからパケットデータを受け取ります。これには、libpcapダンプファイル、ライブキャプチャ（libpcapを介した一般的なインターフェースや特殊なデバイス）、Endace DAGカードなどが含まれます。YAFは、これらのソースからレイヤー2以上の情報を取得し、IPヘッダーにたどり着くまでエンカプセレーションを再帰的に解除します。2. Decoding（デコード）：デエンカプセレーションされたパケットは、レイヤー3および4のデコードレイヤーに渡され、パケットデータからフローキーとカウンターを抽出します。YAFでは、フローキーは伝統的な「5タプル」（ソースと宛先のアドレス、ソースと宛先のポート、プロトコル）にIPバージョン番号（4または6）を加えたもので構成されます。3. The Flow Table（フローテーブル）：YAFのフローテーブルは、ハッシュテーブルでインデックス付けされたピッカブルキューとして実装されています。このデータ構造は、ハッシュテーブルを介したフローテーブル内の任意のエントリへのランダムアクセスを可能にします。フローテーブルの先頭にある最も最近見られたエントリにも定数アクセスが可能です。これにより、アイドルなフローのタイムアウトを効率的に実装することができます。4. encode and export（エンコードとエクスポート）：フローレコードは、フローテーブル内のフローが完了したと判断されたときにエンコードされ、エクスポートされます。エンコードされたフローレコードは、IPFIXプロトコルを使用してエクスポートされ、コレクタまたはIPFIXファイルに送信されます。これらのコンポーネントは、YAFの基本的なデータフローを実現するために重要な役割を果たしています。評価考察Figure1は、論文の3章の前半で解説されています。1-3までの回答は、3.1章Recursive De-encapsulation、3.2章Decoding、3.3章The Flow Tableの内容を元に回答しているようです。それぞれの章の最初の段落を引用して回答しているようでした。内容的にも誤りはなさそうです。4の回答については、Introductionの部分「exports IPFIX flows to a collector or to an IPFIX file [25].」を引用しているように見えます。全体を通してみると、図の解説については大まかには合っているが、完全に正しく回答しているわけでは無いのかと思います。文献紹介で図の解説をする場合は、人の手による確認・修正が必要そうです。(とはいえ、概要を簡単にまとめる分には利用できるレベルだという所感です)今回のQ&Aとしては以上になります。まとめ実際に触ってみて、第一の感想としては大変便利だと感じました。pdfを用意してクエリを投げるだけで、ある程度の情報を得ることができますし、アウトプットする際にも表形式など簡単に欲しいフォーマットにまとめることができます。予備知識が少ない初学者にとってはありがたいツールになりそうです。一方で、LLMによる回答が適切ではない場合もあるということを認識する必要があります。LLMの回答は正しくない場合もあるということを念頭に置いたうえで、論文を読んだ後にLLMで出力した要約を読んで内容が合っているかどうかチェックする（あるいはその逆）など対応すべきだと感じます。とはいえ、簡単に情報を整理抽出できるので、課題は一定あるものの使い方を工夫していけば理解や業務の効率化につながるのではないでしょうか。今回は深堀しませんでしたが、プロンプトの書き方も難しさはあると感じました。巷でも言われていることですが、実行の度に出力が変わったり、文字制限が守られていなかったり（論文の要約は、「150文字で答えて」と要求したつもりが、実は、388文字だった）など意図と異なる部分も多くありました。工夫する余地はありそうです。おわりに今回は、LLMを用いた論文pdfのQ&Aについて紹介しました。個人には、実際に使ってみて便利ではあるもののまだまだ改善余地はあると感じました。今後とも様々な観点で検証を進めていきたいと思います。今回紹介した内容や、その他弊社の取り組みに興味がございましたら、ぜひお声がけください。https://www.ariseanalytics.com/recruit/"
https://www.ariseanalytics.com/activities/report/20231023/,【ARISE university】『論文レベルで理解しよう！』大規模言語モデル(LLM)編,OTHERS2023.10.23Tweet【ARISE university】『論文レベルで理解しよう！』大規模言語モデル(LLM)編ARISE universityARISE analyticsでは、毎週金曜午前中を自己研鑽時間として原則MTG禁止としています。その時間には、社内勉強会である「ARISE university Training」が毎週開催されており、全社員及び業界のトップランナーなどが講師・ファシリテーターとなり、年間 約100件の講座やLTなどを実施しています。今回は「ARISE university Training」で、Marketing Solution Divisionの渋谷さんが行った「【論文レベルで理解しよう！】大規模言語モデル（LLM）編」という講義の資料を展開します。【論文レベルで理解しよう！】​ 大規模言語モデル(LLM)編​fromARISE analytics
https://www.ariseanalytics.com/activities/report/20231023-1/,【ARISE university】『論文レベルで理解しよう！』欠測値処理編,OTHERS2023.10.23Tweet【ARISE university】『論文レベルで理解しよう！』欠測値処理編ARISE universityARISE analyticsでは、毎週金曜午前中を自己研鑽時間として原則MTG禁止としています。その時間には、社内勉強会である「ARISE university Training」が毎週開催されており、全社員及び業界のトップランナーなどが講師・ファシリテーターとなり、年間 約100件の講座やLTなどを実施しています。今回は「ARISE university Training」で、Marketing Solution Divisionの田中さんが行った「【論文レベルで理解しよう！】欠損値処理編」という講義の資料を展開します。
https://www.ariseanalytics.com/activities/report/20231027/,現環境最強のテーブルデータ向けモデルTabPFNの紹介,"ARISE Tech Blog2023.10.27Tweet現環境最強のテーブルデータ向けモデルTabPFNの紹介初めにこんにちは。技術開発 Teamの福嶋です。皆さんはテーブルデータの分類タスクを行う際どのようなモデルを使っていますでしょうか？当社においては、初手としてLightGBMやXGBoostなどの勾配ブースティング木（GBDT）を用いていることが多い印象です。ChatGPTやStable Diffusionなどテキスト・画像データに対して圧倒的な精度を出している深層学習系のモデルはどうかというと、残念ながらあまり利用されていない印象です。実際、テーブルデータに最適化された深層学習の手法で有名なところとしてTabnet[1]などが挙げられますが、精度・処理速度という点においてGBDTには遠く及ばず、業務利用するメリットがあまりありませんでした。そんな常識を覆し、2023/5時点[2]で、オープンデータの平均スコアで最も高い精度を出した深層学習をベースのモデルであるTabPFN[3]について今回ご紹介します。TabPFNの凄さTabPFNの凄さは、「学習速度」にあります。下図は、テーブルデータであるOpenML-CC18 Benchmark[4]を学習した際の精度を示したものになります。（図１：OpenML-CC18 Benchmarkを使用した既存手法における学習時間[横軸]当たりのROC-AUC[縦軸]の結果。左上にあるTabPFNが最も早い学習時間で最も高い精度を出せることを示している [3]より引用）TabPFNは、圧倒的に速い学習時間でGBDTモデルを上回る精度がでています。これは、既存の木・深層学習ベースとは大きく異なる方法で学習していることが理由であり、サンプル数<1000レコード以下,特徴量<100個,目的変数<10クラスであれば1秒以内で学習・推論が可能とされています。また、テーブルデータのモデルを比較したサーベイ[3]によると、公開されているデータセットにおいて精度・学習速度ともに平均的にはTabPFNが最もよいことが示されています。汎用性もあり、非常に優秀なモデルであることが窺がえます。TabPFNの原理それでは実際の仕組みを確認してみます。TabPFNは、簡単には「構造的因果モデルによる人工データ生成」と「PFN（Prior-Data Fitted Networks）[4]による事後分布近似」を活用した、Transformerベースモデルです。目標TabPFNの目標は、学習データ（D=(Xtrain, ytrain)）と推論データの説明変数（Xtest）を仮定したときにおける目的変数（ytest）の事後予測分布（PPD）を推定することです。このPPDは一般的には計算できませんが、妥当な構造的因果モデル空間（SCM）Φを指定することにより、下記式で近似可能であることがPFN[4]によって示されています。手順目標を達成するために、TabPFNの学習と予測は大きく3ステップで行われます。最初の2ステップは実データを使用しない事前学習フェーズであり、最後の1ステップで実データを使い事後分布を計算します。（図３ 学習及び推論の概要図。 [3]より引用）ステップ１：φ∈Φと人工データの生成まず初めに、SCMの空間Φ（下図(c)）からランダムにサンプリングしたφ（下図(b)）を用い、人工データD=(X, y)を生成します。（図４：(a)ベイジアンニューラルネットワーク(BNN)と(b)SCMの因果グラフ構造の違い。(c)はSCMの集合Φのイメージ図。[2]より引用）各SCMは上図(b)のようなDAG構造とノード間の関係を定義する関数fに基づいています。ノイズ変数（上図(b)のz(*)にあたるものだと思われる）をランダムにN個サンプリングし、SCMに則り伝搬することでX=(Zx(0), Zx(1)・・)及びy=(Zy(0)・・・)を生成しているようです。ステップ２：生成したDを用いたTabPFNのモデルqΘの学習次に、1でD=(X, y)をDtrainとDtest=D\Dtrainに分け学習を行います。モデルはTransformerをベースとしたアーキテクチャであり、通常の分類タスクと同様に以下のCrossEntropyLossで更新を行います。ステップ３：実データによる事後分布推定最後に、実データD=(Xtrain, ytrain)とXtestを前述のPPDの近似式に適用します。たった一回のフォワード処理のみでPPDの推定ができるため、短時間で学習・推論が終了します。ステップ2によって学習された事前学習済みモデルは既に著者のgithub[6]に公開されているため、必要な処理は実質ステップ3のみです。TabPFNの実装pipinstalltabpfn==0.1.9fromtabpfnimportTabPFNClassifier

X_train=train_df[FEATURE_COLS].values
y_train=train_df[TARGET_COL].values
X_test=test_df[FEATURE_COLS].values

model=TabPFNClassifier(N_ensemble_configurations=56,device='cuda:0',)model.fit(torch.tensor(X_train,dtype=torch.float),torch.tensor(y_train,dtype=torch.long))pred=model.predict_proba(torch.tensor(X_test,dtype=torch.float))TabPFNの欠点ここまでTabPFNの良い点ばかり書いてきましたが、TabPFNには唯一かつ最大の欠点があります。それは、サンプル数<1000レコード以下,特徴量<100個,目的変数<10クラスの小さなデータセットにしか適用ができないという点です。レコード数に対し二次関数的に増えていく処理時間の問題や1000行以上のデータセットではあまり得意でなく既存手法と比較し精度が出ない[7]といった課題もあるようですアンケート分析などの比較的少量のデータを分析する際には利用用途がありそうですが、ビッグデータへの適用にはまだ課題があります。まとめ今回は、GBDTを上回る深層学習ベースのモデルであるTabPFNを紹介しました。大規模データに適用が難しいという欠点もありますが、少量のデータに対してはとても良い精度が得られるため、GBDTに代わるモデルとして使い道を検討していきたいと思います。参考文献[1]:[1908.07442] TabNet: Attentive Interpretable Tabular Learning (arxiv.org)[2]:When Do Neural Nets Outperform Boosted Trees on Tabular Data?[3]:TABPFN: A TRANSFORMER THAT SOLVES SMALLTABULAR CLASSIFICATION PROBLEMS IN A SECOND[4]:OpenML Benchmarking Suites[5]:TRANSFORMERS CAN DO BAYESIAN INFERENCE[6]:https://github.com/automl/TabPFN/tree/main[7]:[論文著者のX[旧 Twitter]における投稿]（https://twitter.com/SamuelMullr/status/1584795629775380481）"
https://www.ariseanalytics.com/activities/report/20231117/,ARISE Kaggle部活動記 #5,"OTHERS2023.11.17TweetARISE Kaggle部活動記 #5Kaggle部はじめにはじめまして。DX Technology Unit 通信セキュリティ技術開発 Teamの福嶋です。ARISE Kaggle部の活動記へようこそ。今回、データ分析コンペティションプラットフォームであるKaggleにて開催されたCommonLit – Evaluate Student Summariesにソロで参加し、2064人中12位で入賞（金メダル！）、そして念願のKaggle Competition Masterとなることができました。今回は、そのコンペの解法を紹介したいと思います。コンペの概要このコンペの目標は、学生が要約した文章の質を定量的に評価することです。データセットとしては、ある特定の文章の全文とタイトル、設問、それに対する学生の要約文、教師がRubric[1]に則り評価した二種類のスコアが与えられます。要約文を入力として教師の評価した二種類(Content, Wording)のスコアをより精度よく予測できるモデルを作成することが最終的な目標となります。図1: 二種類の評価観点。Disscussion[2]より引用。Contentは「正しく要約されているか」の観点、Wordingは「言葉、文法が適切か」の観点で評価されている。このような自動採点モデルは、多くの学生が要約した文章に対し迅速なフィードバックを可能にし、読解力や文章力の向上の助けになることが期待されています。方針Kaggleで上位入賞している方々の特徴として「異なる特徴によって学習された、精度が高い複数のモデルをアンサンブル」している傾向があります。それを参考に今回は以下の二パターンのモデルを作成し、改良することで精度を上げていくことにしました。「元の文章」「設問」と「要約文」の関係性に焦点を合わせたモデル（長文を入力するので、便宜上Long Modelと呼ぶ）「要点が捉えられているか」「設問に正しく答えられているか」等マクロ的な特徴を捉える「要約文」のみの焦点を合わせたモデル（上記と比較して短い文を入力するので、便宜上Short Modelと呼ぶ）「良い単語・熟語を使っている」「関係代名詞を使っている」等ミクロ的な特徴を捉える前者はContent、後者はWordingの精度向上を意図していますが、ContentとWordingには正の相関関係があり、互いに影響しあっている可能性があったため、両モデルとも両方のスコアを目的変数としました。解法Long Modelまず、基本的な構造を決めていきます。入力データはQAタスクと同様に考え、「要約文」+ [SEP] +「設問」+ [SEP]+「元の文章」の形式で入力し、自然言語モデルのTokenizerに与えることとしました。なお、最大長は1536トークンです。（※ [SEP]は、自然言語処理において複数のテキスト文を入力する際に用いられる区切りトークン(Separate Token)）バックボーン(BERT, RoBERTaなど)は、入力データが最大約1500単語であったこともあり、長文が入力できるものである必要がありました。長文が入力できるものとして、DeBERTa[3]、LongFormer[4], BigBird[5]などが挙げられますが、精度検証した結果deberta-v3-largeが最も精度が高かったため、今回はこちらを採用しました。次にハイパーパラメータや細かい調整についてです。学習率、エポック数などのパラメータのチューニングの多くはあまり精度に影響がありませんでしたが、Pooling, Freezing, Ensembleの改良が特にこのモデルの精度向上に寄与しました。他にもいくつかありますが、今回はこの3つに焦点をあてます。PoolingTransformer系統のモデルは、入力のトークン単位でベクトルを出力するため、文章の分類においてはPooling処理を通し、文章単位のベクトルに圧縮する必要があります。Poolingの方法にはいくつか存在し、有名なところだとCLS Pooling, MaxPooling, Concat Pooling, GeMPoolingなどが挙げられます。有名どころのPoolingを試した結果、Concat Pooling（最終N層のCLSベクトルを連結させる）が最も精度が良かったのですが、予測に8時間かかることが判明しました。今回のコンペでは9時間以内に予測しなければならず、このままではアンサンブルを行う時間的余裕がありません。そこでCLSベクトルを連結させるのではなく平均を取ることでモデル必要パラメータ数が減少し、僅かな精度劣化で推論時間の短縮(8時間→6時間)に成功しました。(なおAMP[7]によっても推論時間の短縮は可能なのですが、なぜか精度が極端に悪くなってしまったので断念しました。)classCLSMeanPooling(nn.Module):def__init__(self,n_layers=4):super(CLSMeanPooling,self,).__init__()self.n_layers=n_layersdefforward(self,all_hidden_states):concatenate_pooling=torch.cat(
            [all_hidden_states[-(i+1)][:,0].unsqueeze(-1)foriinrange(self.n_layers)
             ],-1).mean(-1).squeeze(-1)Freezingこれは、過去の自然言語コンペでも多くの使用例がありますが、バックボーンの先頭N層の学習を行わない(Freezing)というものです。deberta-v3-largeは24層のTransformerによって構成されるためそのうち先頭の12層分を固定することで、過学習を抑制できます。今回の入力データに用いている「元の文章」は4種類しかありませんでしたので、これを行わないと特定の文章構造に最適化されてしまい精度が出ませんでした。deftop_half_layer_freeze(module,n_layer):foriinrange(0,n_layer//2+1,1):forn,pinmodule.encoder.layer[i].named_parameters():p.requires_grad=Falsetop_half_layer_freeze(deberta_model,24)Ensemble(LightGBM with Pooling)「deverta-v3-large」–「Pooling」–「Linear(1024 → 2)」のモデルでも一定精度が出ていました(Private LBで60位ぐらい)。しかし、損失の下がり方が不安定で汎化性能に不安を覚えたため、Long Model内部でもアンサンブルを行うことにしました。とはいえ既に6時間近く推論に使っているため、同じ大きさのモデルをアンサンブルを行うのは不可能です。そのとき、このコンペでzero-shot-learningが活用できないかと考えていた際に偶然見ていた記事[8]を参考に「Pooling層を抜き出してLightGBMで学習し、アンサンブル」することを思いつきます。これが大幅な精度向上につながりました(Private 36位ぐらい)。図2: Long Model アーキテクチャShort ModelLong Modelの精度改善が一段落し、Short Modelの検討に移りました。一から作成することも考えましたが、既にPublicで多く引用されていたNotebook[9]があったため、こちらを参考に一部改良を加えることにしました。このNotebookの特徴として、「バックボーン+ huggingfaceのSequenceClassification」によって学習されたスコアとFeature Engineeringで作成した変数(文章の長さ、誤字数など)を合わせLightGBMで再度スコアを予測する、といった構造があります。（詳細は引用元をご確認ください。）今回は、バックボーン・Feature Engineeringの一部を変更することで精度改善を行います。バックボーンは元のNotebookではdebera-v3-base[10]を用いていましたが、いくつかのパターンを精度検証した結果、Long Modelと同様deberta-v3-largeが最も精度が高かったため、こちらに変更します。またFeature Engineeringの方法として以下の変更を行うことで（CVは上がりませんでしたが）Private LBの精度向上につながりました。「元の文章の長さ」の削除4種類しか元の文章がなく、過学習していそうだったため（実際これを入れた場合はPrivate 16位ぐらいで金圏から脱落していた。）「誤字数」や「引用Overlap数」など〇〇の件数になっていた部分を比率に変更「件数」だと元の文章や要約文の長さに比例してしまい本来評価したい意味合いが変わってしまうような気がしたため比率に変更しました。例えば, 「誤字数」は「誤字しやすいほどスコアが低い」ことを判断したいが, 今回のコンペでは, 「誤字数が多いほど要約文が長くスコアが高い」となっており, 真逆の意味になっていた。図3: Short Model アーキテクチャLong & Short Ensemble最後にLong ModelとShort Modelをアンサンブルします。過去のコンペではOptunaで重みを最適化している例も存在しましたが、あまり精度向上につながらなかったため、単純に平均を取っています。図4: Ensemble アーキテクチャ終わりに今回は、CommonLit – Evaluate Student Summariesで入賞した際の解法についてご紹介しました。ARISE analyticsでは、Kaggle等の分析コンペティションで上位成績を残すと、インセンティブとして報奨がもらえるARISE Tech Master制度があります！（今回は30万貰えます！）次はGrandMaster目指して頑張っていきたいと思います。我々と一緒にKaggle部を盛り上げてくださる方はこちらのページからご連絡お待ちしております!!!!参考文献[1]:https://ja.wikipedia.org/wiki/%E3%83%AB%E3%83%BC%E3%83%96%E3%83%AA%E3%83%83%E3%82%AF[2]:https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/424402[3]:https://arxiv.org/abs/2006.03654[4]:https://arxiv.org/pdf/2004.05150.pdf[5]:https://arxiv.org/pdf/2007.14062.pdf[6]:https://huggingface.co/microsoft/deberta-v3-large[7]:https://developer.nvidia.com/automatic-mixed-precision[8]:Zero-shot learningの紹介：見たことがない画像やニュースを予測してみました – GMOインターネットグループ グループ研究開発本部[9]:Debertav3+LGBM (no autocorrect) | Kaggle[10]:microsoft/deberta-v3-base · Hugging Face"
https://www.ariseanalytics.com/activities/report/20231129/,新人エンジニアがGitHub Copilotを使ってみた。,"ARISE Tech Blog2023.11.29Tweet新人エンジニアがGitHub Copilotを使ってみた。はじめまして、Marketing Solution Division,Service Design Unit所属ソリューションエンジニアの相沢です。今回は、新人エンジニアの私がエンジニア向け研修中に利用した「GitHubCopilot」について、”新人エンジニアが使うことでどのような良いことがあるか”についてお話しします。1.GitHub CopilotとはGitHub Copilotとは、一言でいうと「AIが自動で条件に合うコードを提案してくれるサービス」です。こんな機能を実現したい！とコメントした内容をコードにしてくれたり、既にあるコードを修正する際に他のコードの内容に倣った形でプログラムを作ってくれたりします。ChatGPTでも同様のことができますが、GitHub Copilotはコーディングに特化したAIで、コードを実際に書くコードエディタ上で使うことができます。加えて、毎回質問せずともスマホの予測変換のようにコードを提案してくれるので、より効率的にプログラムを作成することができます。では、実際にGitHub Copilotを使った画面をお見せしながら、どのような良いことがあるかをお話ししていきます。2.GitHub Copilotを使ってみたここでは、GitHub Copilotがコードをどのように提示してくれるかの二つの例を示します。まず初めに、”作りたいもの”をコメントに入力したときの動きをお見せします。この例では、Go言語で作るのでファイル名をtest.goで作成し、作りたいものとして「1~50までの乱数を生成する関数」をコメントに記入しています。すると、GitHub Copilotはコメントに書いた”実装したい内容に合ったコード（画像のグレー文字の部分）”を提示してくれます。このコードの内容は、”0~49までの整数をランダムで生成してくれる関数の値に+1をした値を返してくれる”という内容で、コメントに入力した”作りたいもの（1~50までの乱数を生成する関数）”を生成してくれています。続いて、必要なものを補完してくれる機能の動きをお見せします。この例では、”新しいユーザーを作成する関数”を途中まで書いています。すると、GitHub Copilotは別のファイルで定義していたmodel.Userの構造体（下の画像）を参考にして、「Icon」、「EmailAddress」のフィールドを提示してくれました。このようにコードを途中まで書くと、GitHub Copilotが内容を理解し、プログラムに必要と思われるものを補完してくれます。候補に挙がったコードはtabキーで入力まで行えるので、考える・調べる時間に加えて、入力する時間までも短縮することができます。3.新人エンジニアが使うメリットいざプログラミングを始めてみると、”初学者がはまりやすい罠”というものがいくつかあります。例にもれず私自身も罠にはまり、既に何度も挫折しています。時には失敗も必要ですが、最初から何度も転んでしまうと、立ち上がるのが大変ですよね。そんなときに手を差し伸べてくれるのがGitHub Copilotなのです。GitHub Copilotを使うことで、”初学者がはまりやすい罠”をいくつも回避することができます。直面した罠　その１　何から調べればよいのかわからないプログラミング初学者は、エンジニアリングに取り掛かる際に”まず何から調べればよいか”がわからないことがあるかと思います。「このWebアプリに記事の一覧を取得する機能を作ってくれ」と言われたときに、「Webアプリ」や「一覧取得機能」などのわかっているワードから調べ、調べた中でさらにわかったワードを調べ、さらにわかったワードを…を繰り返して、実現したいものに必要な情報を穴埋めしていくことが多いと思います。しかし、その過程では、筋違いな方向に検索を進めてしまい、次につながるワードが途切れてしまったり、間違いや古い情報に当たってしまったりして、適切な情報にたどり着けないことがあります。そんなときにGitHub Copilot Chat（以下、Copilot Chat）を活用することで、関連するワードから具体的な手順まで知ることができます。この例では、「goで記事の一覧を取得する機能を作りたいです。」とCopilot Chatに対して質問しています。すると、Copilot Chatは具体的な手順を1~5で示し、具体的なコードも返してくれました。これにより、初めから具体的な作業イメージをつかむことができる実装に必要なコードを得ることができる検索のヒントに用いることができるワードを知ることができるという3つのメリットを享受することができます。直面した罠　その２　参考コードのどの部分を変更すればよいかわからないCopilot Chatを利用してコードを生成した場合や、参考コードをインターネットの記事から見つけた場合、そのまま使えるわけではなく、自分が作りたいものと差分があることが大半だと思います。その場合には、どの部分を自身で変える必要があるかを判断するために、さらなる調査が必要になります。知識を身に着けるために調べることも重要ですが、調べるたびにわからないことが増えて頭がパンクしてしまっては元も子もありません。また、膨大に時間がかかってしまうこともあります。しかし、GitHubCopilotに自身が作りたいコードの内容を教えることで、適切にコードを書き換えてくれます。この例では、”直面した罠 その1”で質問した「goで記事の一覧を取得する機能を作りたいです。」という内容に対して出力されたコードに、次の2つの指示を出しています。関数等で使われている「articles」という名前を「topics」に変えてほしい著者の情報を加えてほしいするとCopilot Chatは、2つの指示の内容に合わせて修正したコードを提示してくれます。1については、コード全体を通して、単数形/複数形に関わらず、該当する箇所を修正してくれています。2については、「誰が書いたか」に合わせてAuthorという情報を追加してくれています。このように指示が多少曖昧でも、Copilot Chatが内容を考慮して、適切に修正をしてくれます。直面した罠　その３　タイミングよく回答が得られない。何かわからないことがあったときにインターネットや書籍を利用して調べてもわからないことは日常茶飯事だと思います。そんなとき、先輩エンジニアやプログラミングスクールの先生、オンラインのチューターに質問をすると思いますが、回答待ちの時間があったり、頻繁に会話ができない場面があったりとタイミングよく回答が得られないことがあるかと思います。そんなときにCopilot Chatを使うことで、数十秒で回答がもらえたり、インタラクティブに会話したりすることができます。加えて、拙い文章で質問してもCopilot Chatが不明瞭な部分を丁寧に質問して返してくれます。これらにより、わからないことが出たタイミングでCopilot Chatに質問することで、作業を中断することなく、迅速に疑問を解消することができます。この例では、GitHub Copilotが提案した関数の意味が分からなかったので解説をお願いしています。するとCopilot Chatは、処理の内容を関数名や構造体の名前を交えながら解説してくれています。それでもわからない部分がある場合には、その部分をチャットで質問することで、更に詳しい解説をしてくれます。4.全エンジニアが使うメリット上記3つの新人エンジニアが使うメリットに加えて、全エンジニアにとってのメリットもあります。それは、”自身が書いているコードを元に提案をしてくれること”です。プログラムや関数に関する質問はChatGPTでも同様にすることができますが、前提条件や現在のコードの状況を入力する必要があります。一方で、GitHub Copilotでは、コードエディタ上で利用することができるため、開いているファイルの情報を考慮した提案を行ってくれるのです。そのため、前提条件や現在のコードの状況をAIに学習させる時間を削減して効率よく情報を得ることができます。5.使った感想個人的な感想としては、GitHub Copilotを使って”時間”と”品質”の観点で効率が上がったと感じています。【時間に関して】“適切な情報にたどり着くまでの時間”が大幅に短縮されたと感じています。通常の検索では、検索ワードが適切でないために”ほしい情報”にたどり着けなかったり、それらしい内容にたどり着いたものの情報が古かったりする場合があるため、何度も調べ直し時間がかかることが多いです。一方でGitHub Copilotは、ワードなしに調べる（この部分を〇〇したい等）ことができたり、比較的新しい情報を元にした回答を返してくれるため、短時間で”ほしい情報”を得ることができると感じました。【品質に関して】変数や関数の命名時に”統一感”があり、”内容がわかりやすい”名前を提示してくれるため、可読性が高いコードを書くことができると感じました。6.まとめGitHub Copilotを使うことで、効率よく開発を進めることができます。特にプログラミング初学者にとっては、何をすべきか・関連するワードを提示してくれる自身の作りたいものに合わせてコードをカスタマイズしてくれるいつでも質問ができるというメリットがあります。すべてのエンジニアに向けては、前提条件や現在のコードの状況をAIに学習させることなく自身のコードを元に提案をしてくれるというメリットがあります。この記事を読んで、少しでもGitHub Copilotに興味を持っていただけましたら幸いです。ぜひ、GitHub Copilotを使ってみてください。"
https://www.ariseanalytics.com/activities/report/20231215-2/,GitHub Copilotと一緒にTerraformを書いた体験談 初めてのインフラ構築,"ARISE Tech Blog2023.12.15TweetGitHub Copilotと一緒にTerraformを書いた体験談 初めてのインフラ構築はじめまして、8月にARISEに入社したMarketing Solution Division, Service Design Unit所属の森です。前職ではフロントエンジニアでしたが、案件を通じて自分の作ったアプリを動かせるインフラに興味をもちました。入社直後にARISE社内でプロジェクトが立ち上がり、インフラの枠が空いていたのでインフラ担当として参画させてもらいました。社内プロジェクトではインフラ構築をAWS＋Terraformで行いますが、インフラ初心者の私にとってははじめて知ることばかりで右も左もわからない状態でした。今回は、そんなインフラ初心者の私がGitHub Copilotと一緒にインフラ構築した時の体験を話すことでGitHub Copilotについて紹介していきたいと思います。1. GitHub Copilotとは詳しい紹介はこちらの記事でされているので簡単に説明すると、GitHub CopilotとはGitHubが提供しているクラウド型AIツールで、プログラミングに特化したAIアシスタントサービスになります。プログラミング特化のため、ソースコード内のコメントの内容からコードを書いてくれたり関数名等から処理を予測して書いてくれるなどかなり強力なツールになっています。これから実際に私がインフラ構築をした際の体験を通して、GitHub Copilotを使うことで知見の浅い分野でも高速キャッチアップできることをお話していきたいと思います。2. GitHub CopilotとAWSリソースを構築してみた体験談背景社内プロジェクトがはじまり、今までインフラに携わったことがない私にとってAWS＋Terraformは知らないことだらけだったのでGitHub Copilot導入前は各AWSリソースにどのような設定値があるのかわからずなかなか手が動かせないドキュメントを見ても自分のやりたいことを実現させる方法がわからず、調べものに時間がかかるTerraformで設定するプロパティの意味をドキュメントだけでは理解しきれないといった具合でなかなか作業が進まない状態でした。それがGitHub Copilotを導入したことで、GitHub Copilotが教えてくれた情報を足掛かりに作業を進めることができるようになりました。では実際にどうGitHub Copilotを活用したのかを、TerraformでCognitoを構築した際の体験をお話ししようと思います。体験談1どうコードを書けばいいのか教えてくれるCognitoでユーザープールを作ろうとしたとき、そもそもどういう設定値があるのか知らなかったので手が動かない状態でした。Terraformの公式ドキュメントを見に行ったのですが、設定値が多すぎてまず最初に何を書けばいいのかが全くわからず苦戦していました。そこでGitHub Copilotを使ってみると、コメントからCognitoのユーザープールを作りたいときの書き始め方を提案してくれました。Tabキーを押すとそこからさらに必要と思われる設定値をどんどん提案してくれました。また、Ctrl+Enter（Mac+VSCodeの場合）で複数の候補を提案してくれます。このように、GitHub Copilotを使えばどうコードを書けばいいのか教えてくれるのでとりあえず手を動かすことができます。0からコードを作るときに比べてGitHub Copilotが教えてくれたコードを足掛かりにできるので、知識や経験が浅くてもとりあえず作業を進めることができました。体験談2 わからないところを教えてくれるある程度GitHub Copilotが提案してくれたコードを元に作業を進めていくと、今度は要件に沿って設定値を変更する必要が出てきます。例えばgenerate_sercretの値がfalseになっていますが、これをtrueにするとどういう変化が起きるのかわかりません。Terraformのドキュメントでこの設定値の意味を調べると・generate_secret – (Optional) Should an application secret be generated.と記載されており、これだけではいまいち設定を変更した際のイメージがつきません。そこで、Copilot Chatにgenerate_sercretをtrueにした場合にどういった変化が発生するのかを聞いてみました。Copilot Chatはコードに関する質問をチャットベースでやりとりできるGitHub Copilotの機能の1つです。これにより、generate_sercretをtrueにするとアプリとCognitoが通信する際に秘密鍵を提供する処理が必要になることがわかりました。AWSの公式ドキュメントと照らし合わせても、秘密鍵を割り当てた場合はCognitoにリクエストする際にリクエスト本文にクライアントシークレットを含むハッシュが必要になることが説明されているためCopilot Chatの回答は正しいと確認できました。今回の要件ではそこまでセキュリティを強固にする必要はないため、generate_sercretはfalseでいいと判断することができました。GitHub Copilot導入前はCognitoの秘密鍵について調べ、秘密鍵が生成されることによりどういった変化が生じるのかを調査する必要がありました。しかし、Copilot Chatを使えば自分が今見ているコードを元に質問して回答を得ることができるので調べものの手間をかなり省くことができます。3. 使ってみた感想実際にGitHub Copilotを使ってみて、AIとペアプログラミングをしているかのようにコードを書けるのが非常によいと感じました。一般的にペアプログラミングの利点は作業効率化知識を深めるミスを減らし品質が向上するがあげられますが、一方で1つの作業を2人で実施するので実質コストが2倍になってしまう欠点があります。これがGitHub Copilotを使えば月2000円程度のコストでペアプロができる環境を用意することができます。私のように知見が浅い分野で実際にGitHub Copilotを使い、AIとペアプロしているみたいだなと感じた部分は以下2点です。一緒にコードを書けるので手が止まらない（作業効率化）知見の浅い分野でも聞けば教えてくれるので調査の足掛かりを掴みやすい（作業効率化／知識を深める）一緒にコードを書けるので手が止まらない知見の浅い分野で0から処理を書く際、右も左もわからないのでなかなか手を動かせないと思います。やりたい処理は頭の中にあるけど、それをその言語でどう書けばいいのかわからず手が止まってしまいます。GitHub Copilotはコメントや途中まで記述した内容をもとにどうコードを書けばいいのか提案してくれます。また、候補も複数出してくれるのでその中から自分が実現したいものに近しい結果を選び、必要に応じて部分的に修正すればいいので、0からコードを書くよりも作業の効率が向上します。実際にCongitoのユーザープールをGitHub Copilotにコメントの要件に沿って作成するコードを書いてもらいました。指定した条件は以下の通りです。ユーザプール名はtest-user-poolとすることパスワードは8文字以上で大文字小文字数字記号を含むユーザーを新規に作成できるのは管理者のみカスタム属性として文字列のtestを保持しているこのようにGitHub Copilotがコメントの条件を満たすユーザープールを生成できるコードを提案してくれました。言葉では自分のやりたいことはわかるけれど、知見が浅いとそれをコードに落とし込むのが困難な場合があります。そんな時はコメントで実現したいことを言葉で書き、GitHub Copilotにコードを提案してもらうことでそのコードをもとに処理を追加したり、調べたりと先に進むことができます。知見の浅い分野で調査を進める足掛かりにできる知見が浅い分野だと、こういう処理を書きたい！と思っても何を足掛かりにして調べていけばいいのかわからず見当違いな調べ方をして時間を無駄にしてしまう場合があります。GitHub Copilotを使えば以下のような調査手順になります。どうすれば自分がやりたいことを実現できるかCopilot Chatに聞くCopilot Chatの結果をもとにドキュメント等を調べるドキュメントを見てわからないところがあればさらにCopilot Chatに聞く最初に自分やりたいことを実現する方法をCopilot Chatに聞くことで大枠の方針を決めることができます。その方針を元にドキュメント等を調べていくので、見当違いな調査に時間を奪われる可能性が低くなります。以下は私がCognitoにカスタム属性を追加したいときにCopilot Chatに問い合わせた結果です。Copilot Chatによると、schemaという属性を使うことでカスタム属性を追加できるようです。私が設定したいカスタム属性は数値型最大値100,最小値0という条件だったため、数値型のカスタム属性の最大最小値を設定する方法をCopilot Chatが教えてくれたschemaという情報をもとにTerraformの公式ドキュメントを調べました。すると文字列型のカスタム属性に対して最大最小値を設定する方法は見つかりましたが、数値型に対して設定するプロパティが見つかりませんでした。そこで、Copilot Chatに数字型のカスタム属性で最大最小値を設定する方法を追加で質問してみました。するとCopilot Chatはnumber_attibute_constraintsを使うと教えてくれました。この情報をもとにTerraformの公式ドキュメントを再度確認すると、確かにnumber_attibute_constraintsというプロパティがあり、私が見落としていたことに気づけました。このように、Copilot Chatに質問することで自分のやりたいことを実現する方法をある程度足掛かりのある状態で調べることができるため調査の時間を削減することができます。Copilot Chatに聞く前はドキュメントを見てもいまいちschemaでカスタム属性を設定できるのかどうかわからず、他のサイトを調べたりしていました。Copilot Chatはチャットベースでの会話で質問でき、その際に自分のコードを参照させることができるのでまるでAIとペアプログラミングしているような感じで作業をすることができます。4. まとめGitHub Copilotを使うことで常にAIとペアプログラミングしながら作業を進めることができます。その結果、ペアプログラミングの恩恵である作業効率化知識を深めるを享受することができるため、高速でキャッチアップが可能なのではじめての分野でも調べる時間を最小限にして作業を進めることができるペアプログラミングのようにCopilot Chatに不明な部分を質問することで知見を深めることができるという効果を得られ、知見が浅い分野での参画であっても作業を効率的に進めることができます。知見が深い分野であれば、GitHub Copliotと一緒に作業してコードを提案してもらうことでダブルチェック的な使い方をすることでペアプログラミングにおける品質向上の利点も享受できると思いました。実際私はGitHub Copilotと協力しながらインフラ構築を実施したことにより、AWSの知見が以前よりも格段に深まりました。ぜひ皆さんもGitHub Copilotを使って自分の興味ある分野への知見を深めていってください！"
https://www.ariseanalytics.com/activities/report/20231220/,生成AI・LLMのメモリ最適化・高速化ライブラリ-DeepSpeed-,"ARISE Tech Blog2023.12.20Tweet生成AI・LLMのメモリ最適化・高速化ライブラリ-DeepSpeed-はじめにARISE analyticsの近藤です。弊社では、KDDIグループを支えるためのAI技術開発を行っています。その一環でプロダクト開発を目的とした研究開発も進めています。AIを活用したプロダクトとして、RPA（Robotic Process Automation）などが挙げられます。RPAでは大量のデータを短時間で処理する必要があります。そのため、AIの性能に加え処理速度も重視されます。また、AWSやAzure等のクラウド上で処理する場合は、インスタンスコストを削減するため可能な限りスペックを抑制する必要があります。とくにスペック抑制に関しては、近年普及が進んでいるLLM（Large Language Model）で重視されています。パラメタ数はLLMの性能に影響を及ぼす重要なファクタですが、パラメタ数が大きくなるほど必要とするGPUのスペックも高くなります。具体的には、モデルサイズはパラメタ数×4バイト（パラメタ1つにつき32bitの浮動小数点、すなわち4byteで表現した場合）で表されるため、同等以上のGPUメモリを必要とします。例えば、表１ではGPT-3は175B（1750億）のパラメタを持つため、モデルサイズは7000億byte（651.93GB）となります。このようなモデルを読み込むためには、NVIDIAA100 80GBが9台必要になる計算となります。このスペックを用意しようとすると、かなりの高コストになることが予想されます。表１：2023年10月時点の代表的な大規模言語モデルの情報（引用：A Survey of Large Language Models）DeepSpeedの概要を知ろうDeepSpeedはMicrosoftが開発した深層学習向け高速化・省メモリ化ライブラリです。深層学習のボトルネックである計算量・メモリ量・データ量の扱いを解決するための機能が多数盛り込まれており、近年著しい発展を遂げている拡散モデルやLLMのような生成AIの処理高速化にも活用されています。DeepSpeedの特徴として、以下が挙げられます。数十億～1兆規模のパラメタを持つ超巨大な深層学習モデルの訓練と推論GPU間のデータ転送が高速かつ低遅延数千GPU規模のスケーラビリティの訓練限られたGPUリソース環境における訓練と推論モデル圧縮技術による低遅延な推論、モデルサイズ削減機能いずれも、計算量・メモリ量・データ量の高効率な処理を実現するものです。上記の処理は深層学習（ニューラルネットワーク）を用いた処理に対して汎用的に適用可能であり、生成AIのみならずTransformerや畳み込みニューラルネットワーク、グラフニューラルネットワーク等極めて広い範囲に適用可能です。また、公式の日本語のサポートも非常に手厚く、公式X、公式ブログといった発信が行われていることに加え、日本語での問い合わせも可能という厚遇っぷりです。DeepSpeedには深層学習の課題に合わせた機能が複数用意されています。一部をピックアップして紹介します（日本語の解説資料も存在するため、詳細はそちらをご覧ください）。表２：課題に対するDeepSpeedの機能一覧（DeepSpeed: 深層学習の訓練と推論を劇的に 高速化するフレームワークを基に改編）モデル推論を高速化するためにDeepSpeed Compressionを用いてモデル圧縮を行う、ChatGPTライクなモデルを作るためにDeepSpeed-Chatを用いてより高速な強化学習を実現する、数千GPUで学習を行うために3D parallelismを用いる、といった使い方が可能であり、大規模な学習を行う上で必須のツールと言えるでしょう。このうち、DeepSpeedの一番の特徴といわれるのがZeRO(Zero Redundancy Optimizer)です。パラメタ数が大きいLLMを学習する場合、単体のGPUではモデルがGPUメモリに乗りきらないため、複数のGPUを用いることが不可欠です。複数GPUを用いる場合、モデル全体を各GPUに読み込ませたうえで処理するのが一般的ですが、ZeROでは必要なパラメタだけ読み込ませて処理する、という工夫を行っています。これにより、メモリ利用の効率化に成功しています。図１：2023年10月時点の代表的な大規模言語モデルの情報（引用：DeepSpeed: 深層学習の訓練と推論を劇的に 高速化するフレームワーク）図２：2023年10月時点の代表的な大規模言語モデルの情報（引用：DeepSpeed: 深層学習の訓練と推論を劇的に 高速化するフレームワーク）メモリ効率化を行うという考え方は、ZeROファミリーに共通する考え方です。ZeRO-Offloadでは、複数GPUを持たないケースにおいても、CPUメモリにオフロード（負荷分散）するアイデアにより40B（400億）パラメタのモデルを単体GPUで学習することに成功しています。図３：2023年10月時点の代表的な大規模言語モデルの情報（引用：DeepSpeed: 深層学習の訓練と推論を劇的に 高速化するフレームワーク）DeepSpeedを動かしてみようここまでDeepSpeedの概要と機能を紹介しました。ここからは、実際にDeepSpeedを動かしてみることで、具体的な性能を確認していきます。セットアップ検証に用いた環境（オンプレ）を以下に示します。今回は、transformersに搭載されているdeepspeedのサンプルを用いて測定を行います。インストールは以下コマンドで実行できます。詳細は、DeepSpeed Integrationをご参照ください。pip install deepspeed sacrebleu numactl datasets
git clone https://github.com/huggingface/transformers
cd transformers
pip install .DeepSpeedの設定はjsonファイルを用いて記述します。今回用いた設定ファイルは以下の通りです。ds_config_zero2.json{""fp16"":{""enabled"":""auto"",""loss_scale"":0,""loss_scale_window"":1000,""initial_scale_power"":16,""hysteresis"":2,""min_loss_scale"":1},""bf16"":{""enabled"":""auto""},""optimizer"":{""type"":""AdamW"",""params"":{""lr"":""auto"",""betas"":""auto"",""eps"":""auto"",""weight_decay"":""auto""}},""scheduler"":{""type"":""WarmupLR"",""params"":{""warmup_min_lr"":""auto"",""warmup_max_lr"":""auto"",""warmup_num_steps"":""auto""}},""zero_optimization"":{""stage"":2,""offload_optimizer"":{""device"":""cpu"",""pin_memory"":true},""allgather_partitions"":true,""allgather_bucket_size"":2e8,""overlap_comm"":true,""reduce_scatter"":true,""reduce_bucket_size"":2e8,""contiguous_gradients"":true},""gradient_accumulation_steps"":""auto"",""gradient_clipping"":""auto"",""steps_per_print"":2000,""train_batch_size"":""auto"",""train_micro_batch_size_per_gpu"":""auto"",""wall_clock_breakdown"":false}実験今回は2種類の実験を行います。単体GPUでの測定（ZeRO Offload）複数GPUでの測定（ZeRO Stage-2）問題設定として、サンプルとして公開されているtransformersを用いた2言語間の翻訳タスク（英語→ロシア語）におけるファインチューニングを適用します。実験にあたっての具体的なパラメタは、実行コマンドの引数をご参照ください。今回は、ベースモデル、およびバッチサイズを変更しながら、DeepSpeedを用いた場合と用いない場合のCPU使用率・GPUメモリ使用量・訓練の所要時間を測定します。（簡易な測定のため、測定値は誤差が生じている可能性があります）実行コマンド測定①：単体GPUでの測定（ZeRO Offload）本測定では、単体GPUを用いて測定します。具体的には、以下のコマンドを用いて実行します。共通gitclone https://github.com/huggingface/transformerscdtransformersDeepSpeed適用なしの実行コマンドCUDA_VISIBLE_DEVICES=1python examples/pytorch/translation/run_translation.py\--model_name_or_path${BASE_MODEL}--per_device_train_batch_size${BATCH_SIZE}\--output_dir ~/output_dir --overwrite_output_dir --fp16\--do_train --max_train_samples10000--num_train_epochs1\--dataset_name wmt16 --dataset_config""ro-en""\--source_lang en --target_lang roDeepSpeed（ZeRO Offload）適用ありの実行コマンドdeepspeed --include localhost:1 examples/pytorch/translation/run_translation.py\--deepspeed ds_config_zero2.json\--model_name_or_path${BASE_MODEL}--per_device_train_batch_size${BATCH_SIZE}\--output_dir ~/output_dir --overwrite_output_dir --fp16\--do_train --max_train_samples10000--num_train_epochs1\--dataset_name wmt16 --dataset_config""ro-en""\--source_lang en --target_lang roDeepSpeedを用いない場合は、CUDA_VISIBLE_DEVICESでGPUを指定しますが、DeepSpeedでGPUを指定する場合は、–include localhost:1（ホスト名:GPUのデバイス番号）を指定することにご注意ください（デバイス数だけ指定する場合は、–num_gpus=1で指定できます）。${BASE_MODEL}はt5-smallとt5-baseを、${BATCH_SIZE}は2^0から2^8を用いました。測定結果図４：GPUメモリ使用量図５：CPU使用率図６：訓練の所要時間図４～６に測定結果を示します。なお、凡例はFine-tuningのベースモデルを示します。カッコ内はwoがDeepSpeed適用なし、withがDeepSpeed適用ありの結果を示します。t5-small(with)であれば、t5-smallをベースモデルに、DeepSpeedを用いてFine-tuningを行った結果を示します。プロットされていないマーカーは、GPUメモリが足りず実行できなかったパラメタになります。まず、図４のGPUメモリ使用量を確認すると、DeepSpeedを適用することで若干ながらも抑制できていることが分かります。また、t5-baseにおいては、DeepSpeedを用いない場合バッチサイズ64までしか学習できませんが、DeepSpeedを適用することでバッチサイズ128まで学習することができていることが分かります。図５のCPU利用率は、DeepSpeedを用いた場合常に100%に貼りついています。これはZeRO Offloadを用いることで、一部のデータがCPUにオフロードされたことによるものと考えられます。図６の訓練の所要時間は、バッチサイズが小さい場合、DeepSpeedを適用したときのほうが遅くなる傾向にありました。いっぽうで、バッチサイズが大きくなるにつれてその差は小さくなってきています。これはCPUへのオフロードにおけるデータ転送時間がボトルネックになっていることが想像されます。測定②：GPU2台（ZeRO Stage-2）本測定では、複数GPUを用いて測定します。具体的には、以下のコマンドを用いて実行します（測定①からGPUの指定を省いただけになります）。なお、GPUメモリ使用量は2台のGPUメモリ使用量の合計を算出しました。なお、CPU利用率は、測定①と傾向が変わらないため割愛しました。共通gitclone https://github.com/huggingface/transformerscdtransformersDeepSpeed適用なしの実行コマンドpython examples/pytorch/translation/run_translation.py\--model_name_or_path${BASE_MODEL}--per_device_train_batch_size${BATCH_SIZE}\--output_dir ~/output_dir --overwrite_output_dir --fp16\--do_train --max_train_samples10000--num_train_epochs1\--dataset_name wmt16 --dataset_config""ro-en""\--source_lang en --target_lang roDeepSpeed（ZeRO-Offload）適用ありの実行コマンドdeepspeed examples/pytorch/translation/run_translation.py\--deepspeed ds_config_zero2.json\--model_name_or_path${BASE_MODEL}--per_device_train_batch_size${BATCH_SIZE}\--output_dir ~/output_dir --overwrite_output_dir --fp16\--do_train --max_train_samples10000--num_train_epochs1\--dataset_name wmt16 --dataset_config""ro-en""\--source_lang en --target_lang ro測定結果図７：GPUメモリ使用量図８：訓練の所要時間基本的には測定①と同様の傾向になりますが、一部で異なる傾向が見られています。まずはGPUメモリ使用量に注目します。図９を用いて測定①と測定②を比較すると、分かりやすいでしょう。図９：測定①と測定②のGPUメモリ使用量比較DeepSpeedを適用することでメモリ使用量を抑えることができる傾向に変わりはありませんが、測定②のほうが抑制割合がより顕著なものとなっています。これは、ZeROを用いたパラメタの分散によるメモリ効率化によるものと考えられます。すなわち、ZeROを用いない場合はモデルのパラメタをまるごとGPUメモリにのせる一方で、ZeROを用いる場合はモデルのパラメタを分割してGPUメモリにのせることができるため、1台のGPUを用いるよりもより効率的にGPUメモリを抑制できたと考えられます。図８の訓練の所要時間についても、基本的な傾向は変わらないものの、バッチサイズが52よりも大きくなると、DeepSpeedを用いたときのほうが高速になりました。これは、バッチサイズが大きくなることによりGPUメモリへのデータ転送量が増加したことが関係していると考えられます。このような時、DeepSpeedを用いたパラメタ分散により、メモリへのデータ転送量を抑制することができ、結果としてボトルネックになりやすいデータ転送時間の抑制につながったと考えられます。実験のまとめと感想本実験を通し、以下のことが確認できました。DeepSpeedを活用することで必要なGPUメモリを抑えられることケースによっては学習速度を縮められること利用には少しノウハウが必要ですが、コツを掴めばより大規模学習にも活用できるツールであることを感じました。今回は検証できませんでしたが、より高速な記憶媒体であるMNVeにパラメタをオフロードする機能Zero-Infinityも存在します。MNVeであれば、GPUよりもより低コストで大きなメモリを確保できるため、（多少遅くなるとは思われますが）なかなか実現しづらかったサイズのLLM検証も期待できるようになります。また、今回は学習をターゲットにしましたが、推論においてもいくつかの機能が搭載されているため、こちらも検証できればと思っています。おわりに本記事では、Microsoftが開発した深層学習向け高速化・省メモリ化ライブラリDeepSpeedを取り上げ、具体的な特徴と導入手順、実験結果を紹介しました。実験を通し、DeepSpeedを活用することで必要なGPUメモリを抑えられること、ケースによっては学習速度を縮められることが分かりました。AI活用においては、AIができることだけではなく、マシンスペックの制約を下げコスト抑制につなげること、高速化を通してより快適に使用できることが重要です。DeepSpeedはそのための技術のひとつとなりうるでしょう。本記事を通して、DeepSpeedに対する知見が深まれば、筆者としては喜ばしい限りです。さいごに、ARISE analyticsではKDDIグループを支えるためのAI技術開発を行っています。今回紹介した内容や、その他記事で触れている弊社の取り組みに興味がございましたらぜひお声がけください。▼採用ページはこちらhttps://www.ariseanalytics.com/recruit/"
